{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import ParallelCorpus\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "num_epochs = 1\n",
    "batch_size = 64\n",
    "learning_rate = 4e-4\n",
    "embedding_dim = 512\n",
    "#hidden_dim = 2 * embedding_dim\n",
    "max_allowed_sentence_len = 50\n",
    "#force = 1\n",
    "volatile = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137\n",
      "1136\n"
     ]
    }
   ],
   "source": [
    "# CHANGE: CORRECT DATA LOCATIONS.\n",
    "\n",
    "training_set = ParallelCorpus(\n",
    "        source_path=\"./data/train/train_bpe.fr\", target_path=\"./data/train/train_bpe.en\",\n",
    "        max_sentence_length=max_allowed_sentence_len\n",
    "    )\n",
    "\n",
    "validation_set = ParallelCorpus(\n",
    "        source_path=\"./data/val/val_bpe.fr\", target_path=\"./data/val/val_bpe.en\",\n",
    "        max_sentence_length=max_allowed_sentence_len, use_indices_from=training_set\n",
    "    )\n",
    "\n",
    "test_set = ParallelCorpus(\n",
    "        source_path=\"./data/test/test_2017_flickr_bpe.fr\", target_path=\"./data/test/test_2017_flickr_bpe.en\",\n",
    "        max_sentence_length=max_allowed_sentence_len, use_indices_from=training_set\n",
    "    )\n",
    "\n",
    "print(len(training_set.target_w2i))\n",
    "print(training_set.target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137\n",
      "1136\n"
     ]
    }
   ],
   "source": [
    "print(len(training_set.target_w2i))\n",
    "print(training_set.target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"A Vanilla Sequence to Sequence (Seq2Seq) model with LSTMs.\n",
    "    Ref: Sequence to Sequence Learning with Neural Nets\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, src_emb_dim, trg_emb_dim, src_vocab_size,\n",
    "        trg_vocab_size, src_hidden_dim, trg_hidden_dim,\n",
    "        pad_token_src, pad_token_trg, bidirectional=False,\n",
    "        nlayers_src=1, nlayers_trg=1\n",
    "    ):\n",
    "        \"\"\"Initialize Seq2Seq Model.\"\"\"\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.src_emb_dim = src_emb_dim\n",
    "        self.trg_emb_dim = trg_emb_dim\n",
    "        self.src_hidden_dim = src_hidden_dim\n",
    "        self.trg_hidden_dim = trg_hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.nlayers_src = nlayers_src\n",
    "        self.nlayers_trg = nlayers_trg\n",
    "        self.pad_token_src = pad_token_src\n",
    "        self.pad_token_trg = pad_token_trg\n",
    "        \n",
    "        # Word Embedding look-up table for the soruce language\n",
    "        self.src_embedding = nn.Embedding(\n",
    "            self.src_vocab_size,\n",
    "            self.src_emb_dim,\n",
    "            self.pad_token_src,\n",
    "        )\n",
    "\n",
    "        # Word Embedding look-up table for the target language\n",
    "        self.trg_embedding = nn.Embedding(\n",
    "            self.trg_vocab_size,\n",
    "            self.trg_emb_dim,\n",
    "            self.pad_token_trg,\n",
    "        )\n",
    "\n",
    "        # Encoder GRU\n",
    "        self.encoder = nn.GRU(\n",
    "            self.src_emb_dim // 2 if self.bidirectional else self.src_emb_dim,\n",
    "            self.src_hidden_dim,\n",
    "            self.nlayers_src,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Decoder GRU\n",
    "        self.decoder = nn.GRU(\n",
    "            self.trg_emb_dim,\n",
    "            self.trg_hidden_dim,\n",
    "            self.nlayers_trg,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Projection layer from decoder hidden states to target language vocabulary\n",
    "        self.decoder2vocab = nn.Linear(trg_hidden_dim, trg_vocab_size)\n",
    "\n",
    "    def forward(self, input_src, input_trg, src_lengths):\n",
    "        # Lookup word embeddings in source and target minibatch\n",
    "        src_emb = self.src_embedding(input_src)\n",
    "        trg_emb = self.trg_embedding(input_trg)\n",
    "        \n",
    "        # Pack padded sequence for length masking in encoder RNN (This requires sorting input sequence by length)\n",
    "        src_emb = pack_padded_sequence(src_emb, src_lengths, batch_first=True)\n",
    "        \n",
    "        # Run sequence of embeddings through the encoder GRU\n",
    "        _, src_h_t = self.encoder(src_emb)\n",
    "        \n",
    "        # Extract the last hidden state of the GRU\n",
    "        h_t = torch.cat((src_h_t[-1], src_h_t[-2]), 1) if self.bidirectional else src_h_t[-1]\n",
    "\n",
    "        # Initialize the decoder GRU with the last hidden state of the encoder and \n",
    "        # run target inputs through the decoder.\n",
    "        trg_h, _ = self.decoder(trg_emb, h_t.unsqueeze(0).expand(self.nlayers_trg, h_t.size(0), h_t.size(1)))\n",
    "        \n",
    "        # Merge batch and time dimensions to pass to a linear layer\n",
    "        trg_h_reshape = trg_h.contiguous().view(\n",
    "            trg_h.size(0) * trg_h.size(1), trg_h.size(2)\n",
    "        )\n",
    "        \n",
    "        # Affine transformation of all decoder hidden states\n",
    "        decoder2vocab = self.decoder2vocab(trg_h_reshape)\n",
    "        \n",
    "        # Reshape\n",
    "        decoder2vocab = decoder2vocab.view(\n",
    "            trg_h.size(0), trg_h.size(1), decoder2vocab.size(1)\n",
    "        )\n",
    "\n",
    "        return decoder2vocab\n",
    "    \n",
    "    def decode(self, decoder2vocab):\n",
    "        # Turn decoder output into a probabiltiy distribution over vocabulary\n",
    "        decoder2vocab_reshape = decoder2vocab.view(-1, decoder2vocab.size(2))\n",
    "        word_probs = F.softmax(decoder2vocab_reshape)\n",
    "        word_probs = word_probs.view(\n",
    "            decoder2vocab.size(0), decoder2vocab.size(1), decoder2vocab.size(2)\n",
    "        )\n",
    "\n",
    "        return word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def  get_parallel_minibatchget_par (lines, src_word2id, trg_word2id, index, batch_size, volatile=False):\n",
    "        \n",
    "#         # Get source sentences for this minibatch\n",
    "#         src_lines = [\n",
    "#             ['<s>'] + list(line[1]) + ['</s>']\n",
    "#             for line in lines[index: index + batch_size]\n",
    "#         ]\n",
    "\n",
    "#         # Get target sentences for this minibatch\n",
    "#         trg_lines = [\n",
    "#             ['<s>'] + line[0].split() + ['</s>']\n",
    "#             for line in lines[index: index + batch_size]\n",
    "#         ]\n",
    "        \n",
    "#         # Sort source sentences by length for length masking in RNNs\n",
    "#         src_lens = [len(line) for line in src_lines]\n",
    "#         sorted_indices = np.argsort(src_lens)[::-1]\n",
    "        \n",
    "#         # Reorder sentences based on source lengths\n",
    "#         sorted_src_lines = [src_lines[idx] for idx in sorted_indices]\n",
    "#         sorted_trg_lines = [trg_lines[idx] for idx in sorted_indices]\n",
    "        \n",
    "#         # Compute new sentence lengths\n",
    "#         sorted_src_lens = [len(line) for line in sorted_src_lines]\n",
    "#         sorted_trg_lens = [len(line) for line in sorted_trg_lines]\n",
    "        \n",
    "#         # Get max source and target lengths to pad input and output sequences\n",
    "#         max_src_len = max(sorted_src_lens)\n",
    "#         max_trg_len = max(sorted_trg_lens)\n",
    "        \n",
    "#         # Construct padded source input sequence\n",
    "#         input_lines_src = [\n",
    "#             [src_word2id[w] if w in src_word2id else src_word2id['<unk>'] for w in line] +\n",
    "#             [src_word2id['<pad>']] * (max_src_len - len(line))\n",
    "#             for line in sorted_src_lines\n",
    "#         ]\n",
    "\n",
    "#         # Construct padded target input sequence\n",
    "#         input_lines_trg = [\n",
    "#             [trg_word2id[w] if w in trg_word2id else trg_word2id['<unk>'] for w in line[:-1]] +\n",
    "#             [trg_word2id['<pad>']] * (max_trg_len - len(line))\n",
    "#             for line in sorted_trg_lines\n",
    "#         ]\n",
    "\n",
    "#         # Construct padded target output sequence (Note: Output sequence is just the input shifted by 1 position)\n",
    "#         # This is for teacher-forcing\n",
    "#         output_lines_trg = [\n",
    "#             [trg_word2id[w] if w in trg_word2id else trg_word2id['<unk>'] for w in line[1:]] +\n",
    "#             [trg_word2id['<pad>']] * (max_trg_len - len(line))\n",
    "#             for line in sorted_trg_lines\n",
    "#         ]\n",
    "\n",
    "#         input_lines_src = Variable(torch.LongTensor(input_lines_src), volatile=volatile)\n",
    "#         input_lines_trg = Variable(torch.LongTensor(input_lines_trg), volatile=volatile)\n",
    "#         output_lines_trg = Variable(torch.LongTensor(output_lines_trg), volatile=volatile)\n",
    "\n",
    "#         return {\n",
    "#             'input_src': input_lines_src,\n",
    "#             'input_trg': input_lines_trg,\n",
    "#             'output_trg': output_lines_trg,\n",
    "#             'src_lens': sorted_src_lens\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137\n",
      "1136\n"
     ]
    }
   ],
   "source": [
    "print(len(training_set.target_w2i))\n",
    "print(training_set.target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(training_set, batch_size=batch_size)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137\n",
      "1136\n"
     ]
    }
   ],
   "source": [
    "print(len(training_set.target_w2i))\n",
    "print(training_set.target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(\n",
    "    src_emb_dim=128, trg_emb_dim=128,\n",
    "    src_vocab_size=training_set.source_vocab_size,\n",
    "    trg_vocab_size=training_set.target_vocab_size,\n",
    "    src_hidden_dim=embedding_dim, trg_hidden_dim=embedding_dim,\n",
    "    pad_token_src=training_set.source_pad,\n",
    "    pad_token_trg=training_set.target_pad,\n",
    ")\n",
    "\n",
    "\n",
    "if cuda_available:\n",
    "    seq2seq = seq2seq.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(seq2seq.parameters(), lr=learning_rate)\n",
    "weight_mask = torch.ones(len(training_set.target_w2i)-1)\n",
    "if cuda_available:\n",
    "    weight_mask = weight_mask.cuda()\n",
    "weight_mask[training_set.target_pad] = 0\n",
    "loss_criterion = nn.CrossEntropyLoss(weight=weight_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137\n",
      "1136\n"
     ]
    }
   ],
   "source": [
    "#from corpus import ParallelCorpus\n",
    "\n",
    "print(len(training_set.target_w2i))\n",
    "print(training_set.target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Training Loss : 4.154\n",
      "Epoch : 0 Dev Loss : 4.179\n",
      "Epoch : 0 Test Loss : 4.340\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, num_epochs):\n",
    "    losses = []\n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in training_loader:\n",
    "        \n",
    "        \n",
    "        source_batch = Variable(source_batch, volatile=volatile)\n",
    "        output_batch = Variable(target_batch[:,1:], volatile=volatile)\n",
    "        target_batch = Variable(target_batch[:,:-1], volatile=volatile)\n",
    "        #print(target_batch.size())\n",
    "        #print(output_batch.size())\n",
    "        \n",
    "        if cuda_available:\n",
    "            source_batch.cuda()\n",
    "            source_batch.cuda()\n",
    "            output_batch.cuda()\n",
    "            \n",
    "        decoder_out = seq2seq(\n",
    "            input_src=source_batch, input_trg=target_batch, src_lengths=source_lengths\n",
    "        )\n",
    "        \n",
    "        #print(decoder_out.contiguous().view(-1, decoder_out.size(2)).size())\n",
    "        #print(output_batch.contiguous().view(-1).size())\n",
    "        \n",
    "        loss = loss_criterion(\n",
    "            decoder_out.contiguous().view(-1, decoder_out.size(2)),\n",
    "            output_batch.contiguous().view(-1)\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm(seq2seq.parameters(), 5.)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "        \n",
    "    dev_nll = []\n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in validation_loader:\n",
    "        \n",
    "        \n",
    "        source_batch = Variable(source_batch, volatile=volatile)\n",
    "        output_batch = Variable(target_batch[:,1:], volatile=volatile)\n",
    "        target_batch = Variable(target_batch[:,:-1], volatile=volatile)\n",
    "        #print(target_batch.size())\n",
    "        #print(output_batch.size())\n",
    "        \n",
    "        if cuda_available:\n",
    "            source_batch.cuda()\n",
    "            source_batch.cuda()\n",
    "            output_batch.cuda()\n",
    "            \n",
    "        decoder_out = seq2seq(\n",
    "            input_src=source_batch, input_trg=target_batch, src_lengths=source_lengths\n",
    "        )\n",
    "        \n",
    "        #print(decoder_out.contiguous().view(-1, decoder_out.size(2)).size())\n",
    "        #print(output_batch.contiguous().view(-1).size())\n",
    "        \n",
    "        loss = loss_criterion(\n",
    "            decoder_out.contiguous().view(-1, decoder_out.size(2)),\n",
    "            output_batch.contiguous().view(-1)\n",
    "        )\n",
    "        dev_nll.append(loss.item())\n",
    "        \n",
    "    test_nll = []    \n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in test_loader:\n",
    "        \n",
    "        \n",
    "        source_batch = Variable(source_batch, volatile=volatile)\n",
    "        output_batch = Variable(target_batch[:,1:], volatile=volatile)\n",
    "        target_batch = Variable(target_batch[:,:-1], volatile=volatile)\n",
    "        #print(target_batch.size())\n",
    "        #print(output_batch.size())\n",
    "        \n",
    "        if cuda_available:\n",
    "            source_batch.cuda()\n",
    "            source_batch.cuda()\n",
    "            output_batch.cuda()\n",
    "            \n",
    "        decoder_out = seq2seq(\n",
    "            input_src=source_batch, input_trg=target_batch, src_lengths=source_lengths\n",
    "        )\n",
    "        \n",
    "        #print(decoder_out.contiguous().view(-1, decoder_out.size(2)).size())\n",
    "        #print(output_batch.contiguous().view(-1).size())\n",
    "        \n",
    "        loss = loss_criterion(\n",
    "            decoder_out.contiguous().view(-1, decoder_out.size(2)),\n",
    "            output_batch.contiguous().view(-1)\n",
    "        )\n",
    "        test_nll.append(loss.item())\n",
    "    \n",
    "    \n",
    "    print('Epoch : %d Training Loss : %.3f' % (epoch, np.mean(losses)))\n",
    "    print('Epoch : %d Dev Loss : %.3f' % (epoch, np.mean(dev_nll)))\n",
    "    print('Epoch : %d Test Loss : %.3f' % (epoch, np.mean(test_nll)))\n",
    "    print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'of', 'people', 'waiting', 'for', 'the', 'train', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['group', 'of', 'people', 'waiting', 'for', 'the', 'subway', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "_, sentence = torch.max(decoder_out[0],1)\n",
    "test_pred = [training_set.target_i2w[word] for word in sentence.cpu().numpy()]\n",
    "print(test_pred)\n",
    "test_real = [training_set.target_i2w[word] for word in output_batch[0].cpu().numpy()]\n",
    "print(test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(seq2seq, \"seq2seq_10_alpha_cpu.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
