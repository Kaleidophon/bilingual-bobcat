{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import ParallelCorpus\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64179170-5ace-4fe1-a7fd-0e3edcf17395\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 4e-4\n",
    "embedding_dim = 256\n",
    "hidden_dim = embedding_dim*2\n",
    "#hidden_dim = 2 * embedding_dim\n",
    "max_allowed_sentence_len = 50\n",
    "drop = 0.2\n",
    "#force = 1\n",
    "bidirectional = False\n",
    "LSTM_instead = False\n",
    "volatile = False\n",
    "position_based = True\n",
    "context = True\n",
    "\n",
    "name= str(uuid.uuid4())\n",
    "print(name)\n",
    "save_dir = \"models/\"\n",
    "save_dir2 = \"dicts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE: CORRECT DATA LOCATIONS.\n",
    "\n",
    "training_set = ParallelCorpus(\n",
    "        source_path=\"./data/train/train_bpe.fr\", target_path=\"./data/train/train_bpe.en\",\n",
    "        max_sentence_length=max_allowed_sentence_len\n",
    "    )\n",
    "\n",
    "validation_set = ParallelCorpus(\n",
    "        source_path=\"./data/val/val_bpe.fr\", target_path=\"./data/val/val_bpe.en\",\n",
    "        max_sentence_length=max_allowed_sentence_len, use_indices_from=training_set\n",
    "    )\n",
    "\n",
    "test_set = ParallelCorpus(\n",
    "        source_path=\"./data/test/test_2017_flickr_bpe.fr\", target_path=\"./data/test/test_2017_flickr_bpe.en\",\n",
    "        max_sentence_length=max_allowed_sentence_len, use_indices_from=training_set\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE: CORRECT DATA LOCATIONS.\n",
    "\n",
    "training_set = ParallelCorpus(\n",
    "        source_path=\"./data2/train_bpe.fr\", target_path=\"./data2/train_bpe.en\",\n",
    "        max_sentence_length=max_allowed_sentence_len\n",
    "    )\n",
    "\n",
    "validation_set = ParallelCorpus(\n",
    "        source_path=\"./data2/val_bpe.fr\", target_path=\"./data2/val_bpe.en\",\n",
    "        max_sentence_length=max_allowed_sentence_len, use_indices_from=training_set\n",
    "    )\n",
    "\n",
    "test_set = ParallelCorpus(\n",
    "        source_path=\"./data2/test_2017_flickr_bpe.fr\", target_path=\"./data2/test_2017_flickr_bpe.en\",\n",
    "        max_sentence_length=max_allowed_sentence_len, use_indices_from=training_set\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"A Vanilla Sequence to Sequence (Seq2Seq) model with LSTMs.\n",
    "    Ref: Sequence to Sequence Learning with Neural Nets\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, trg_emb_dim,\n",
    "        trg_vocab_size, trg_hidden_dim,\n",
    "        pad_token_trg, drop, context=True,\n",
    "        LSTM_instead=False, bidirectional=False,\n",
    "        nlayers_trg=1,\n",
    "    ):\n",
    "        \"\"\"Initialize Seq2Seq Model.\"\"\"\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.trg_emb_dim = trg_emb_dim\n",
    "        self.trg_hidden_dim = trg_hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.nlayers_trg = nlayers_trg\n",
    "        self.pad_token_trg = pad_token_trg\n",
    "        self.attn_soft = nn.Softmax(dim=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.LSTM_instead=LSTM_instead\n",
    "        self.context = context\n",
    "\n",
    "        # Word Embedding look-up table for the target language\n",
    "        self.trg_embedding = nn.Embedding(\n",
    "            self.trg_vocab_size,\n",
    "            self.trg_emb_dim,\n",
    "            self.pad_token_trg,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Decoder GRU // LSTM\n",
    "        if (not self.LSTM_instead):\n",
    "        \n",
    "            self.decoder = nn.GRU(\n",
    "                self.trg_emb_dim,\n",
    "                self.trg_hidden_dim,\n",
    "                self.nlayers_trg,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:\n",
    "            self.decoder = nn.LSTM(\n",
    "                self.trg_emb_dim,\n",
    "                self.trg_hidden_dim,\n",
    "                self.nlayers_trg,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "#         self.scaler = nn.Linear(\n",
    "#             self.trg_hidden_dim,\n",
    "#             self.trg_emb_dim*2,\n",
    "#         )\n",
    "        \n",
    "#         self.scaler2 = nn.Linear(\n",
    "#             self.trg_hidden_dim,\n",
    "#             self.trg_emb_dim,\n",
    "#         )\n",
    "        \n",
    "        # Projection layer from decoder hidden states to target language vocabulary\n",
    "        \n",
    "        if (not self.context):\n",
    "            self.decoder2vocab = nn.Linear(trg_hidden_dim, trg_vocab_size)\n",
    "        else:\n",
    "            self.decoder2vocab = nn.Linear(trg_hidden_dim + trg_emb_dim*2, trg_vocab_size)\n",
    "\n",
    "    def forward(self, encoder_out, h_t, input_trg, source_lengths, teacher ):\n",
    "        trg_emb = self.trg_embedding(input_trg)\n",
    "        trg_emb = self.dropout(trg_emb)\n",
    "        \n",
    "        h_t = h_t.unsqueeze(0).expand(self.nlayers_trg, h_t.size(0), h_t.size(1))\n",
    "        if (self.LSTM_instead):\n",
    "            h_t = (h_t,h_t)\n",
    "        \n",
    "        hidden = []\n",
    "        outputs = []\n",
    "        trg_in = trg_emb[:,0,:].unsqueeze(1)\n",
    "        for i in range(input_trg.size(1)):\n",
    "#             print( \" \")\n",
    "            if (teacher):\n",
    "                trg_in = trg_emb[:,i,:].unsqueeze(1)\n",
    "#             print(trg_in.size())\n",
    "            trg_h, h_t = self.decoder(trg_in, h_t)\n",
    "            hidden.append(h_t.squeeze())\n",
    "#             print(h_t.size())\n",
    "\n",
    "            if (self.context):\n",
    "                context = self.attention(h_t.squeeze().unsqueeze(1),encoder_out,source_lengths, encoder_out.size(1))\n",
    "                trg_h = torch.cat((trg_h,context),2)\n",
    "            \n",
    "            trg_h_reshape = trg_h.contiguous().view(\n",
    "            trg_h.size(0) * trg_h.size(1), trg_h.size(2)\n",
    "            )\n",
    "            # Affine transformation of all decoder hidden states\n",
    "            decoder2vocab = self.decoder2vocab(trg_h_reshape)\n",
    "            # Reshape\n",
    "            decoder2vocab = decoder2vocab.view(\n",
    "                trg_h.size(0), trg_h.size(1), decoder2vocab.size(1)\n",
    "            )\n",
    "\n",
    "        \n",
    "            \n",
    "            outputs.append(decoder2vocab.squeeze())\n",
    "            \n",
    "            if (not teacher):\n",
    "                _, word = torch.max(decoder2vocab,2)\n",
    "                trg_in = self.trg_embedding(word)\n",
    "            \n",
    "        outputs = torch.stack(outputs,1)\n",
    "        return outputs    \n",
    "            \n",
    "            \n",
    "            \n",
    "#             if (not teacher):\n",
    "#                 trg_in = self.scaler2(decoder_out)\n",
    "#                 trg_in = self.relu(trg_in)\n",
    "            \n",
    "        # hiddens = torch.stack(hidden,1)\n",
    "        # trg_h = torch.stack(outputs,1)\n",
    "#         print(trg_h.size())\n",
    "#         print(hiddens.size())\n",
    "        #if torch.cuda.is_available()\n",
    "        #    trg_\n",
    "        #print(asdfs)\n",
    "        \n",
    "            \n",
    "        #hiddens = self.scaler(hiddens) \n",
    "        #hiddens = self.relu(hiddens)\n",
    "#         if (self.context):\n",
    "#             context = self.attention(hiddens,encoder_out,source_lengths, encoder_out.size(1))\n",
    "#             print(hiddens.size())\n",
    "#             print(encoder_out.size())\n",
    "\n",
    "#             trg_h = torch.cat((trg_h,context),2)\n",
    "        \n",
    "        \n",
    "#         # Initialize the decoder GRU with the last hidden state of the encoder and \n",
    "#         # run target inputs through the decoder.\n",
    "        \n",
    "#         # Merge batch and time dimensions to pass to a linear layer\n",
    "#         trg_h_reshape = trg_h.contiguous().view(\n",
    "#             trg_h.size(0) * trg_h.size(1), trg_h.size(2)\n",
    "#         )\n",
    "        \n",
    "#         # Affine transformation of all decoder hidden states\n",
    "#         decoder2vocab = self.decoder2vocab(trg_h_reshape)\n",
    "        \n",
    "#         # Reshape\n",
    "#         decoder2vocab = decoder2vocab.view(\n",
    "#             trg_h.size(0), trg_h.size(1), decoder2vocab.size(1)\n",
    "#         )\n",
    "    \n",
    "    \n",
    "    def attention(self, hidden_to_attn, encoder_outputs, source_lengths, max_len):\n",
    "                \n",
    "        \n",
    "#         print(hidden_to_attn.size())\n",
    "#         print(encoder_outputs.size())\n",
    "        # repeat the lstm out in third dimension and the encoder outputs in second dimension so we can make a meshgrid\n",
    "        # so we can do elementwise mul for all possible combinations of h_j and s_i\n",
    "        h_j = encoder_outputs.unsqueeze(1).repeat(1,hidden_to_attn.size(1),1,1)\n",
    "        s_i = hidden_to_attn.unsqueeze(2).repeat(1,1,encoder_outputs.size(1),1)\n",
    "#         print(h_j.size())\n",
    "#         print(s_i.size())\n",
    "        # get the dot product between the two to get the energy\n",
    "        # the unsqueezes are there to emulate transposing. so we can use matmul as torch.dot doesnt accept matrices\n",
    "        energy = s_i.unsqueeze(3).matmul(h_j.unsqueeze(4)).squeeze(4)\n",
    "        \n",
    "#         # this is concat attention, its a different form then the ones we need\n",
    "#         cat = torch.cat((s_i,h_j),3)\n",
    "        \n",
    "#         energy = self.attn_layer(cat)\n",
    "\n",
    "        # reshaping the encoder outputs for later\n",
    "        encoder_outputs = encoder_outputs.unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.repeat(1,energy.size(1),1,1)\n",
    "    \n",
    "        # apply softmax to the energys \n",
    "        allignment = self.attn_soft(energy)\n",
    "        \n",
    "        # create a mask like : [1,1,1,0,0,0] whos goal is to multiply the attentions of the pads with 0, rest with 1\n",
    "        idxes = torch.arange(0,max_len).unsqueeze(0).long()\n",
    "        #print(idxes.size())\n",
    "        mask = Variable((idxes<source_lengths.unsqueeze(1)).float())\n",
    "        \n",
    "        # format the mask to be same size() as the attentions\n",
    "        mask = mask.unsqueeze(1).unsqueeze(3).repeat(1,allignment.size(1),1,1)\n",
    "        \n",
    "        # apply mask\n",
    "        masked = allignment * mask\n",
    "        \n",
    "        # now we have to rebalance the other values so they sum to 1 again\n",
    "        # this is done by dividing each value by the sum of the sequence\n",
    "        # calculate sums\n",
    "        msum = masked.sum(-2).repeat(1,1,masked.size(2)).unsqueeze(3)\n",
    "        \n",
    "        # rebalance\n",
    "        attentions = masked.div(msum)\n",
    "        \n",
    "        # now we shape the attentions to be similar to context in size\n",
    "        allignment = allignment.repeat(1,1,1,encoder_outputs.size(3))\n",
    "\n",
    "        # make context vector by element wise mul\n",
    "        context = attentions * encoder_outputs\n",
    "        \n",
    "\n",
    "        context2 = torch.sum(context,2)\n",
    "        \n",
    "        \n",
    "        return context2\n",
    "    \n",
    "#     def attention2(self, hidden_to_attn, encoder_outputs, source_lengths, max_len):\n",
    "#         # repeat the lstm out in third dimension and the encoder outputs in second dimension so we can make a meshgrid\n",
    "#         # so we can do elementwise mul for all possible combinations of h_j and s_i\n",
    "#         h_j = encoder_outputs.unsqueeze(1).repeat(1, hidden_to_attn.size(1), 1, 1)\n",
    "#         s_i = hidden_to_attn.unsqueeze(2).repeat(1, 1, encoder_outputs.size(1), 1)\n",
    "\n",
    "#         # get the dot product between the two to get the energy\n",
    "#         # the unsqueezes are there to emulate transposing. so we can use matmul as torch.dot doesnt accept matrices\n",
    "#         energy = s_i.unsqueeze(3).matmul(h_j.unsqueeze(4)).squeeze(4)\n",
    "\n",
    "#         #         # this is concat attention, its a different form then the ones we need\n",
    "#         #         cat = torch.cat((s_i,h_j),3)\n",
    "\n",
    "#         #         energy = self.attn_layer(cat)\n",
    "\n",
    "#         # reshaping the encoder outputs for later\n",
    "#         encoder_outputs = encoder_outputs.unsqueeze(1)\n",
    "#         encoder_outputs = encoder_outputs.repeat(1, energy.size(1), 1, 1)\n",
    "\n",
    "#         # apply softmax to the energys\n",
    "#         allignment = self.attn_soft(energy)\n",
    "\n",
    "#         # create a mask like : [1,1,1,0,0,0] whos goal is to multiply the attentions of the pads with 0, rest with 1\n",
    "\n",
    "#         idxes = torch.arange(0, max_len).unsqueeze(0).long()\n",
    "\n",
    "#         if torch.cuda.is_available():\n",
    "#             idxes = idxes.cuda()\n",
    "\n",
    "#         mask = Variable((idxes < source_lengths.unsqueeze(1)).float())\n",
    "\n",
    "#         # format the mask to be same size() as the attentions\n",
    "#         mask = mask.unsqueeze(1).unsqueeze(3).repeat(1, allignment.size(1), 1, 1)\n",
    "\n",
    "#         # apply mask\n",
    "#         masked = allignment * mask\n",
    "\n",
    "#         # now we have to rebalance the other values so they sum to 1 again\n",
    "#         # this is done by dividing each value by the sum of the sequence\n",
    "#         # calculate sums\n",
    "#         msum = masked.sum(-2).repeat(1, 1, masked.size(2)).unsqueeze(3)\n",
    "\n",
    "#         # rebalance\n",
    "#         attentions = masked.div(msum)\n",
    "\n",
    "#         # now we shape the attentions to be similar to context in size\n",
    "#         allignment = allignment.repeat(1, 1, 1, encoder_outputs.size(3))\n",
    "\n",
    "#         # make context vector by element wise mul\n",
    "#         context = attentions * encoder_outputs\n",
    "\n",
    "#         context2 = torch.sum(context, 2)\n",
    "\n",
    "#         return context2\n",
    "    \n",
    "#     def decode(self, decoder2vocab):\n",
    "#         # Turn decoder output into a probabiltiy distribution over vocabulary\n",
    "#         decoder2vocab_reshape = decoder2vocab.view(-1, decoder2vocab.size(2))\n",
    "#         word_probs = F.softmax(decoder2vocab_reshape)\n",
    "#         word_probs = word_probs.view(\n",
    "#             decoder2vocab.size(0), decoder2vocab.size(1), decoder2vocab.size(2)\n",
    "#         )\n",
    "\n",
    "#         return word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, src_emb_dim,\n",
    "        src_vocab_size,\n",
    "        src_hidden_dim,\n",
    "        pad_token_src,\n",
    "        drop,\n",
    "        position_based=True,\n",
    "        LSTM_instead=False,\n",
    "        bidirectional=False,\n",
    "        nlayers_src=1\n",
    "        \n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.src_emb_dim = src_emb_dim\n",
    "        self.src_hidden_dim = src_hidden_dim\n",
    "        self.pad_token_src = pad_token_src\n",
    "        self.bidirectional = bidirectional\n",
    "        self.nlayers_src = nlayers_src\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.LSTM_instead=LSTM_instead\n",
    "        self.position_based=position_based\n",
    " \n",
    "        \n",
    "        \n",
    "        # Word Embedding look-up table for the soruce language\n",
    "        self.src_embedding = nn.Embedding(\n",
    "            self.src_vocab_size,\n",
    "            self.src_emb_dim,\n",
    "            self.pad_token_src,\n",
    "        )\n",
    "        self.pos_embedding = nn.Embedding(\n",
    "            self.src_vocab_size,\n",
    "            self.src_emb_dim,\n",
    "            0,\n",
    "        )\n",
    "#         self.scaler = nn.Linear(\n",
    "#             self.src_hidden_dim,\n",
    "#             self.src_emb_dim*2,\n",
    "#         )\n",
    "        self.scale_h0 = nn.Linear(\n",
    "            self.src_emb_dim*2, self.src_hidden_dim\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Encoder GRU\n",
    "        self.encoder = nn.GRU(\n",
    "            self.src_emb_dim // 2 if self.bidirectional else self.src_emb_dim,\n",
    "            self.src_hidden_dim,\n",
    "            self.nlayers_src,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_src, src_lengths, positions):\n",
    "        src_emb = self.src_embedding(input_src) # BxSxE\n",
    "        \n",
    "        if (not self.position_based):\n",
    "            src_emb = self.dropout(src_emb)\n",
    "            src_emb = pack_padded_sequence(src_emb, src_lengths, batch_first=True)\n",
    "            packed_output , src_h_t = self.encoder(src_emb) # out:\n",
    "            h_t = torch.cat((src_h_t[-1], src_h_t[-2]), 1) if self.bidirectional else src_h_t[-1] # BxH\n",
    "            out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            #out = self.scaler(out)\n",
    "        else:\n",
    "            src_pos = self.pos_embedding(positions)\n",
    "            out = torch.cat((src_pos,src_emb),2)\n",
    "            out = self.dropout(out)\n",
    "            hidden = torch.mean(out,1)\n",
    "            h_t = self.scale_h0(hidden)\n",
    "            h_t = self.relu(h_t)\n",
    "            \n",
    "            \n",
    "        # out = BxSxH\n",
    "        return out, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def  get_parallel_minibatchget_par (lines, src_word2id, trg_word2id, index, batch_size, volatile=False):\n",
    "        \n",
    "#         # Get source sentences for this minibatch\n",
    "#         src_lines = [\n",
    "#             ['<s>'] + list(line[1]) + ['</s>']\n",
    "#             for line in lines[index: index + batch_size]\n",
    "#         ]\n",
    "\n",
    "#         # Get target sentences for this minibatch\n",
    "#         trg_lines = [\n",
    "#             ['<s>'] + line[0].split() + ['</s>']\n",
    "#             for line in lines[index: index + batch_size]\n",
    "#         ]\n",
    "        \n",
    "#         # Sort source sentences by length for length masking in RNNs\n",
    "#         src_lens = [len(line) for line in src_lines]\n",
    "#         sorted_indices = np.argsort(src_lens)[::-1]\n",
    "        \n",
    "#         # Reorder sentences based on source lengths\n",
    "#         sorted_src_lines = [src_lines[idx] for idx in sorted_indices]\n",
    "#         sorted_trg_lines = [trg_lines[idx] for idx in sorted_indices]\n",
    "        \n",
    "#         # Compute new sentence lengths\n",
    "#         sorted_src_lens = [len(line) for line in sorted_src_lines]\n",
    "#         sorted_trg_lens = [len(line) for line in sorted_trg_lines]\n",
    "        \n",
    "#         # Get max source and target lengths to pad input and output sequences\n",
    "#         max_src_len = max(sorted_src_lens)\n",
    "#         max_trg_len = max(sorted_trg_lens)\n",
    "        \n",
    "#         # Construct padded source input sequence\n",
    "#         input_lines_src = [\n",
    "#             [src_word2id[w] if w in src_word2id else src_word2id['<unk>'] for w in line] +\n",
    "#             [src_word2id['<pad>']] * (max_src_len - len(line))\n",
    "#             for line in sorted_src_lines\n",
    "#         ]\n",
    "\n",
    "#         # Construct padded target input sequence\n",
    "#         input_lines_trg = [\n",
    "#             [trg_word2id[w] if w in trg_word2id else trg_word2id['<unk>'] for w in line[:-1]] +\n",
    "#             [trg_word2id['<pad>']] * (max_trg_len - len(line))\n",
    "#             for line in sorted_trg_lines\n",
    "#         ]\n",
    "\n",
    "#         # Construct padded target output sequence (Note: Output sequence is just the input shifted by 1 position)\n",
    "#         # This is for teacher-forcing\n",
    "#         output_lines_trg = [\n",
    "#             [trg_word2id[w] if w in trg_word2id else trg_word2id['<unk>'] for w in line[1:]] +\n",
    "#             [trg_word2id['<pad>']] * (max_trg_len - len(line))\n",
    "#             for line in sorted_trg_lines\n",
    "#         ]\n",
    "\n",
    "#         input_lines_src = Variable(torch.LongTensor(input_lines_src), volatile=volatile)\n",
    "#         input_lines_trg = Variable(torch.LongTensor(input_lines_trg), volatile=volatile)\n",
    "#         output_lines_trg = Variable(torch.LongTensor(output_lines_trg), volatile=volatile)\n",
    "\n",
    "#         return {\n",
    "#             'input_src': input_lines_src,\n",
    "#             'input_trg': input_lines_trg,\n",
    "#             'output_trg': output_lines_trg,\n",
    "#             'src_lens': sorted_src_lens\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(training_set, batch_size=batch_size)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(\n",
    "    trg_emb_dim=embedding_dim,\n",
    "    trg_vocab_size=training_set.target_vocab_size,\n",
    "    trg_hidden_dim=hidden_dim,\n",
    "    pad_token_trg=training_set.target_pad,\n",
    "    drop=drop,\n",
    "    bidirectional=bidirectional,\n",
    "    context=False\n",
    ")\n",
    "\n",
    "encoder = Encoder(\n",
    "    src_emb_dim=embedding_dim,\n",
    "    src_vocab_size=training_set.source_vocab_size,\n",
    "    src_hidden_dim=hidden_dim,\n",
    "    pad_token_src=training_set.source_pad,\n",
    "    drop=drop,\n",
    "    bidirectional=bidirectional,\n",
    "    position_based=position_based\n",
    ")\n",
    "\n",
    "\n",
    "if cuda_available:\n",
    "    seq2seq = seq2seq.cuda()\n",
    "    encoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(seq2seq.parameters()) + list(encoder.parameters()), lr=learning_rate)\n",
    "weight_mask = torch.ones(training_set.target_vocab_size)\n",
    "if cuda_available:\n",
    "    weight_mask = weight_mask.cuda()\n",
    "weight_mask[training_set.target_pad] = 0\n",
    "loss_criterion = nn.CrossEntropyLoss(weight=weight_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {\"train_loss\":[],\n",
    "               \"val_loss\":[], \n",
    "               \"test_loss\":[],\n",
    "               \"best_epoch\" : 0, \n",
    "               \"best_val_loss\" : 999,\n",
    "               \"num_epochs\" : num_epochs,\n",
    "               \"batch_size\" : batch_size,\n",
    "               \"learning_rate\" : learning_rate,\n",
    "               \"embedding_dim\" : embedding_dim,\n",
    "               \"hidden_dim\" : hidden_dim,\n",
    "               #hidden_dim = 2 * embedding_dim,\n",
    "               \"max_allowed_sentence_len\" : max_allowed_sentence_len,\n",
    "               \"drop\" : drop,\n",
    "               #force = 1,\n",
    "               \"bidirectional\" : bidirectional,\n",
    "               \"LSTM_instead\" : LSTM_instead,\n",
    "               \"volatile\" : volatile,\n",
    "               \"position_based\" : position_based,\n",
    "               \"context\" : context \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001/020] Batch 000453/000453 [0.3/s] 384.30828619003296\n",
      "Epoch : 0 Training Loss : 4.033\n",
      "Epoch : 0 Dev Loss : 7.792\n",
      "Epoch : 0 Test Loss : 7.870\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "iterations = len(training_loader)\n",
    "for epoch in range(0, 1):\n",
    "    losses = []\n",
    "    start = time.time()\n",
    "    \n",
    "    batch = 0\n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in training_loader:\n",
    "        batch_start = time.time()\n",
    "        force = True\n",
    "        \n",
    "        source_batch = Variable(source_batch, volatile=volatile)\n",
    "        output_batch = Variable(target_batch[:,1:], volatile=volatile)\n",
    "        target_batch = Variable(target_batch[:,:-1], volatile=volatile)\n",
    "        batch_positions = Variable(batch_positions, volatile=volatile)\n",
    "        #print(target_batch.size())\n",
    "        #print(output_batch.size())\n",
    "        \n",
    "        if cuda_available:\n",
    "            source_batch.cuda()\n",
    "            source_batch.cuda()\n",
    "            output_batch.cuda()\n",
    "            \n",
    "        encoder_out, h_t = encoder(\n",
    "            input_src=source_batch, src_lengths=source_lengths, positions=batch_positions,\n",
    "        )    \n",
    "            \n",
    "        decoder_out = seq2seq(\n",
    "            encoder_out=encoder_out, h_t=h_t,\n",
    "            input_trg=target_batch,source_lengths=source_lengths, teacher=force\n",
    "        )\n",
    "        \n",
    "#         print(decoder_out.contiguous().view(-1, decoder_out.size(2)).size())\n",
    "#         print(output_batch.contiguous().view(-1).size())\n",
    "        \n",
    "        loss = loss_criterion(\n",
    "            decoder_out.contiguous().view(-1, decoder_out.size(2)),\n",
    "            output_batch.contiguous().view(-1)\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm(seq2seq.parameters(), 5.)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        print('\\r[Epoch {:03d}/{:03d}] Batch {:06d}/{:06d} [{:.1f}/s] '.format(epoch+1, num_epochs, batch+1, iterations, batch_time), end='')\n",
    "        batch +=1\n",
    "    dev_nll = []\n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in validation_loader:\n",
    "        force = False\n",
    "        \n",
    "        source_batch = Variable(source_batch, volatile=volatile)\n",
    "        output_batch = Variable(target_batch[:,1:], volatile=volatile)\n",
    "        target_batch = Variable(target_batch[:,:-1], volatile=volatile)\n",
    "        #print(target_batch.size())\n",
    "        #print(output_batch.size())\n",
    "        \n",
    "        if cuda_available:\n",
    "            source_batch.cuda()\n",
    "            source_batch.cuda()\n",
    "            output_batch.cuda()\n",
    "            \n",
    "        encoder_out, h_t = encoder(\n",
    "            input_src=source_batch, src_lengths=source_lengths, positions=batch_positions, \n",
    "        )    \n",
    "            \n",
    "        decoder_out = seq2seq(\n",
    "            encoder_out=encoder_out, h_t=h_t,\n",
    "            input_trg=target_batch,source_lengths=source_lengths, teacher=force\n",
    "        )\n",
    "        \n",
    "        #print(decoder_out.contiguous().view(-1, decoder_out.size(2)).size())\n",
    "        #print(output_batch.contiguous().view(-1).size())\n",
    "        \n",
    "        loss = loss_criterion(\n",
    "            decoder_out.contiguous().view(-1, decoder_out.size(2)),\n",
    "            output_batch.contiguous().view(-1)\n",
    "        )\n",
    "        dev_nll.append(loss.item())\n",
    "        \n",
    "        \n",
    "    test_nll = []    \n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in test_loader:\n",
    "        force = False\n",
    "        \n",
    "        source_batch = Variable(source_batch, volatile=volatile)\n",
    "        output_batch = Variable(target_batch[:,1:], volatile=volatile)\n",
    "        target_batch = Variable(target_batch[:,:-1], volatile=volatile)\n",
    "        #print(target_batch.size())\n",
    "        #print(output_batch.size())\n",
    "        \n",
    "        if cuda_available:\n",
    "            source_batch.cuda()\n",
    "            source_batch.cuda()\n",
    "            output_batch.cuda()\n",
    "            \n",
    "        encoder_out, h_t = encoder(\n",
    "            input_src=source_batch, src_lengths=source_lengths, positions=batch_positions, \n",
    "        )    \n",
    "            \n",
    "        decoder_out = seq2seq(\n",
    "            encoder_out=encoder_out, h_t=h_t,\n",
    "            input_trg=target_batch,source_lengths=source_lengths, teacher=force\n",
    "        )\n",
    "        \n",
    "        #print(decoder_out.contiguous().view(-1, decoder_out.size(2)).size())\n",
    "        #print(output_batch.contiguous().view(-1).size())\n",
    "        \n",
    "        loss = loss_criterion(\n",
    "            decoder_out.contiguous().view(-1, decoder_out.size(2)),\n",
    "            output_batch.contiguous().view(-1)\n",
    "        )\n",
    "        test_nll.append(loss.item())\n",
    "    \n",
    "    output_dict[\"train_loss\"].append(np.mean(losses))\n",
    "    output_dict[\"val_loss\"].append(np.mean(dev_nll))\n",
    "    output_dict[\"test_loss\"].append(np.mean(test_nll))\n",
    "    \n",
    "    if np.mean(dev_nll) < output_dict[\"best_val_loss\"]:\n",
    "        \n",
    "        output_dict[\"best_epoch\"] = epoch\n",
    "        output_dict[\"best_val_loss\"] = np.mean(dev_nll)\n",
    "        torch.save(seq2seq, \"{}{}_{}.model\".format(save_dir, seq2seq.__class__.__name__.lower(), name))\n",
    "        torch.save(encoder, \"{}{}_{}.model\".format(save_dir, encoder.__class__.__name__.lower(), name))\n",
    "    \n",
    "    \n",
    "    print(time.time() - start)\n",
    "    print('Epoch : %d Training Loss : %.3f' % (epoch, np.mean(losses)))\n",
    "    print('Epoch : %d Dev Loss : %.3f' % (epoch, np.mean(dev_nll)))\n",
    "    print('Epoch : %d Test Loss : %.3f' % (epoch, np.mean(test_nll)))\n",
    "    print('-------------------------------------------------------------')\n",
    "\n",
    "np.save( \"{}{}.npy\".format(save_dir2, name),output_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(seq2seq, \"decoder_cpu.model\")\n",
    "torch.save(encoder, \"encoder_cpu.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sentence = torch.max(decoder_out[0],1)\n",
    "test_pred = [training_set.target_i2w[word] for word in sentence.cpu().numpy()]\n",
    "print(test_pred)\n",
    "test_real = [training_set.target_i2w[word] for word in output_batch[0].cpu().numpy()]\n",
    "print(test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_dict = {\"train_loss\":[], \"val_loss\":[], \"test_loss\":[], \"best_epoch\" : 0, \"best_val_loss\" : 999}\n",
    "output_dict[\"best_epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict[\"train_loss\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
