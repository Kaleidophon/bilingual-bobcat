{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from corpus import ParallelCorpus\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5861ada2-57c8-4611-a9e3-60d7fc438ea2\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 4e-4\n",
    "embedding_dim = 256\n",
    "hidden_dim = embedding_dim*2\n",
    "#hidden_dim = 2 * embedding_dim\n",
    "max_allowed_sentence_len = 50\n",
    "drop = 0.2\n",
    "#force = 1\n",
    "bidirectional = False\n",
    "LSTM_instead = False\n",
    "volatile = False\n",
    "position_based = False\n",
    "context = True\n",
    "force = 0.5\n",
    "\n",
    "name= str(uuid.uuid4())\n",
    "print(name)\n",
    "save_dir = \"models/\"\n",
    "save_dir2 = \"dicts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE: CORRECT DATA LOCATIONS.\n",
    "\n",
    "training_set = ParallelCorpus(\n",
    "        source_path=\"./data/train/train_bpe5k.fr\", target_path=\"./data/train/train_bpe5k.en\",\n",
    "        max_sentence_length=max_allowed_sentence_len\n",
    "    )\n",
    "\n",
    "validation_set = ParallelCorpus(\n",
    "        source_path=\"./data/val/val_bpe5k.fr\", target_path=\"./data/val/val_bpe5k.en\",\n",
    "        max_sentence_length=max_allowed_sentence_len, use_indices_from=training_set\n",
    "    )\n",
    "\n",
    "test_set = ParallelCorpus(\n",
    "        source_path=\"./data/test/test_2017_flickr_bpe5k.fr\", target_path=\"./data/test/test_2017_flickr_bpe5k.en\",\n",
    "        max_sentence_length=max_allowed_sentence_len, use_indices_from=training_set\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"A Vanilla Sequence to Sequence (Seq2Seq) model with LSTMs.\n",
    "    Ref: Sequence to Sequence Learning with Neural Nets\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, trg_emb_dim,\n",
    "        trg_vocab_size, trg_hidden_dim,\n",
    "        pad_token_trg, drop, context=True,\n",
    "        LSTM_instead=False, bidirectional=False,\n",
    "        nlayers_trg=1,\n",
    "    ):\n",
    "        \"\"\"Initialize Seq2Seq Model.\"\"\"\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.trg_emb_dim = trg_emb_dim\n",
    "        self.trg_hidden_dim = trg_hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.nlayers_trg = nlayers_trg\n",
    "        self.pad_token_trg = pad_token_trg\n",
    "        self.attn_soft = nn.Softmax(dim=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.LSTM_instead=LSTM_instead\n",
    "        self.context = context\n",
    "\n",
    "        # Word Embedding look-up table for the target language\n",
    "        self.trg_embedding = nn.Embedding(\n",
    "            self.trg_vocab_size,\n",
    "            self.trg_emb_dim,\n",
    "            self.pad_token_trg,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Decoder GRU // LSTM\n",
    "        if (not self.LSTM_instead):\n",
    "        \n",
    "            self.decoder = nn.GRU(\n",
    "                self.trg_emb_dim,\n",
    "                self.trg_hidden_dim,\n",
    "                self.nlayers_trg,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:\n",
    "            self.decoder = nn.LSTM(\n",
    "                self.trg_emb_dim,\n",
    "                self.trg_hidden_dim,\n",
    "                self.nlayers_trg,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "#         self.scaler = nn.Linear(\n",
    "#             self.trg_hidden_dim,\n",
    "#             self.trg_emb_dim*2,\n",
    "#         )\n",
    "        \n",
    "#         self.scaler2 = nn.Linear(\n",
    "#             self.trg_hidden_dim,\n",
    "#             self.trg_emb_dim,\n",
    "#         )\n",
    "        \n",
    "        # Projection layer from decoder hidden states to target language vocabulary\n",
    "        \n",
    "        if (not self.context):\n",
    "            self.decoder2vocab = nn.Linear(trg_hidden_dim, trg_vocab_size)\n",
    "        else:\n",
    "            self.decoder2vocab = nn.Linear(trg_hidden_dim + trg_emb_dim*2, trg_vocab_size)\n",
    "\n",
    "    def forward(self, encoder_out, h_t, input_trg, source_lengths, teacher ):\n",
    "        trg_emb = self.trg_embedding(input_trg)\n",
    "        trg_emb = self.dropout(trg_emb)\n",
    "        \n",
    "        h_t = h_t.unsqueeze(0).expand(self.nlayers_trg, h_t.size(0), h_t.size(1))\n",
    "        if (self.LSTM_instead):\n",
    "            h_t = (h_t,h_t)\n",
    "        \n",
    "        hidden = []\n",
    "        outputs = []\n",
    "        trg_in = trg_emb[:,0,:].unsqueeze(1)\n",
    "        for i in range(input_trg.size(1)):\n",
    "#             print( \" \")\n",
    "            if (teacher):\n",
    "                trg_in = trg_emb[:,i,:].unsqueeze(1)\n",
    "#             print(trg_in.size())\n",
    "            trg_h, h_t = self.decoder(trg_in, h_t)\n",
    "            hidden.append(h_t.squeeze())\n",
    "#             print(h_t.size())\n",
    "\n",
    "            if (self.context):\n",
    "                context = self.attention(h_t.squeeze().unsqueeze(1),encoder_out,source_lengths, encoder_out.size(1))\n",
    "                trg_h = torch.cat((trg_h,context),2)\n",
    "            \n",
    "            trg_h_reshape = trg_h.contiguous().view(\n",
    "            trg_h.size(0) * trg_h.size(1), trg_h.size(2)\n",
    "            )\n",
    "            # Affine transformation of all decoder hidden states\n",
    "            decoder2vocab = self.decoder2vocab(trg_h_reshape)\n",
    "            # Reshape\n",
    "            decoder2vocab = decoder2vocab.view(\n",
    "                trg_h.size(0), trg_h.size(1), decoder2vocab.size(1)\n",
    "            )\n",
    "\n",
    "        \n",
    "            \n",
    "            outputs.append(decoder2vocab.squeeze())\n",
    "            \n",
    "            if (not teacher):\n",
    "                _, word = torch.max(decoder2vocab,2)\n",
    "                trg_in = self.trg_embedding(word)\n",
    "            \n",
    "        outputs = torch.stack(outputs,1)\n",
    "        return outputs    \n",
    "            \n",
    "            \n",
    "            \n",
    "#             if (not teacher):\n",
    "#                 trg_in = self.scaler2(decoder_out)\n",
    "#                 trg_in = self.relu(trg_in)\n",
    "            \n",
    "        # hiddens = torch.stack(hidden,1)\n",
    "        # trg_h = torch.stack(outputs,1)\n",
    "#         print(trg_h.size())\n",
    "#         print(hiddens.size())\n",
    "        #if torch.cuda.is_available()\n",
    "        #    trg_\n",
    "        #print(asdfs)\n",
    "        \n",
    "            \n",
    "        #hiddens = self.scaler(hiddens) \n",
    "        #hiddens = self.relu(hiddens)\n",
    "#         if (self.context):\n",
    "#             context = self.attention(hiddens,encoder_out,source_lengths, encoder_out.size(1))\n",
    "#             print(hiddens.size())\n",
    "#             print(encoder_out.size())\n",
    "\n",
    "#             trg_h = torch.cat((trg_h,context),2)\n",
    "        \n",
    "        \n",
    "#         # Initialize the decoder GRU with the last hidden state of the encoder and \n",
    "#         # run target inputs through the decoder.\n",
    "        \n",
    "#         # Merge batch and time dimensions to pass to a linear layer\n",
    "#         trg_h_reshape = trg_h.contiguous().view(\n",
    "#             trg_h.size(0) * trg_h.size(1), trg_h.size(2)\n",
    "#         )\n",
    "        \n",
    "#         # Affine transformation of all decoder hidden states\n",
    "#         decoder2vocab = self.decoder2vocab(trg_h_reshape)\n",
    "        \n",
    "#         # Reshape\n",
    "#         decoder2vocab = decoder2vocab.view(\n",
    "#             trg_h.size(0), trg_h.size(1), decoder2vocab.size(1)\n",
    "#         )\n",
    "    \n",
    "    \n",
    "    def attention(self, hidden_to_attn, encoder_outputs, source_lengths, max_len):\n",
    "                \n",
    "        \n",
    "#         print(hidden_to_attn.size())\n",
    "#         print(encoder_outputs.size())\n",
    "        # repeat the lstm out in third dimension and the encoder outputs in second dimension so we can make a meshgrid\n",
    "        # so we can do elementwise mul for all possible combinations of h_j and s_i\n",
    "        h_j = encoder_outputs.unsqueeze(1).repeat(1,hidden_to_attn.size(1),1,1)\n",
    "        s_i = hidden_to_attn.unsqueeze(2).repeat(1,1,encoder_outputs.size(1),1)\n",
    "#         print(h_j.size())\n",
    "#         print(s_i.size())\n",
    "        # get the dot product between the two to get the energy\n",
    "        # the unsqueezes are there to emulate transposing. so we can use matmul as torch.dot doesnt accept matrices\n",
    "        energy = s_i.unsqueeze(3).matmul(h_j.unsqueeze(4)).squeeze(4)\n",
    "        \n",
    "#         # this is concat attention, its a different form then the ones we need\n",
    "#         cat = torch.cat((s_i,h_j),3)\n",
    "        \n",
    "#         energy = self.attn_layer(cat)\n",
    "\n",
    "        # reshaping the encoder outputs for later\n",
    "        encoder_outputs = encoder_outputs.unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.repeat(1,energy.size(1),1,1)\n",
    "    \n",
    "        # apply softmax to the energys \n",
    "        allignment = self.attn_soft(energy)\n",
    "        \n",
    "        # create a mask like : [1,1,1,0,0,0] whos goal is to multiply the attentions of the pads with 0, rest with 1\n",
    "        idxes = torch.arange(0,max_len).unsqueeze(0).long().cuda()\n",
    "        #print(idxes.size())\n",
    "        mask = Variable((idxes<source_lengths.unsqueeze(1)).float())\n",
    "        \n",
    "        # format the mask to be same size() as the attentions\n",
    "        mask = mask.unsqueeze(1).unsqueeze(3).repeat(1,allignment.size(1),1,1)\n",
    "        \n",
    "        # apply mask\n",
    "        masked = allignment * mask\n",
    "        \n",
    "        # now we have to rebalance the other values so they sum to 1 again\n",
    "        # this is done by dividing each value by the sum of the sequence\n",
    "        # calculate sums\n",
    "        msum = masked.sum(-2).repeat(1,1,masked.size(2)).unsqueeze(3)\n",
    "        \n",
    "        # rebalance\n",
    "        attentions = masked.div(msum)\n",
    "        \n",
    "        # now we shape the attentions to be similar to context in size\n",
    "        allignment = allignment.repeat(1,1,1,encoder_outputs.size(3))\n",
    "\n",
    "        # make context vector by element wise mul\n",
    "        context = attentions * encoder_outputs\n",
    "        \n",
    "\n",
    "        context2 = torch.sum(context,2)\n",
    "        \n",
    "        \n",
    "        return context2\n",
    "    \n",
    "#     def attention2(self, hidden_to_attn, encoder_outputs, source_lengths, max_len):\n",
    "#         # repeat the lstm out in third dimension and the encoder outputs in second dimension so we can make a meshgrid\n",
    "#         # so we can do elementwise mul for all possible combinations of h_j and s_i\n",
    "#         h_j = encoder_outputs.unsqueeze(1).repeat(1, hidden_to_attn.size(1), 1, 1)\n",
    "#         s_i = hidden_to_attn.unsqueeze(2).repeat(1, 1, encoder_outputs.size(1), 1)\n",
    "\n",
    "#         # get the dot product between the two to get the energy\n",
    "#         # the unsqueezes are there to emulate transposing. so we can use matmul as torch.dot doesnt accept matrices\n",
    "#         energy = s_i.unsqueeze(3).matmul(h_j.unsqueeze(4)).squeeze(4)\n",
    "\n",
    "#         #         # this is concat attention, its a different form then the ones we need\n",
    "#         #         cat = torch.cat((s_i,h_j),3)\n",
    "\n",
    "#         #         energy = self.attn_layer(cat)\n",
    "\n",
    "#         # reshaping the encoder outputs for later\n",
    "#         encoder_outputs = encoder_outputs.unsqueeze(1)\n",
    "#         encoder_outputs = encoder_outputs.repeat(1, energy.size(1), 1, 1)\n",
    "\n",
    "#         # apply softmax to the energys\n",
    "#         allignment = self.attn_soft(energy)\n",
    "\n",
    "#         # create a mask like : [1,1,1,0,0,0] whos goal is to multiply the attentions of the pads with 0, rest with 1\n",
    "\n",
    "#         idxes = torch.arange(0, max_len).unsqueeze(0).long()\n",
    "\n",
    "#         if torch.cuda.is_available():\n",
    "#             idxes = idxes.cuda()\n",
    "\n",
    "#         mask = Variable((idxes < source_lengths.unsqueeze(1)).float())\n",
    "\n",
    "#         # format the mask to be same size() as the attentions\n",
    "#         mask = mask.unsqueeze(1).unsqueeze(3).repeat(1, allignment.size(1), 1, 1)\n",
    "\n",
    "#         # apply mask\n",
    "#         masked = allignment * mask\n",
    "\n",
    "#         # now we have to rebalance the other values so they sum to 1 again\n",
    "#         # this is done by dividing each value by the sum of the sequence\n",
    "#         # calculate sums\n",
    "#         msum = masked.sum(-2).repeat(1, 1, masked.size(2)).unsqueeze(3)\n",
    "\n",
    "#         # rebalance\n",
    "#         attentions = masked.div(msum)\n",
    "\n",
    "#         # now we shape the attentions to be similar to context in size\n",
    "#         allignment = allignment.repeat(1, 1, 1, encoder_outputs.size(3))\n",
    "\n",
    "#         # make context vector by element wise mul\n",
    "#         context = attentions * encoder_outputs\n",
    "\n",
    "#         context2 = torch.sum(context, 2)\n",
    "\n",
    "#         return context2\n",
    "    \n",
    "#     def decode(self, decoder2vocab):\n",
    "#         # Turn decoder output into a probabiltiy distribution over vocabulary\n",
    "#         decoder2vocab_reshape = decoder2vocab.view(-1, decoder2vocab.size(2))\n",
    "#         word_probs = F.softmax(decoder2vocab_reshape)\n",
    "#         word_probs = word_probs.view(\n",
    "#             decoder2vocab.size(0), decoder2vocab.size(1), decoder2vocab.size(2)\n",
    "#         )\n",
    "\n",
    "#         return word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, src_emb_dim,\n",
    "        src_vocab_size,\n",
    "        src_hidden_dim,\n",
    "        pad_token_src,\n",
    "        drop,\n",
    "        position_based=True,\n",
    "        LSTM_instead=False,\n",
    "        bidirectional=False,\n",
    "        nlayers_src=1\n",
    "        \n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.src_emb_dim = src_emb_dim\n",
    "        self.src_hidden_dim = src_hidden_dim\n",
    "        self.pad_token_src = pad_token_src\n",
    "        self.bidirectional = bidirectional\n",
    "        self.nlayers_src = nlayers_src\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.LSTM_instead=LSTM_instead\n",
    "        self.position_based=position_based\n",
    " \n",
    "        \n",
    "        \n",
    "        # Word Embedding look-up table for the soruce language\n",
    "        self.src_embedding = nn.Embedding(\n",
    "            self.src_vocab_size,\n",
    "            self.src_emb_dim,\n",
    "            self.pad_token_src,\n",
    "        )\n",
    "        self.pos_embedding = nn.Embedding(\n",
    "            self.src_vocab_size,\n",
    "            self.src_emb_dim,\n",
    "            0,\n",
    "        )\n",
    "#         self.scaler = nn.Linear(\n",
    "#             self.src_hidden_dim,\n",
    "#             self.src_emb_dim*2,\n",
    "#         )\n",
    "        self.scale_h0 = nn.Linear(\n",
    "            self.src_emb_dim*2, self.src_hidden_dim\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Encoder GRU\n",
    "        self.encoder = nn.GRU(\n",
    "            self.src_emb_dim // 2 if self.bidirectional else self.src_emb_dim,\n",
    "            self.src_hidden_dim,\n",
    "            self.nlayers_src,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_src, src_lengths, positions):\n",
    "        src_emb = self.src_embedding(input_src) # BxSxE\n",
    "        \n",
    "        if (not self.position_based):\n",
    "            src_emb = self.dropout(src_emb)\n",
    "            src_emb = pack_padded_sequence(src_emb, src_lengths, batch_first=True)\n",
    "            packed_output , src_h_t = self.encoder(src_emb) # out:\n",
    "            h_t = torch.cat((src_h_t[-1], src_h_t[-2]), 1) if self.bidirectional else src_h_t[-1] # BxH\n",
    "            out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            #out = self.scaler(out)\n",
    "        else:\n",
    "            src_pos = self.pos_embedding(positions)\n",
    "            out = torch.cat((src_pos,src_emb),2)\n",
    "            out = self.dropout(out)\n",
    "            hidden = torch.mean(out,1)\n",
    "            h_t = self.scale_h0(hidden)\n",
    "            h_t = self.relu(h_t)\n",
    "            \n",
    "            \n",
    "        # out = BxSxH\n",
    "        return out, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def  get_parallel_minibatchget_par (lines, src_word2id, trg_word2id, index, batch_size, volatile=False):\n",
    "        \n",
    "#         # Get source sentences for this minibatch\n",
    "#         src_lines = [\n",
    "#             ['<s>'] + list(line[1]) + ['</s>']\n",
    "#             for line in lines[index: index + batch_size]\n",
    "#         ]\n",
    "\n",
    "#         # Get target sentences for this minibatch\n",
    "#         trg_lines = [\n",
    "#             ['<s>'] + line[0].split() + ['</s>']\n",
    "#             for line in lines[index: index + batch_size]\n",
    "#         ]\n",
    "        \n",
    "#         # Sort source sentences by length for length masking in RNNs\n",
    "#         src_lens = [len(line) for line in src_lines]\n",
    "#         sorted_indices = np.argsort(src_lens)[::-1]\n",
    "        \n",
    "#         # Reorder sentences based on source lengths\n",
    "#         sorted_src_lines = [src_lines[idx] for idx in sorted_indices]\n",
    "#         sorted_trg_lines = [trg_lines[idx] for idx in sorted_indices]\n",
    "        \n",
    "#         # Compute new sentence lengths\n",
    "#         sorted_src_lens = [len(line) for line in sorted_src_lines]\n",
    "#         sorted_trg_lens = [len(line) for line in sorted_trg_lines]\n",
    "        \n",
    "#         # Get max source and target lengths to pad input and output sequences\n",
    "#         max_src_len = max(sorted_src_lens)\n",
    "#         max_trg_len = max(sorted_trg_lens)\n",
    "        \n",
    "#         # Construct padded source input sequence\n",
    "#         input_lines_src = [\n",
    "#             [src_word2id[w] if w in src_word2id else src_word2id['<unk>'] for w in line] +\n",
    "#             [src_word2id['<pad>']] * (max_src_len - len(line))\n",
    "#             for line in sorted_src_lines\n",
    "#         ]\n",
    "\n",
    "#         # Construct padded target input sequence\n",
    "#         input_lines_trg = [\n",
    "#             [trg_word2id[w] if w in trg_word2id else trg_word2id['<unk>'] for w in line[:-1]] +\n",
    "#             [trg_word2id['<pad>']] * (max_trg_len - len(line))\n",
    "#             for line in sorted_trg_lines\n",
    "#         ]\n",
    "\n",
    "#         # Construct padded target output sequence (Note: Output sequence is just the input shifted by 1 position)\n",
    "#         # This is for teacher-forcing\n",
    "#         output_lines_trg = [\n",
    "#             [trg_word2id[w] if w in trg_word2id else trg_word2id['<unk>'] for w in line[1:]] +\n",
    "#             [trg_word2id['<pad>']] * (max_trg_len - len(line))\n",
    "#             for line in sorted_trg_lines\n",
    "#         ]\n",
    "\n",
    "#         input_lines_src = Variable(torch.LongTensor(input_lines_src), volatile=volatile)\n",
    "#         input_lines_trg = Variable(torch.LongTensor(input_lines_trg), volatile=volatile)\n",
    "#         output_lines_trg = Variable(torch.LongTensor(output_lines_trg), volatile=volatile)\n",
    "\n",
    "#         return {\n",
    "#             'input_src': input_lines_src,\n",
    "#             'input_trg': input_lines_trg,\n",
    "#             'output_trg': output_lines_trg,\n",
    "#             'src_lens': sorted_src_lens\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_loader = DataLoader(training_set, batch_size=batch_size)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(\n",
    "    trg_emb_dim=embedding_dim,\n",
    "    trg_vocab_size=training_set.target_vocab_size,\n",
    "    trg_hidden_dim=hidden_dim,\n",
    "    pad_token_trg=training_set.target_pad,\n",
    "    drop=drop,\n",
    "    bidirectional=bidirectional,\n",
    "    context=context\n",
    ")\n",
    "\n",
    "encoder = Encoder(\n",
    "    src_emb_dim=embedding_dim,\n",
    "    src_vocab_size=training_set.source_vocab_size,\n",
    "    src_hidden_dim=hidden_dim,\n",
    "    pad_token_src=training_set.source_pad,\n",
    "    drop=drop,\n",
    "    bidirectional=bidirectional,\n",
    "    position_based=position_based\n",
    ")\n",
    "\n",
    "\n",
    "if cuda_available:\n",
    "    seq2seq = seq2seq.cuda()\n",
    "    encoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(seq2seq.parameters()) + list(encoder.parameters()), lr=learning_rate)\n",
    "weight_mask = torch.ones(training_set.target_vocab_size)\n",
    "if cuda_available:\n",
    "    weight_mask = weight_mask.cuda()\n",
    "weight_mask[training_set.target_pad] = 0\n",
    "loss_criterion = nn.CrossEntropyLoss(weight=weight_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dict = {\"train_loss\":[],\n",
    "               \"val_loss\":[], \n",
    "               \"test_loss\":[],\n",
    "               \"best_epoch\" : 0, \n",
    "               \"best_val_loss\" : 999,\n",
    "               \"num_epochs\" : num_epochs,\n",
    "               \"batch_size\" : batch_size,\n",
    "               \"learning_rate\" : learning_rate,\n",
    "               \"embedding_dim\" : embedding_dim,\n",
    "               \"hidden_dim\" : hidden_dim,\n",
    "               #hidden_dim = 2 * embedding_dim,\n",
    "               \"max_allowed_sentence_len\" : max_allowed_sentence_len,\n",
    "               \"drop\" : drop,\n",
    "               #force = 1,\n",
    "               \"bidirectional\" : bidirectional,\n",
    "               \"LSTM_instead\" : LSTM_instead,\n",
    "               \"volatile\" : volatile,\n",
    "               \"position_based\" : position_based,\n",
    "               \"context\" : context\n",
    "               \"force\" : force\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rasyan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001/010] Batch 000453/000453 [0.2/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rasyan\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\Rasyan\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227.02750492095947\n",
      "Epoch : 1 Training Loss : 3.699\n",
      "Epoch : 1 Dev Loss : 8.195\n",
      "Epoch : 1 Test Loss : 7.962\n",
      "-------------------------------------------------------------\n",
      "[Epoch 002/010] Batch 000453/000453 [0.2/s] 227.58037781715393\n",
      "Epoch : 2 Training Loss : 2.564\n",
      "Epoch : 2 Dev Loss : 7.137\n",
      "Epoch : 2 Test Loss : 7.221\n",
      "-------------------------------------------------------------\n",
      "[Epoch 003/010] Batch 000453/000453 [0.2/s] 225.52018070220947\n",
      "Epoch : 3 Training Loss : 2.135\n",
      "Epoch : 3 Dev Loss : 6.899\n",
      "Epoch : 3 Test Loss : 6.972\n",
      "-------------------------------------------------------------\n",
      "[Epoch 004/010] Batch 000453/000453 [0.2/s] 225.6797547340393\n",
      "Epoch : 4 Training Loss : 1.887\n",
      "Epoch : 4 Dev Loss : 6.641\n",
      "Epoch : 4 Test Loss : 6.862\n",
      "-------------------------------------------------------------\n",
      "[Epoch 005/010] Batch 000453/000453 [0.2/s] 234.45887112617493\n",
      "Epoch : 5 Training Loss : 1.714\n",
      "Epoch : 5 Dev Loss : 6.667\n",
      "Epoch : 5 Test Loss : 6.803\n",
      "-------------------------------------------------------------\n",
      "[Epoch 006/010] Batch 000453/000453 [0.2/s] 232.29530882835388\n",
      "Epoch : 6 Training Loss : 1.585\n",
      "Epoch : 6 Dev Loss : 6.588\n",
      "Epoch : 6 Test Loss : 6.800\n",
      "-------------------------------------------------------------\n",
      "[Epoch 007/010] Batch 000453/000453 [0.2/s] 232.89311909675598\n",
      "Epoch : 7 Training Loss : 1.482\n",
      "Epoch : 7 Dev Loss : 6.693\n",
      "Epoch : 7 Test Loss : 6.872\n",
      "-------------------------------------------------------------\n",
      "[Epoch 008/010] Batch 000453/000453 [0.2/s] 228.3531572818756\n",
      "Epoch : 8 Training Loss : 1.398\n",
      "Epoch : 8 Dev Loss : 6.661\n",
      "Epoch : 8 Test Loss : 6.917\n",
      "-------------------------------------------------------------\n",
      "[Epoch 009/010] Batch 000453/000453 [0.2/s] 232.9705605506897\n",
      "Epoch : 9 Training Loss : 1.325\n",
      "Epoch : 9 Dev Loss : 6.623\n",
      "Epoch : 9 Test Loss : 6.885\n",
      "-------------------------------------------------------------\n",
      "[Epoch 010/010] Batch 000453/000453 [0.2/s] 231.19790697097778\n",
      "Epoch : 10 Training Loss : 1.262\n",
      "Epoch : 10 Dev Loss : 6.618\n",
      "Epoch : 10 Test Loss : 6.876\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "iterations = len(training_loader)\n",
    "for epoch in range(0, num_epochs):\n",
    "    losses = []\n",
    "    start = time.time()\n",
    "    \n",
    "    batch = 0\n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in training_loader:\n",
    "        batch_start = time.time()\n",
    "        use_teacher_forcing = True if random.random() <= force else False\n",
    "        \n",
    "        source_batch = Variable(source_batch, volatile=volatile)\n",
    "        output_batch = Variable(target_batch[:,1:], volatile=volatile)\n",
    "        target_batch = Variable(target_batch[:,:-1], volatile=volatile)\n",
    "        batch_positions = Variable(batch_positions, volatile=volatile)\n",
    "        #print(target_batch.size())\n",
    "        #print(output_batch.size())\n",
    "        \n",
    "        if cuda_available:\n",
    "            source_batch.cuda()\n",
    "            source_batch.cuda()\n",
    "            output_batch.cuda()\n",
    "            \n",
    "        encoder_out, h_t = encoder(\n",
    "            input_src=source_batch, src_lengths=source_lengths, positions=batch_positions,\n",
    "        )    \n",
    "            \n",
    "        decoder_out = seq2seq(\n",
    "            encoder_out=encoder_out, h_t=h_t,\n",
    "            input_trg=target_batch,source_lengths=source_lengths, teacher=use_teacher_forcing\n",
    "        )\n",
    "        \n",
    "#         print(decoder_out.contiguous().view(-1, decoder_out.size(2)).size())\n",
    "#         print(output_batch.contiguous().view(-1).size())\n",
    "        \n",
    "        loss = loss_criterion(\n",
    "            decoder_out.contiguous().view(-1, decoder_out.size(2)),\n",
    "            output_batch.contiguous().view(-1)\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm(seq2seq.parameters(), 5.)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        print('\\r[Epoch {:03d}/{:03d}] Batch {:06d}/{:06d} [{:.1f}/s] '.format(epoch+1, num_epochs, batch+1, iterations, batch_time), end='')\n",
    "        batch +=1\n",
    "    dev_nll = []\n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in validation_loader:\n",
    "        use_teacher_forcing = False\n",
    "        \n",
    "        source_batch = Variable(source_batch, volatile=volatile)\n",
    "        output_batch = Variable(target_batch[:,1:], volatile=volatile)\n",
    "        target_batch = Variable(target_batch[:,:-1], volatile=volatile)\n",
    "        #print(target_batch.size())\n",
    "        #print(output_batch.size())\n",
    "        \n",
    "        if cuda_available:\n",
    "            source_batch.cuda()\n",
    "            source_batch.cuda()\n",
    "            output_batch.cuda()\n",
    "            \n",
    "        encoder_out, h_t = encoder(\n",
    "            input_src=source_batch, src_lengths=source_lengths, positions=batch_positions, \n",
    "        )    \n",
    "            \n",
    "        decoder_out = seq2seq(\n",
    "            encoder_out=encoder_out, h_t=h_t,\n",
    "            input_trg=target_batch,source_lengths=source_lengths, teacher=use_teacher_forcing\n",
    "        )\n",
    "        \n",
    "        #print(decoder_out.contiguous().view(-1, decoder_out.size(2)).size())\n",
    "        #print(output_batch.contiguous().view(-1).size())\n",
    "        \n",
    "        loss = loss_criterion(\n",
    "            decoder_out.contiguous().view(-1, decoder_out.size(2)),\n",
    "            output_batch.contiguous().view(-1)\n",
    "        )\n",
    "        dev_nll.append(loss.item())\n",
    "        \n",
    "        \n",
    "    test_nll = []    \n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in test_loader:\n",
    "        use_teacher_forcing = False\n",
    "        \n",
    "        source_batch = Variable(source_batch, volatile=volatile)\n",
    "        output_batch = Variable(target_batch[:,1:], volatile=volatile)\n",
    "        target_batch = Variable(target_batch[:,:-1], volatile=volatile)\n",
    "        #print(target_batch.size())\n",
    "        #print(output_batch.size())\n",
    "        \n",
    "        if cuda_available:\n",
    "            source_batch.cuda()\n",
    "            source_batch.cuda()\n",
    "            output_batch.cuda()\n",
    "            \n",
    "        encoder_out, h_t = encoder(\n",
    "            input_src=source_batch, src_lengths=source_lengths, positions=batch_positions, \n",
    "        )    \n",
    "            \n",
    "        decoder_out = seq2seq(\n",
    "            encoder_out=encoder_out, h_t=h_t,\n",
    "            input_trg=target_batch,source_lengths=source_lengths, teacher=use_teacher_forcing\n",
    "        )\n",
    "        \n",
    "        #print(decoder_out.contiguous().view(-1, decoder_out.size(2)).size())\n",
    "        #print(output_batch.contiguous().view(-1).size())\n",
    "        \n",
    "        loss = loss_criterion(\n",
    "            decoder_out.contiguous().view(-1, decoder_out.size(2)),\n",
    "            output_batch.contiguous().view(-1)\n",
    "        )\n",
    "        test_nll.append(loss.item())\n",
    "    \n",
    "    output_dict[\"train_loss\"].append(np.mean(losses))\n",
    "    output_dict[\"val_loss\"].append(np.mean(dev_nll))\n",
    "    output_dict[\"test_loss\"].append(np.mean(test_nll))\n",
    "    \n",
    "    if np.mean(dev_nll) < output_dict[\"best_val_loss\"]:\n",
    "        \n",
    "        output_dict[\"best_epoch\"] = epoch\n",
    "        output_dict[\"best_val_loss\"] = np.mean(dev_nll)\n",
    "        torch.save(seq2seq, \"{}{}_{}.model\".format(save_dir, seq2seq.__class__.__name__.lower(), name))\n",
    "        torch.save(encoder, \"{}{}_{}.model\".format(save_dir, encoder.__class__.__name__.lower(), name))\n",
    "    \n",
    "    \n",
    "    print(time.time() - start)\n",
    "    print('Epoch : %d Training Loss : %.3f' % (epoch+1, np.mean(losses)))\n",
    "    print('Epoch : %d Dev Loss : %.3f' % (epoch+1, np.mean(dev_nll)))\n",
    "    print('Epoch : %d Test Loss : %.3f' % (epoch+1, np.mean(test_nll)))\n",
    "    print('-------------------------------------------------------------')\n",
    "\n",
    "np.save( \"{}{}.npy\".format(save_dir2, name),output_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'person', 'is', 'sleeping', 'in', 'the', 'street', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
      "['a', 'person', 'sleeping', 'on', 'the', 'street', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "_, sentence = torch.max(decoder_out[3],1)\n",
    "test_pred = [training_set.target_i2w[word] for word in sentence.cpu().numpy()]\n",
    "print(test_pred)\n",
    "test_real = [training_set.target_i2w[word] for word in output_batch[3].cpu().numpy()]\n",
    "print(test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, eval_set, target_path, reference_file_path):\n",
    "    data_loader = DataLoader(eval_set, batch_size=5)\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    idx2word = eval_set.target_i2w\n",
    "    sorted_sentence_ids = eval_set.target_sentence_ids.cpu().numpy()\n",
    "    translated_sentences = []\n",
    "\n",
    "    # Decode\n",
    "    # for source_batch, target_batch, source_lengths, target_lengths, batch_positions in data_loader:\n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in data_loader:\n",
    "\n",
    "        encoder_out, h_t = encoder(\n",
    "            input_src=source_batch, src_lengths=source_lengths, positions=batch_positions,\n",
    "        )\n",
    "        decoder_out = decoder(\n",
    "            encoder_out=encoder_out, h_t=h_t,\n",
    "            input_trg=target_batch, source_lengths=source_lengths, teacher=False\n",
    "        )\n",
    "\n",
    "        # Get predicted word for every batch instance\n",
    "        normalized_output = softmax(decoder_out)\n",
    "        predictions = normalized_output.max(2)[1]  # Only get indices\n",
    "\n",
    "        for sentence_index in range(predictions.shape[0]):\n",
    "            token_indices = list(predictions[sentence_index, :].numpy())\n",
    "\n",
    "            tokens = list(map(lambda idx: idx2word[idx], token_indices))\n",
    "            eos_index = len(tokens)\n",
    "            if \"<eos>\" in tokens:\n",
    "                eos_index = tokens.index(\"<eos>\")\n",
    "\n",
    "            tokens = tokens[:eos_index]  # Cut off after first end of sentence token\n",
    "\n",
    "            translated_sentence = \" \".join(tokens).replace(\"@@ \", \"\").replace(\"@@\", \"\")\n",
    "            print(translated_sentence)\n",
    "            translated_sentences.append(translated_sentence)\n",
    "\n",
    "    # Bring sentence back into the order they were in the test set\n",
    "    resorted_sentences = [None] * len(translated_sentences)\n",
    "    for target_id, sentence in zip(sorted_sentence_ids, translated_sentences):\n",
    "        resorted_sentences[target_id] = sentence\n",
    "\n",
    "    # Write to file\n",
    "    with codecs.open(target_path, \"wb\", \"utf-8\") as target_file:\n",
    "        for sentence in resorted_sentences:\n",
    "            target_file.write(\"{}\\n\".format(sentence))\n",
    "\n",
    "    out = subprocess.getoutput(\n",
    "        \"perl ./multi-bleu.perl {} < {}\".format(reference_file_path, target_path)\n",
    "    )\n",
    "    print(out[out.index(\"BLEU\"):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'src_lengths'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-1fddff128659>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m evaluate(\n\u001b[0;32m      5\u001b[0m     \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"./eval_out_{}.txt\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mreference_file_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"./data/test/test_2017_flickr_truecased.en\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;32m<ipython-input-16-53d36ae4472d>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(encoder, decoder, eval_set, target_path, reference_file_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         encoder_out, h_t = encoder(\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0minput_src\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_positions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         )\n\u001b[0;32m     15\u001b[0m         decoder_out = decoder(\n",
      "\u001b[1;32mC:\\Users\\Rasyan\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'src_lengths'"
     ]
    }
   ],
   "source": [
    "name = \"2dfa87d0-ace2-4e7f-a25b-23c6de00ba56\"\n",
    "encoder = torch.load(\"{}{}_{}.model\".format(save_dir, \"seq2seq\", name))\n",
    "decoder = torch.load(\"{}{}_{}.model\".format(save_dir, \"encoder\", name))\n",
    "evaluate(\n",
    "    encoder, decoder, test_set, target_path=\"./eval_out_{}.txt\".format(name),\n",
    "    reference_file_path=\"./data/test/test_2017_flickr_truecased.en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dict[\"train_loss\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
