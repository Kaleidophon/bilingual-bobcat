{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5942d515a601>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# count the number of times each word appears\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0men_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0men_corpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import torch.utils.data\n",
    "import time\n",
    "import math\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global paramters\n",
    "batch_size = 37\n",
    "embedding_dims = 128\n",
    "vocab_size_limit = 10000\n",
    "en_file = \"wa/dev.en\"\n",
    "fr_file = \"wa/dev.fr\"\n",
    "num_epochs = 30\n",
    "learning_rate = 0.01\n",
    "max_sentence_len = 50\n",
    "hidden_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preproces corpus\n",
    "en_corpus = []\n",
    "fr_corpus = []\n",
    "en_sentences = []\n",
    "fr_sentences = []\n",
    "allow = string.ascii_letters + \" \"\n",
    "\n",
    "with open(\"wa/stopwords.en\") as sw: # stopwords file is from nltk https://www.nltk.org/data.html\n",
    "    en_stop_words = sw.read().split()\n",
    "    \n",
    "with open(\"wa/stopwords.fr\") as sw: # stopwords file is from nltk https://www.nltk.org/data.html\n",
    "    fr_stop_words = sw.read().split()\n",
    "\n",
    "# read  corpus and remove characteres not in allow\n",
    "with open(en_file) as f:\n",
    "    for line in f:\n",
    "        line = re.sub('[^%s]' % allow, '', line)\n",
    "        words = line.split()\n",
    "        words = [x.lower() for x in words]\n",
    "        en_corpus.append(words)\n",
    "        \n",
    "        tmp = []\n",
    "        for word in words:\n",
    "            if word not in en_stop_words:\n",
    "                tmp.append(word)\n",
    "            else:\n",
    "                tmp.append(\"<unk>\")\n",
    "        en_sentences.append(tmp)\n",
    "        \n",
    "# read  corpus and remove characteres not in allow\n",
    "with open(fr_file) as f:\n",
    "    for line in f:\n",
    "        line = re.sub('[^%s]' % allow, '', line)\n",
    "        words = line.split()\n",
    "        words = [x.lower() for x in words]\n",
    "        fr_corpus.append(words)\n",
    "        \n",
    "        tmp = []\n",
    "        for word in words:\n",
    "            if word not in fr_stop_words:\n",
    "                tmp.append(word)\n",
    "            else:\n",
    "                tmp.append(\"<unk>\")\n",
    "        fr_sentences.append(tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# snippit to limit to get a count for each word \n",
    "# and limit the vocabulary by vocab_size_limit\n",
    "\n",
    "# count the number of times each word appears\n",
    "en_vocab = defaultdict(lambda: 0)\n",
    "for line in en_corpus:\n",
    "    for word in line:\n",
    "        if word not in en_stop_words:\n",
    "            en_vocab[word] += 1\n",
    "            \n",
    "fr_vocab = defaultdict(lambda: 0)\n",
    "for line in fr_corpus:\n",
    "    for word in line:\n",
    "        if word not in fr_stop_words:\n",
    "            fr_vocab[word] += 1\n",
    "            \n",
    "print(len(en_vocab))\n",
    "print(len(fr_vocab))\n",
    "\n",
    "# sort by count and limit the corpus by vocab_size_limit, \n",
    "tmp = sorted(en_vocab.items(), key = lambda w: w[1],reverse=True)[:vocab_size_limit]\n",
    "if vocab_size_limit <= len(tmp):\n",
    "    en_vocab = set(map(lambda word: word[0], tmp[:vocab_size_limit]))\n",
    "else:\n",
    "    en_vocab = set(map(lambda word: word[0], tmp))\n",
    "    \n",
    "tmp = sorted(fr_vocab.items(), key = lambda w: w[1],reverse=True)[:vocab_size_limit]\n",
    "if vocab_size_limit <= len(tmp):\n",
    "    fr_vocab = set(map(lambda word: word[0], tmp[:vocab_size_limit]))\n",
    "else:\n",
    "    fr_vocab = set(map(lambda word: word[0], tmp))\n",
    "    \n",
    "# add unk word, assigns all words not in vocab (stopwords/infrequent) to it\n",
    "en_vocab.add('<unk>')\n",
    "fr_vocab.add('<unk>')\n",
    "en_vocab.add('<pad>')\n",
    "fr_vocab.add('<pad>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create word2index and index2word\n",
    "\n",
    "# word to index and vice versa\n",
    "en_word2idx = {w: idx for (idx, w) in enumerate(en_vocab)}\n",
    "en_idx2word = {idx: w for (idx, w) in enumerate(en_vocab)}\n",
    "\n",
    "fr_word2idx = {w: idx for (idx, w) in enumerate(fr_vocab)}\n",
    "fr_idx2word = {idx: w for (idx, w) in enumerate(fr_vocab)}\n",
    "\n",
    "en_vocab_size = len(en_vocab)\n",
    "fr_vocab_size = len(en_vocab)\n",
    "\n",
    "# Change the words in the sentence by the vocabulary index\n",
    "en_sentence_indexs = []\n",
    "fr_sentence_indexs = []\n",
    "for en, fr in zip(en_sentences,fr_sentences):\n",
    "    en_sentence_indexs.append([en_word2idx[word] for word in en[:max_sentence_len]])\n",
    "    fr_sentence_indexs.append([fr_word2idx[word] for word in fr[:max_sentence_len]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# get the lengths of all sentences\n",
    "seq_lengths = torch.LongTensor([len(s) for s in en_sentence_indexs]).cuda()\n",
    "fr_seq_lengths = torch.LongTensor([len(s) for s in fr_sentence_indexs]).cuda()\n",
    "en_pad = en_word2idx['<pad>']\n",
    "fr_pad = fr_word2idx['<pad>']\n",
    "\n",
    "# pad sequences\n",
    "def pad_sequences(sentence_indexs, seq_lengths, pad):\n",
    "    seq_tensor = torch.zeros((1,1)).new_full((len(sentence_indexs),seq_lengths.max()),pad).long().cuda()\n",
    "    print(seq_tensor.size())\n",
    "    for idx, (seq, seqlen) in enumerate(zip(sentence_indexs, seq_lengths)):\n",
    "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "    return seq_tensor\n",
    "\n",
    "seq_tensor = pad_sequences(en_sentence_indexs, seq_lengths, en_pad)\n",
    "fr_seq_tensor = pad_sequences(fr_sentence_indexs, fr_seq_lengths, fr_pad)\n",
    "\n",
    "def sort_batch(en, fr, lengths ,fr_lengths):\n",
    "\n",
    "    seq_lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "    seq_tensor = en[perm_idx]\n",
    "\n",
    "    targ_tensor = fr[perm_idx]\n",
    "    fr_len = fr_lengths[perm_idx]\n",
    "    return seq_tensor, targ_tensor, seq_lengths, fr_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PaddedTensorDataset(torch.utils.data.Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, data_tensor, target_tensor, length_tensor, length_target):\n",
    "        assert data_tensor.size(0) == target_tensor.size(0) == length_tensor.size(0) == length_target.size(0)\n",
    "        self.data_tensor = data_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.length_tensor = length_tensor\n",
    "        self.length_target = length_target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_tensor[index], self.target_tensor[index], self.length_tensor[index], self.length_target[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to sort the paddedTensorDataset\n",
    "# its called sort_batch because it was originally used per batch,\n",
    "# but its better to do it once with the whole dataset.\n",
    "def sort_batch(en, fr, lengths ,fr_lengths):\n",
    "\n",
    "    seq_lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "    seq_tensor = en[perm_idx]\n",
    "\n",
    "    targ_tensor = fr[perm_idx]\n",
    "    fr_len = fr_lengths[perm_idx]\n",
    "    return seq_tensor, targ_tensor, seq_lengths, fr_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = torch.utils.data.DataLoader(PaddedTensorDataset(seq_tensor, fr_seq_tensor, seq_lengths, fr_seq_lengths), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embed align model, not really needed for this assignment\n",
    "# But some code could prove usefull?\n",
    "\n",
    "class embed_align(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(embed_align, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.M1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.M2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.N1 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.N2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.softplus = nn.Softplus()    \n",
    "            \n",
    "            \n",
    "    def forward(self, x, lengths):\n",
    "        embeddings = self.embeddings(x)\n",
    "\n",
    "        packed_input = pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
    "        packed_output, (ht, ct) = self.lstm(packed_input)\n",
    "        lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        x = lstm_out\n",
    "        x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1) \n",
    "\n",
    "        u = self.M1(x)\n",
    "        s = self.M2(x)\n",
    "        s = self.softplus(s)\n",
    "\n",
    "\n",
    "        \n",
    "        m = torch.distributions.normal.Normal(u, s)\n",
    "        z = m.sample()\n",
    "\n",
    "        \n",
    "        e = self.N1(z)\n",
    "        f = self.N2(z)\n",
    "        \n",
    "        return(z, e, f, u, s)\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize model functions\n",
    "ea = embed_align(en_vocab_size, embedding_dims).cuda()\n",
    "loss_function_avg = nn.CrossEntropyLoss().cuda()\n",
    "loss_function_sum = nn.CrossEntropyLoss(size_average=False).cuda()\n",
    "optimizer = torch.optim.Adam(ea.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# not usefull for this project, just here to give context\n",
    "def get_loss(e1 ,en1,  f1 , fr1 , u1, s1):\n",
    "    loss = []\n",
    "    for e,en,f,fr,u,s in zip(e1 ,en1,  f1 , fr1 , u1, s1):\n",
    "        # english loss\n",
    "\n",
    "        e_loss =  loss_function_sum(e,en)\n",
    "        #print(e_loss)\n",
    "\n",
    "\n",
    "        # french loss\n",
    "        e_len = len(e)\n",
    "        f_losses = []\n",
    "        for fr_value in fr:\n",
    "            target = en.new_full((e_len, 0), fr_value)\n",
    "            f_losses.append( loss_function_avg(f, target) )\n",
    "\n",
    "\n",
    "        f_loss = sum(f_losses)\n",
    "        #print(f_loss)\n",
    "        # Kl divergence\n",
    "\n",
    "\n",
    "        # kl_loss = torch.mean(0.5 * torch.sum(torch.exp(s) + u**2 - 1. - s, 1))\n",
    "#         prior = torch.distributions.normal.Normal(torch.FloatTensor([0]).cuda(),torch.FloatTensor([1]).cuda())\n",
    "#         posterior = torch.distributions.normal.Normal(u, s)\n",
    "#         kl_loss = (torch.distributions.kl.kl_divergence(posterior, prior))\n",
    "#         print(kl_loss)\n",
    "        \n",
    "        \n",
    "        kl = []\n",
    "        prior = torch.distributions.multivariate_normal.MultivariateNormal(torch.ones(hidden_dim).cuda(), torch.eye(hidden_dim).cuda())\n",
    "\n",
    "        # if sentence has only one word\n",
    "        if(len(u.size()) == 1):\n",
    "            posterior = torch.distributions.multivariate_normal.MultivariateNormal(u, torch.diag(s))\n",
    "            kl.append(torch.distributions.kl.kl_divergence(posterior, prior).sum())\n",
    "        else:\n",
    "            for u2, s2 in zip(u,s):\n",
    "                posterior = torch.distributions.multivariate_normal.MultivariateNormal(u2, torch.diag(s2))\n",
    "                kl.append(torch.distributions.kl.kl_divergence(posterior, prior).sum())\n",
    "\n",
    "        kl_loss = sum(kl)\n",
    "#         print(kl_loss)\n",
    "\n",
    "        loss.append( -e_loss -f_loss +kl_loss ) \n",
    "    #print(loss)\n",
    "    return(sum(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train function, not neccesairly usefull for this project\n",
    "\n",
    "epoch_losses = []\n",
    "\n",
    "iterations = math.ceil(len(en_sentences) / batch_size)\n",
    "\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    start = time.time()\n",
    "    epoch_loss = 0\n",
    "    for en_batch, fr_batch, lengths, fr_lengths in dataset:\n",
    "\n",
    "        en_batch, fr_batch , lengths, fr_lengths = sort_batch(en_batch, fr_batch , lengths, fr_lengths)\n",
    "        batch_losses = []\n",
    "        batch_start = time.time()\n",
    "        batch = 0\n",
    "        en = torch.autograd.Variable(en_batch)\n",
    "        fr = torch.autograd.Variable(fr_batch)\n",
    "        z, e, f, u, s = ea(en, lengths)\n",
    "        \n",
    "        loss = get_loss(e, en, f, fr, u, s)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += loss\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_time = time.time() - batch_start \n",
    "        print('\\r[Epoch {:03d}/{:03d}] Batch {:06d}/{:06d} [{:.1f}/s] '.format(epoch+1, num_epochs, batch, iterations, batch_time), end='')\n",
    "\n",
    "    epoch_loss /= iterations\n",
    "    #score = evaluator.lst(net.embeddings.weight.data)\n",
    "    print('Time: {:.1f}s Loss: {:.3f} LST: {:.6f}'.format(time.time() - start, epoch_loss, 0))\n",
    "\n",
    "    epoch_losses.append(epoch_loss)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
