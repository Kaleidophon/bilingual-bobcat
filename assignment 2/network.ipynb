{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STD\n",
    "import codecs\n",
    "from functools import reduce\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "embedding_dims = 100\n",
    "hidden_dims = embedding_dims*2\n",
    "en_file = \"data/train/train.en\"\n",
    "fr_file = \"data/train/train.fr\"\n",
    "num_epochs = 30\n",
    "learning_rate = 0.01\n",
    "max_sentence_len = 50\n",
    "cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do\n",
    "\n",
    "    -Currently has 17k vocab size for french and 15k for english, smaller? seems fast enough... not neccesairy?\n",
    "    \n",
    "    \n",
    "# usefulll for later\n",
    "\n",
    "    -That is correct. Simply set ignore_index to your padding index. Also set size_average to false and divide the loss by the number of non padding tokens. <<--- someone said this, but cant you just let loss itself avarage it out by not setting it to false? thats what i did now, might try some runs with this method when we have an evaluation method to check for difference.\n",
    "    \n",
    "# questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same code as from model.py\n",
    "# copied here so i could make adjustments as i see fit\n",
    "\n",
    "# if code is commented out its old code that is no longer used\n",
    "\n",
    "# >>>>>>\n",
    "# is new code\n",
    "# <<<<<<\n",
    "\n",
    "class ParallelCorpus(Dataset):\n",
    "    \"\"\"\n",
    "    Class that contains a parallel corpus used for training IBM Model 1 and 2.\n",
    "    \"\"\"\n",
    "   \n",
    "    #  def __init__(self, source_path, target_path):\n",
    "    \n",
    "    # >>>>>>>>>>>>>> added additional inputs.\n",
    "    def __init__(self, source_path, target_path, max_sentence_len, cuda=False):\n",
    "    # <<<<<<<<<<<<<<\n",
    "    \n",
    "        # Enable a way to read multiple paths into one corpus if wanted\n",
    "        source_paths = source_path if self.is_listlike(source_path) else (source_path, )\n",
    "        target_paths = target_path if self.is_listlike(target_path) else (target_path, )\n",
    "\n",
    "        self.source_sentences = self.read_all(source_paths)\n",
    "        self.target_sentences = self.read_all(target_paths)\n",
    "        \n",
    "        # >>>>> Limit to sentences shorter then max_sentence_size\n",
    "        # also adds SOS and EOS tags to target sentence\n",
    "        self.source_sentences = [self.source_sentences[i] for i in range(len(self.source_sentences)) if len(self.target_sentences[i]) <= max_sentence_len]\n",
    "        self.target_sentences = [[\"<SOS>\"] + sentence + [\"<EOS>\"] for sentence in self.target_sentences if len(sentence) <= max_sentence_len]\n",
    "        # <<<<<\n",
    "        \n",
    "        self.size = len(self.source_sentences)\n",
    "        \n",
    "        # self.source_vocab = set()\n",
    "        # self._source_vocab_size = None\n",
    "        \n",
    "        \n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \n",
    "    \n",
    "        # getting additional vocab info\n",
    "        self.source_vocab, self.source_w2i, self.source_i2w, self.source_vocab_size = self.set_vocab(self.source_sentences)  \n",
    "        self.target_vocab, self.target_w2i, self.target_i2w, self.target_vocab_size = self.set_vocab(self.target_sentences)\n",
    "        \n",
    "        self.source_vocab_size = len(self.source_vocab)\n",
    "        self.target_vocab_size = len(self.target_vocab)\n",
    "        \n",
    "        # convert word sentences to index sentences\n",
    "        self.source_idx = self.set_index_sentences(self.source_sentences, self.source_w2i)\n",
    "        self.target_idx = self.set_index_sentences(self.target_sentences, self.target_w2i)\n",
    "        \n",
    "        # getting the lengths of the sentences\n",
    "        self.source_lengths = [len(s) for s in self.source_idx]\n",
    "        self.target_lengths = [len(s) for s in self.target_idx]\n",
    "        \n",
    "        # create postition vectors\n",
    "        self.pos_base = [i for i in range(1,max_sentence_len+1)]\n",
    "        self.positions = [self.pos_base[:i] for i in self.source_lengths]\n",
    "        \n",
    "        self.source_pad = self.source_w2i['<pad>']\n",
    "        self.target_pad = self.target_w2i['<pad>']\n",
    "        \n",
    "        # pad the sentences\n",
    "        self.source_padded = self.pad_sequences(self.source_idx, self.source_lengths, self.source_pad)\n",
    "        self.target_padded = self.pad_sequences(self.target_idx, self.target_lengths, self.target_pad)\n",
    "        self.positions = self.pad_sequences(self.positions, self.source_lengths, 0)\n",
    "        \n",
    "        # turn into tensors\n",
    "        if (cuda):\n",
    "            self.source_tensor = torch.LongTensor(self.source_padded).cuda()\n",
    "            self.target_tensor = torch.LongTensor(self.target_padded).cuda()\n",
    "            self.source_lengths = torch.LongTensor(self.source_lengths).cuda()\n",
    "            self.target_lengths = torch.LongTensor(self.target_lengths).cuda()\n",
    "            self.positions = torch.LongTensor(self.positions).cuda()\n",
    "        else:\n",
    "            self.source_tensor = torch.LongTensor(self.source_padded)\n",
    "            self.target_tensor = torch.LongTensor(self.target_padded)\n",
    "            self.source_lengths = torch.LongTensor(self.source_lengths)\n",
    "            self.target_lengths = torch.LongTensor(self.target_lengths)\n",
    "            self.positions = torch.LongTensor(self.positions)\n",
    "            \n",
    "        # sort them by sentence size, decending.\n",
    "        self.source_tensor, self.target_tensor, self.source_lengths, self.target_lengths, self.positions = \\\n",
    "            self.sort_data(self.source_tensor, self.target_tensor, self.source_lengths, self.target_lengths, self.positions)\n",
    "        \n",
    "    def set_vocab(self, sentences):\n",
    "        vocab = ['<pad>', '<unk>', \"<SOS>\", \"<EOS>\"]\n",
    "        for line in sentences:\n",
    "            for word in line:\n",
    "                if word not in vocab:\n",
    "                    vocab.append(word)\n",
    "                    \n",
    "        word2idx = {w: idx for (idx, w) in enumerate(vocab)}\n",
    "        idx2word = {idx: w for (idx, w) in enumerate(vocab)}\n",
    "\n",
    "        vocab_size = len(vocab)\n",
    "        \n",
    "        return vocab, word2idx, idx2word, vocab_size\n",
    "        \n",
    "    def set_index_sentences(self, sentences, w2i):\n",
    "        sentence_indexs = []\n",
    "        for sentence in sentences:\n",
    "            sentence_indexs.append([w2i[word] for word in sentence])\n",
    "        return sentence_indexs\n",
    "    \n",
    "    def pad_sequences(self, sentence_indexs, seq_lengths, pad):\n",
    "        padding = np.full((len(sentence_indexs),max(seq_lengths)),pad) \n",
    "        for idx, (seq, seqlen) in enumerate(zip(sentence_indexs, seq_lengths)):\n",
    "            padding[idx, :seqlen] = seq\n",
    "        return padding\n",
    "    \n",
    "    def sort_data(self, source, target, source_lengths ,target_lengths, pos):\n",
    "\n",
    "        target_lengths, perm_idx = target_lengths.sort(0, descending=True)\n",
    "        source_tensor = source[perm_idx]\n",
    "        target_tensor = target[perm_idx]\n",
    "        source_lengths = source_lengths[perm_idx]\n",
    "        positions = pos[perm_idx]\n",
    "        return source_tensor, target_tensor, source_lengths, target_lengths, positions\n",
    "    \n",
    "    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "        \n",
    "\n",
    "    def read_all(self, paths):\n",
    "        \"\"\"\n",
    "        Read all corpus file at given paths and merge results.\n",
    "        \"\"\"\n",
    "        def _combine_lists(a, b):\n",
    "            a.extend(b)\n",
    "            return a\n",
    "\n",
    "        sentences_per_path = [self.read_corpus_file(path) for path in paths]\n",
    "        return reduce(_combine_lists, sentences_per_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_corpus_file(path, filter_characters=[]):\n",
    "        sentences = []\n",
    "\n",
    "        with codecs.open(path, \"rb\", \"utf-8\") as corpus:\n",
    "            for line in corpus.readlines():\n",
    "                tokens = [token for token in line.strip().split() if token not in filter_characters]\n",
    "                sentences.append(tokens)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "#     @property\n",
    "#     def parallel_sentences(self):\n",
    "#         return zip(self.source_sentences, self.target_sentences)\n",
    "\n",
    "#     @property\n",
    "#     def source_vocab_size(self):\n",
    "#         if self._source_vocab_size is None:\n",
    "#             self.source_vocab = {token for sentence in self.target_sentences for token in sentence}\n",
    "#             self._source_vocab_size = len(self.source_vocab)\n",
    "#         return self._source_vocab_size\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         return (\n",
    "#             (source_sentence, target_sentence)\n",
    "#             for source_sentence, target_sentence in zip(self.source_sentences, self.target_sentences)\n",
    "#         )\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.size\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         return self.source_sentences[item], self.target_sentences[item]\n",
    "\n",
    "\n",
    "    # >>>>>>>>>>>>>\n",
    "    def __len__(self):\n",
    "        return self.source_tensor.size(0) \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.source_tensor[index], self.target_tensor[index], self.source_lengths[index], self.target_lengths[index], self.positions[index]\n",
    "    # <<<<<<<<<<<<<<<\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_listlike(obj):\n",
    "        return type(obj) in (tuple, list, set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ParallelCorpus(fr_file, en_file, max_sentence_len, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, source_vocab_size, embedding_dims,max_sentence_len):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(source_vocab_size, embedding_dims) #embed words\n",
    "        self.pos_embeddings = nn.Embedding(max_sentence_len+1, embedding_dims) #embed positions\n",
    "        \n",
    "    def forward(self, source_sentences, positions):\n",
    "        words = self.word_embeddings(source_sentences)\n",
    "        pos = self.pos_embeddings(positions)\n",
    "        return torch.cat((words,pos),2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embedding_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Decoder stuff\n",
    "        self.word_embeddings = nn.Embedding(target_vocab_size, embedding_dims)\n",
    "        self.lstm = nn.LSTM(embedding_dims, hidden_dims, batch_first=True)\n",
    "        self.scale_h0 = nn.Linear(embedding_dims*2, hidden_dims)\n",
    "        self.out = nn.Linear(embedding_dims*2 + hidden_dims, target_vocab_size)\n",
    "        \n",
    "        #Attention stuff\n",
    "        \n",
    "        # used only in the concat version\n",
    "        self.attn_layer = nn.Linear(embedding_dims*2 + hidden_dims, 1)\n",
    "        \n",
    "        self.attn_soft = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, target_sentences, target_lengths, source_lengths, encoder_outputs, max_len):\n",
    "        words = self.word_embeddings(target_sentences)\n",
    "        \n",
    "        # avg out encoder data to use as hidden state\n",
    "        avg = torch.mean(encoder_outputs,1)\n",
    "        hidden = self.scale_h0(avg)\n",
    "        \n",
    "        # het hidden state from encoder RNN\n",
    "        packed_input = pack_padded_sequence(words, target_lengths, batch_first=True)\n",
    "        packed_output, (ht, ct) = self.lstm(packed_input, (torch.unsqueeze(hidden,0),torch.unsqueeze(hidden,0)))\n",
    "        lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # prepare lstm Hidden layers for input into attention,\n",
    "        # we add the hidden layer as zeroth lstm_out and remove the last one\n",
    "        # this is because we need h_i-1, not h_i\n",
    "        lstm_to_attn = torch.cat((torch.unsqueeze(hidden,1),lstm_out),1)\n",
    "        lstm_to_attn = lstm_to_attn = lstm_to_attn[:,:-1,:]\n",
    "        \n",
    "        context = self.attention(lstm_to_attn, encoder_outputs, source_lengths, max_len)\n",
    "        \n",
    "        \n",
    "        # combine with non existing context vectors\n",
    "        combined = torch.cat((lstm_out,lstm_out),2)\n",
    "        out = self.out(combined)\n",
    "        return out\n",
    "    \n",
    "    def attention(self, lstm_to_attn, encoder_outputs, source_lengths, max_len):\n",
    "                \n",
    "        # repeat the lstm out in third dimension and the encoder outputs in second dimension so we can make a meshgrid\n",
    "        # so we can do elementwise mul for all possible combinations of h_j and s_i\n",
    "        h_j = encoder_outputs.unsqueeze(1).repeat(1,lstm_to_attn.size(1),1,1)\n",
    "        s_i = lstm_to_attn.unsqueeze(2).repeat(1,1,encoder_outputs.size(1),1)\n",
    "        \n",
    "        # get the dot product between the two to get the energy\n",
    "        # the unsqueezes are there to emulate transposing. so we can use matmul as torch.dot doesnt accept matrices\n",
    "        energy = s_i.unsqueeze(3).matmul(h_j.unsqueeze(4)).squeeze(4)\n",
    "        \n",
    "#         # this is concat attention, its a different form then the ones we need\n",
    "#         cat = torch.cat((s_i,h_j),3)\n",
    "        \n",
    "#         energy = self.attn_layer(cat)\n",
    "\n",
    "        # reshaping the encoder outputs for later\n",
    "        encoder_outputs = encoder_outputs.unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.repeat(1,energy.size(1),1,1)\n",
    "    \n",
    "        # apply softmax to the energys \n",
    "        allignment = self.attn_soft(energy)\n",
    "        \n",
    "        # create a mask like : [1,1,1,0,0,0] whos goal is to multiply the attentions of the pads with 0, rest with 1\n",
    "        idxes = torch.arange(0,max_len,out=max_len).unsqueeze(0)\n",
    "        mask = Variable((idxes<source_lengths.unsqueeze(1)).float())\n",
    "        \n",
    "        # format the mask to be same size() as the attentions\n",
    "        mask = mask.unsqueeze(1).unsqueeze(3).repeat(1,allignment.size(1),1,1)\n",
    "        \n",
    "        # apply mask\n",
    "        masked = allignment * mask\n",
    "        \n",
    "        # now we have to rebalance the other values so they sum to 1 again\n",
    "        # this is done by dividing each value by the sum of the sequence\n",
    "        # calculate sums\n",
    "        msum = masked.sum(-2).repeat(1,1,masked.size(2)).unsqueeze(3)\n",
    "        \n",
    "        # rebalance\n",
    "        attentions = masked.div(msum)\n",
    "        \n",
    "        # now we shape the attentions to be similar to context in size\n",
    "        allignment = allignment.repeat(1,1,1,encoder_outputs.size(3))\n",
    "\n",
    "        # make context vector by element wise mul\n",
    "        context = attentions * encoder_outputs\n",
    "        \n",
    "        return context\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialiase models, loss and optimizer\n",
    "if (cuda):\n",
    "    encoder = Encoder(dataset.source_vocab_size, embedding_dims, max_sentence_len).cuda()\n",
    "    decoder = Decoder(dataset.target_vocab_size, embedding_dims).cuda()\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index = dataset.target_pad).cuda()\n",
    "else:\n",
    "    encoder = Encoder(dataset.source_vocab_size, embedding_dims, max_sentence_len).cpu()\n",
    "    decoder = Decoder(dataset.target_vocab_size, embedding_dims).cpu()\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index = dataset.target_pad).cpu()\n",
    "    \n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001/030] Batch 000454/000454 [0.0/s] Time: 36.7s Loss: 3.968 score2?: 0.000000\n",
      "[Epoch 002/030] Batch 000454/000454 [0.0/s] Time: 36.2s Loss: 3.176 score2?: 0.000000\n",
      "[Epoch 003/030] Batch 000454/000454 [0.0/s] Time: 36.2s Loss: 2.605 score2?: 0.000000\n",
      "[Epoch 004/030] Batch 000454/000454 [0.0/s] Time: 36.1s Loss: 2.218 score2?: 0.000000\n",
      "[Epoch 005/030] Batch 000454/000454 [0.0/s] Time: 36.1s Loss: 1.938 score2?: 0.000000\n",
      "[Epoch 006/030] Batch 000454/000454 [0.0/s] Time: 36.1s Loss: 1.740 score2?: 0.000000\n",
      "[Epoch 007/030] Batch 000454/000454 [0.1/s] Time: 36.2s Loss: 1.572 score2?: 0.000000\n",
      "[Epoch 008/030] Batch 000454/000454 [0.0/s] Time: 36.2s Loss: 1.444 score2?: 0.000000\n",
      "[Epoch 009/030] Batch 000454/000454 [0.0/s] Time: 36.1s Loss: 1.351 score2?: 0.000000\n",
      "[Epoch 010/030] Batch 000454/000454 [0.0/s] Time: 36.1s Loss: 1.272 score2?: 0.000000\n",
      "[Epoch 011/030] Batch 000454/000454 [0.0/s] Time: 36.1s Loss: 1.201 score2?: 0.000000\n",
      "[Epoch 012/030] Batch 000454/000454 [0.1/s] Time: 36.2s Loss: 1.140 score2?: 0.000000\n",
      "[Epoch 013/030] Batch 000454/000454 [0.1/s] Time: 36.1s Loss: 1.102 score2?: 0.000000\n",
      "[Epoch 014/030] Batch 000454/000454 [0.0/s] Time: 36.4s Loss: 1.063 score2?: 0.000000\n",
      "[Epoch 015/030] Batch 000454/000454 [0.0/s] Time: 36.2s Loss: 1.017 score2?: 0.000000\n",
      "[Epoch 016/030] Batch 000454/000454 [0.0/s] Time: 36.2s Loss: 0.980 score2?: 0.000000\n",
      "[Epoch 017/030] Batch 000454/000454 [0.0/s] Time: 36.2s Loss: 0.953 score2?: 0.000000\n",
      "[Epoch 018/030] Batch 000454/000454 [0.0/s] Time: 36.2s Loss: 0.927 score2?: 0.000000\n",
      "[Epoch 019/030] Batch 000454/000454 [0.0/s] Time: 36.2s Loss: 0.909 score2?: 0.000000\n",
      "[Epoch 020/030] Batch 000454/000454 [0.0/s] Time: 36.2s Loss: 0.893 score2?: 0.000000\n",
      "[Epoch 021/030] Batch 000454/000454 [0.0/s] Time: 36.1s Loss: 0.877 score2?: 0.000000\n",
      "[Epoch 022/030] Batch 000454/000454 [0.0/s] Time: 36.2s Loss: 0.850 score2?: 0.000000\n",
      "[Epoch 023/030] Batch 000454/000454 [0.0/s] Time: 36.1s Loss: 0.828 score2?: 0.000000\n",
      "[Epoch 024/030] Batch 000454/000454 [0.1/s] Time: 36.1s Loss: 0.824 score2?: 0.000000\n",
      "[Epoch 025/030] Batch 000454/000454 [0.0/s] Time: 36.1s Loss: 0.816 score2?: 0.000000\n",
      "[Epoch 026/030] Batch 000454/000454 [0.0/s] Time: 36.1s Loss: 0.808 score2?: 0.000000\n",
      "[Epoch 027/030] Batch 000454/000454 [0.1/s] Time: 36.2s Loss: 0.794 score2?: 0.000000\n",
      "[Epoch 028/030] Batch 000454/000454 [0.0/s] Time: 36.1s Loss: 0.784 score2?: 0.000000\n",
      "[Epoch 029/030] Batch 000454/000454 [0.0/s] Time: 36.1s Loss: 0.782 score2?: 0.000000\n",
      "[Epoch 030/030] Batch 000454/000454 [0.0/s] Time: 36.2s Loss: 0.769 score2?: 0.000000\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "iterations = len(data_loader)\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    start = time.time()\n",
    "    batch_losses = []\n",
    "    batch = 0\n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in data_loader:\n",
    "        batch_start = time.time()\n",
    "        max_len = source_lengths.max()\n",
    "        source_batch = source_batch[:,:max_len]\n",
    "        batch_positions = batch_positions[:,:max_len]\n",
    "        \n",
    "        source_batch = torch.autograd.Variable(source_batch)\n",
    "        target_batch = torch.autograd.Variable(target_batch)\n",
    "        source_lengths = torch.autograd.Variable(source_lengths)\n",
    "        target_lengths = torch.autograd.Variable(target_lengths)\n",
    "        batch_positions = torch.autograd.Variable(batch_positions)\n",
    "        \n",
    "        if (cuda):\n",
    "            source_batch = source_batch.cuda()\n",
    "            target_batch = target_batch.cuda()\n",
    "            source_lengths = source_lengths.cuda()\n",
    "            target_lengths = target_lengths.cuda()\n",
    "            batch_positions = batch_positions.cuda()\n",
    "            \n",
    "            \n",
    "        # get encoder and decoder outputs\n",
    "        encoder_out = encoder(source_batch, batch_positions)\n",
    "        decoder_out = decoder(target_batch, target_lengths, source_lengths, encoder_out, max_len)\n",
    "        \n",
    "        # remove the start token from the targets and the end token from the decoder\n",
    "        decoder_out2 = decoder_out[:,:-1,:]\n",
    "        target_batch2 = target_batch[:,1:decoder_out.size(1)]\n",
    "        \n",
    "        # pytorch expects the inputs for the loss function to be: (batch x classes)\n",
    "        # cant handle sentences so we just transform our data to treath each individual word as a batch:\n",
    "        # so new batch size = old batch * padded sentence length\n",
    "        # should not matter as it all gets averaged out anyway\n",
    "        decoder_out3 = decoder_out2.contiguous().view(decoder_out2.size(0)*decoder_out2.size(1),decoder_out2.size(2))\n",
    "        target_batch3 = target_batch2.contiguous().view(target_batch2.size(0)*target_batch2.size(1))\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = loss_function(decoder_out3,target_batch3)\n",
    "        batch_losses.append(loss)\n",
    "        \n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_time = time.time() - batch_start \n",
    "        print('\\r[Epoch {:03d}/{:03d}] Batch {:06d}/{:06d} [{:.1f}/s] '.format(epoch+1, num_epochs, batch+1, iterations, batch_time), end='')\n",
    "        batch += 1\n",
    "        \n",
    "    avg_loss = sum(batch_losses) / iterations \n",
    "    epoch_losses.append(avg_loss)\n",
    "    print('Time: {:.1f}s Loss: {:.3f} score2?: {:.6f}'.format(time.time() - start, avg_loss, 0))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything below is non usefull, testing code used to try stuff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
