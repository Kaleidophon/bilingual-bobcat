{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STD\n",
    "import codecs\n",
    "from functools import reduce\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "embedding_dims = 100\n",
    "hidden_dims = embedding_dims*2\n",
    "en_file = \"data/train/train.en\"\n",
    "fr_file = \"data/train/train.fr\"\n",
    "num_epochs = 30\n",
    "learning_rate = 0.01\n",
    "max_sentence_len = 50\n",
    "cuda = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do\n",
    "\n",
    "    -Currently has 17k vocab size for french and 15k for english, smaller? seems fast enough... not neccesairy?\n",
    "    \n",
    "    \n",
    "# usefulll for later\n",
    "\n",
    "    -That is correct. Simply set ignore_index to your padding index. Also set size_average to false and divide the loss by the number of non padding tokens. <<--- someone said this, but cant you just let loss itself avarage it out by not setting it to false? thats what i did now, might try some runs with this method when we have an evaluation method to check for difference.\n",
    "    \n",
    "# questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same code as from model.py\n",
    "# copied here so i could make adjustments as i see fit\n",
    "\n",
    "# if code is commented out its old code that is no longer used\n",
    "\n",
    "# >>>>>>\n",
    "# is new code\n",
    "# <<<<<<\n",
    "\n",
    "class ParallelCorpus(Dataset):\n",
    "    \"\"\"\n",
    "    Class that contains a parallel corpus used for training IBM Model 1 and 2.\n",
    "    \"\"\"\n",
    "   \n",
    "    #  def __init__(self, source_path, target_path):\n",
    "    \n",
    "    # >>>>>>>>>>>>>> added additional inputs.\n",
    "    def __init__(self, source_path, target_path, max_sentence_len, cuda=False):\n",
    "    # <<<<<<<<<<<<<<\n",
    "    \n",
    "        # Enable a way to read multiple paths into one corpus if wanted\n",
    "        source_paths = source_path if self.is_listlike(source_path) else (source_path, )\n",
    "        target_paths = target_path if self.is_listlike(target_path) else (target_path, )\n",
    "\n",
    "        self.source_sentences = self.read_all(source_paths)\n",
    "        self.target_sentences = self.read_all(target_paths)\n",
    "        \n",
    "        # >>>>> Limit to sentences shorter then max_sentence_size\n",
    "        # also adds SOS and EOS tags to target sentence\n",
    "        self.source_sentences = [self.source_sentences[i] for i in range(len(self.source_sentences)) if len(self.target_sentences[i]) <= max_sentence_len]\n",
    "        self.target_sentences = [[\"<SOS>\"] + sentence + [\"<EOS>\"] for sentence in self.target_sentences if len(sentence) <= max_sentence_len]\n",
    "        # <<<<<\n",
    "        \n",
    "        self.size = len(self.source_sentences)\n",
    "        \n",
    "        # self.source_vocab = set()\n",
    "        # self._source_vocab_size = None\n",
    "        \n",
    "        \n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \n",
    "    \n",
    "        # getting additional vocab info\n",
    "        self.source_vocab, self.source_w2i, self.source_i2w, self.source_vocab_size = self.set_vocab(self.source_sentences)  \n",
    "        self.target_vocab, self.target_w2i, self.target_i2w, self.target_vocab_size = self.set_vocab(self.target_sentences)\n",
    "        \n",
    "        self.source_vocab_size = len(self.source_vocab)\n",
    "        self.target_vocab_size = len(self.target_vocab)\n",
    "        \n",
    "        # convert word sentences to index sentences\n",
    "        self.source_idx = self.set_index_sentences(self.source_sentences, self.source_w2i)\n",
    "        self.target_idx = self.set_index_sentences(self.target_sentences, self.target_w2i)\n",
    "        \n",
    "        # getting the lengths of the sentences\n",
    "        self.source_lengths = [len(s) for s in self.source_idx]\n",
    "        self.target_lengths = [len(s) for s in self.target_idx]\n",
    "        \n",
    "        # create postition vectors\n",
    "        self.pos_base = [i for i in range(1,max_sentence_len+1)]\n",
    "        self.positions = [self.pos_base[:i] for i in self.source_lengths]\n",
    "        \n",
    "        self.source_pad = self.source_w2i['<pad>']\n",
    "        self.target_pad = self.target_w2i['<pad>']\n",
    "        \n",
    "        # pad the sentences\n",
    "        self.source_padded = self.pad_sequences(self.source_idx, self.source_lengths, self.source_pad)\n",
    "        self.target_padded = self.pad_sequences(self.target_idx, self.target_lengths, self.target_pad)\n",
    "        self.positions = self.pad_sequences(self.positions, self.source_lengths, 0)\n",
    "        \n",
    "        # turn into tensors\n",
    "        if (cuda):\n",
    "            self.source_tensor = torch.LongTensor(self.source_padded).cuda()\n",
    "            self.target_tensor = torch.LongTensor(self.target_padded).cuda()\n",
    "            self.source_lengths = torch.LongTensor(self.source_lengths).cuda()\n",
    "            self.target_lengths = torch.LongTensor(self.target_lengths).cuda()\n",
    "            self.positions = torch.LongTensor(self.positions).cuda()\n",
    "        else:\n",
    "            self.source_tensor = torch.LongTensor(self.source_padded)\n",
    "            self.target_tensor = torch.LongTensor(self.target_padded)\n",
    "            self.source_lengths = torch.LongTensor(self.source_lengths)\n",
    "            self.target_lengths = torch.LongTensor(self.target_lengths)\n",
    "            self.positions = torch.LongTensor(self.positions)\n",
    "            \n",
    "        # sort them by sentence size, decending.\n",
    "        self.source_tensor, self.target_tensor, self.source_lengths, self.target_lengths, self.positions = \\\n",
    "            self.sort_data(self.source_tensor, self.target_tensor, self.source_lengths, self.target_lengths, self.positions)\n",
    "        \n",
    "    def set_vocab(self, sentences):\n",
    "        vocab = ['<pad>', '<unk>', \"<SOS>\", \"<EOS>\"]\n",
    "        for line in sentences:\n",
    "            for word in line:\n",
    "                if word not in vocab:\n",
    "                    vocab.append(word)\n",
    "                    \n",
    "        word2idx = {w: idx for (idx, w) in enumerate(vocab)}\n",
    "        idx2word = {idx: w for (idx, w) in enumerate(vocab)}\n",
    "\n",
    "        vocab_size = len(vocab)\n",
    "        \n",
    "        return vocab, word2idx, idx2word, vocab_size\n",
    "        \n",
    "    def set_index_sentences(self, sentences, w2i):\n",
    "        sentence_indexs = []\n",
    "        for sentence in sentences:\n",
    "            sentence_indexs.append([w2i[word] for word in sentence])\n",
    "        return sentence_indexs\n",
    "    \n",
    "    def pad_sequences(self, sentence_indexs, seq_lengths, pad):\n",
    "        padding = np.full((len(sentence_indexs),max(seq_lengths)),pad) \n",
    "        for idx, (seq, seqlen) in enumerate(zip(sentence_indexs, seq_lengths)):\n",
    "            padding[idx, :seqlen] = seq\n",
    "        return padding\n",
    "    \n",
    "    def sort_data(self, source, target, source_lengths ,target_lengths, pos):\n",
    "\n",
    "        target_lengths, perm_idx = target_lengths.sort(0, descending=True)\n",
    "        source_tensor = source[perm_idx]\n",
    "        target_tensor = target[perm_idx]\n",
    "        source_lengths = source_lengths[perm_idx]\n",
    "        positions = pos[perm_idx]\n",
    "        return source_tensor, target_tensor, source_lengths, target_lengths, positions\n",
    "    \n",
    "    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "        \n",
    "\n",
    "    def read_all(self, paths):\n",
    "        \"\"\"\n",
    "        Read all corpus file at given paths and merge results.\n",
    "        \"\"\"\n",
    "        def _combine_lists(a, b):\n",
    "            a.extend(b)\n",
    "            return a\n",
    "\n",
    "        sentences_per_path = [self.read_corpus_file(path) for path in paths]\n",
    "        return reduce(_combine_lists, sentences_per_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_corpus_file(path, filter_characters=[]):\n",
    "        sentences = []\n",
    "\n",
    "        with codecs.open(path, \"rb\", \"utf-8\") as corpus:\n",
    "            for line in corpus.readlines():\n",
    "                tokens = [token for token in line.strip().split() if token not in filter_characters]\n",
    "                sentences.append(tokens)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "#     @property\n",
    "#     def parallel_sentences(self):\n",
    "#         return zip(self.source_sentences, self.target_sentences)\n",
    "\n",
    "#     @property\n",
    "#     def source_vocab_size(self):\n",
    "#         if self._source_vocab_size is None:\n",
    "#             self.source_vocab = {token for sentence in self.target_sentences for token in sentence}\n",
    "#             self._source_vocab_size = len(self.source_vocab)\n",
    "#         return self._source_vocab_size\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         return (\n",
    "#             (source_sentence, target_sentence)\n",
    "#             for source_sentence, target_sentence in zip(self.source_sentences, self.target_sentences)\n",
    "#         )\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.size\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         return self.source_sentences[item], self.target_sentences[item]\n",
    "\n",
    "\n",
    "    # >>>>>>>>>>>>>\n",
    "    def __len__(self):\n",
    "        return self.source_tensor.size(0) \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.source_tensor[index], self.target_tensor[index], self.source_lengths[index], self.target_lengths[index], self.positions[index]\n",
    "    # <<<<<<<<<<<<<<<\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_listlike(obj):\n",
    "        return type(obj) in (tuple, list, set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = ParallelCorpus(fr_file, en_file, max_sentence_len, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, source_vocab_size, embedding_dims,max_sentence_len):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(source_vocab_size, embedding_dims) #embed words\n",
    "        self.pos_embeddings = nn.Embedding(max_sentence_len+1, embedding_dims) #embed positions\n",
    "        \n",
    "    def forward(self, source_sentences, positions):\n",
    "        words = self.word_embeddings(source_sentences)\n",
    "        pos = self.pos_embeddings(positions)\n",
    "        return torch.cat((words,pos),2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embedding_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Decoder stuff\n",
    "        self.word_embeddings = nn.Embedding(target_vocab_size, embedding_dims)\n",
    "        self.lstm = nn.LSTM(embedding_dims, hidden_dims, batch_first=True)\n",
    "        self.scale_h0 = nn.Linear(embedding_dims*2, hidden_dims)\n",
    "        self.out = nn.Linear(embedding_dims*2 + hidden_dims, target_vocab_size)\n",
    "        \n",
    "        #Attention stuff\n",
    "        \n",
    "        # used only in the concat version\n",
    "        self.attn_layer = nn.Linear(embedding_dims*2 + hidden_dims, 1)\n",
    "        \n",
    "        self.attn_soft = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, target_sentences, target_lengths, source_lengths, encoder_outputs, max_len):\n",
    "        words = self.word_embeddings(target_sentences)\n",
    "        \n",
    "        # avg out encoder data to use as hidden state\n",
    "        avg = torch.mean(encoder_outputs,1)\n",
    "        hidden = self.scale_h0(avg)\n",
    "        \n",
    "        # get hidden state from encoder RNN\n",
    "        packed_input = pack_padded_sequence(words, target_lengths, batch_first=True)\n",
    "        packed_output, (ht, ct) = self.lstm(packed_input, (torch.unsqueeze(hidden,0),torch.unsqueeze(hidden,0)))\n",
    "        lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # prepare lstm Hidden layers for input into attention,\n",
    "        # we add the hidden layer as zeroth lstm_out and remove the last one\n",
    "        # this is because we need h_i-1, not h_i\n",
    "        lstm_to_attn = torch.cat((torch.unsqueeze(hidden,1),lstm_out),1)\n",
    "        lstm_to_attn = lstm_to_attn = lstm_to_attn[:,:-1,:]\n",
    "        \n",
    "        context = self.attention(lstm_to_attn, encoder_outputs, source_lengths, max_len)\n",
    "        \n",
    "        # combine with non existing context vectors\n",
    "        combined = torch.cat((lstm_out,context),2)\n",
    "        out = self.out(combined)\n",
    "        return out\n",
    "    \n",
    "    def attention(self, lstm_to_attn, encoder_outputs, source_lengths, max_len):\n",
    "                \n",
    "        # repeat the lstm out in third dimension and the encoder outputs in second dimension so we can make a meshgrid\n",
    "        # so we can do elementwise mul for all possible combinations of h_j and s_i\n",
    "        h_j = encoder_outputs.unsqueeze(1).repeat(1,lstm_to_attn.size(1),1,1)\n",
    "        s_i = lstm_to_attn.unsqueeze(2).repeat(1,1,encoder_outputs.size(1),1)\n",
    "        \n",
    "        # get the dot product between the two to get the energy\n",
    "        # the unsqueezes are there to emulate transposing. so we can use matmul as torch.dot doesnt accept matrices\n",
    "        energy = s_i.unsqueeze(3).matmul(h_j.unsqueeze(4)).squeeze(4)\n",
    "        \n",
    "#         # this is concat attention, its a different form then the ones we need\n",
    "#         cat = torch.cat((s_i,h_j),3)\n",
    "        \n",
    "#         energy = self.attn_layer(cat)\n",
    "\n",
    "        # reshaping the encoder outputs for later\n",
    "        encoder_outputs = encoder_outputs.unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.repeat(1,energy.size(1),1,1)\n",
    "    \n",
    "        # apply softmax to the energys \n",
    "        allignment = self.attn_soft(energy)\n",
    "        \n",
    "        # create a mask like : [1,1,1,0,0,0] whos goal is to multiply the attentions of the pads with 0, rest with 1\n",
    "        idxes = torch.arange(0,max_len,out=max_len).unsqueeze(0)\n",
    "        mask = Variable((idxes<source_lengths.unsqueeze(1)).float())\n",
    "        \n",
    "        # format the mask to be same size() as the attentions\n",
    "        mask = mask.unsqueeze(1).unsqueeze(3).repeat(1,allignment.size(1),1,1)\n",
    "        \n",
    "        # apply mask\n",
    "        masked = allignment * mask\n",
    "        \n",
    "        # now we have to rebalance the other values so they sum to 1 again\n",
    "        # this is done by dividing each value by the sum of the sequence\n",
    "        # calculate sums\n",
    "        msum = masked.sum(-2).repeat(1,1,masked.size(2)).unsqueeze(3)\n",
    "        \n",
    "        # rebalance\n",
    "        attentions = masked.div(msum)\n",
    "        \n",
    "        # now we shape the attentions to be similar to context in size\n",
    "        allignment = allignment.repeat(1,1,1,encoder_outputs.size(3))\n",
    "\n",
    "        # make context vector by element wise mul\n",
    "        context = attentions * encoder_outputs\n",
    "        \n",
    "        context2 = torch.sum(context,2)\n",
    "        \n",
    "        return context2\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialiase models, loss and optimizer\n",
    "if (cuda):\n",
    "    encoder = Encoder(dataset.source_vocab_size, embedding_dims, max_sentence_len).cuda()\n",
    "    decoder = Decoder(dataset.source_vocab_size, max_sentence_len, dataset.target_vocab_size, embedding_dims).cuda()\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index = dataset.target_pad).cuda()\n",
    "else:\n",
    "    encoder = Encoder(dataset.source_vocab_size, embedding_dims, max_sentence_len).cpu()\n",
    "    decoder = Decoder(dataset.target_vocab_size, embedding_dims).cpu()\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index = dataset.target_pad).cpu()\n",
    "    \n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'max_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-f72f0811bce8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# get encoder and decoder outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mencoder_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_positions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mdecoder_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# remove the start token from the targets and the end token from the decoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Rasyan\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'max_len'"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "iterations = len(data_loader)\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    start = time.time()\n",
    "    batch_losses = []\n",
    "    batch = 0\n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in data_loader:\n",
    "        batch_start = time.time()\n",
    "        max_len = source_lengths.max()\n",
    "        source_batch = source_batch[:,:max_len]\n",
    "        batch_positions = batch_positions[:,:max_len]\n",
    "        \n",
    "        source_batch = torch.autograd.Variable(source_batch)\n",
    "        target_batch = torch.autograd.Variable(target_batch)\n",
    "        source_lengths = torch.autograd.Variable(source_lengths)\n",
    "        target_lengths = torch.autograd.Variable(target_lengths)\n",
    "        batch_positions = torch.autograd.Variable(batch_positions)\n",
    "        \n",
    "        if (cuda):\n",
    "            source_batch = source_batch.cuda()\n",
    "            target_batch = target_batch.cuda()\n",
    "            source_lengths = source_lengths.cuda()\n",
    "            target_lengths = target_lengths.cuda()\n",
    "            batch_positions = batch_positions.cuda()\n",
    "            \n",
    "            \n",
    "        # get encoder and decoder outputs\n",
    "        encoder_out = encoder(source_batch, batch_positions)\n",
    "        decoder_out = decoder(target_batch, target_lengths, source_lengths, encoder_out, max_len)\n",
    "        \n",
    "        # remove the start token from the targets and the end token from the decoder\n",
    "        decoder_out2 = decoder_out[:,:-1,:]\n",
    "        target_batch2 = target_batch[:,1:decoder_out.size(1)]\n",
    "        \n",
    "        # pytorch expects the inputs for the loss function to be: (batch x classes)\n",
    "        # cant handle sentences so we just transform our data to treath each individual word as a batch:\n",
    "        # so new batch size = old batch * padded sentence length\n",
    "        # should not matter as it all gets averaged out anyway\n",
    "        decoder_out3 = decoder_out2.contiguous().view(decoder_out2.size(0)*decoder_out2.size(1),decoder_out2.size(2))\n",
    "        target_batch3 = target_batch2.contiguous().view(target_batch2.size(0)*target_batch2.size(1))\n",
    "        \n",
    "        # calculate loss\n",
    "        #print(decoder_out2.size())\n",
    "        #print(target_batch[0])\n",
    "        _, sentence = torch.max(decoder_out2[0],1)\n",
    "        test_pred = [dataset.target_i2w[word] for word in sentence.cpu().numpy()]\n",
    "        print(test_pred)\n",
    "        test_real = [dataset.target_i2w[word] for word in target_batch2[0].cpu().numpy()]\n",
    "        print(test_real)\n",
    "        \n",
    "        loss = loss_function(decoder_out3,target_batch3)\n",
    "        batch_losses.append(loss)\n",
    "        \n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_time = time.time() - batch_start \n",
    "        print('\\r[Epoch {:03d}/{:03d}] Batch {:06d}/{:06d} [{:.1f}/s] '.format(epoch+1, num_epochs, batch+1, iterations, batch_time), end='')\n",
    "        batch += 1\n",
    "        \n",
    "    avg_loss = sum(batch_losses) / iterations \n",
    "    epoch_losses.append(avg_loss)\n",
    "    print('Time: {:.1f}s Loss: {:.3f} score2?: {:.6f}'.format(time.time() - start, avg_loss, 0))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, source_vocab_size, max_sentence_len, target_vocab_size, embedding_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Encoder stuff\n",
    "        self.encoder_embeddings = nn.Embedding(source_vocab_size, embedding_dims) #embed words\n",
    "        self.pos_embeddings = nn.Embedding(max_sentence_len+1, embedding_dims) #embed positions\n",
    "        \n",
    "        # Decoder stuff\n",
    "        self.decoder_embeddings = nn.Embedding(target_vocab_size, embedding_dims)\n",
    "        self.lstm = nn.LSTM(embedding_dims, hidden_dims, batch_first=True)\n",
    "        self.scale_h0 = nn.Linear(embedding_dims*2, hidden_dims)\n",
    "        self.out = nn.Linear(embedding_dims*2 + hidden_dims, target_vocab_size)\n",
    "        \n",
    "        # Attention stuff\n",
    "        \n",
    "        # used only in the concat version\n",
    "        self.attn_layer = nn.Linear(embedding_dims*2 + hidden_dims, 1)\n",
    "        \n",
    "        self.attn_soft = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, target_sentences, target_lengths, source_lengths, source_sentences, positions, max_len):\n",
    "        words = self.decoder_embeddings(target_sentences)\n",
    "        \n",
    "        \n",
    "        encoder_outputs = encoder(source_sentences, positions)\n",
    "        \n",
    "        # avg out encoder data to use as hidden state\n",
    "        avg = torch.mean(encoder_outputs,1)\n",
    "        hidden = self.scale_h0(avg)\n",
    "        \n",
    "        # get hidden state from encoder RNN\n",
    "        packed_input = pack_padded_sequence(words, target_lengths, batch_first=True)\n",
    "        packed_output, (ht, ct) = self.lstm(packed_input, (torch.unsqueeze(hidden,0),torch.unsqueeze(hidden,0)))\n",
    "        lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # prepare lstm Hidden layers for input into attention,\n",
    "        # we add the hidden layer as zeroth lstm_out and remove the last one\n",
    "        # this is because we need h_i-1, not h_i\n",
    "        lstm_to_attn = torch.cat((torch.unsqueeze(hidden,1),lstm_out),1)\n",
    "        lstm_to_attn = lstm_to_attn = lstm_to_attn[:,:-1,:]\n",
    "        \n",
    "        context = self.attention(lstm_to_attn, encoder_outputs, source_lengths, max_len)\n",
    "        \n",
    "        # combine with non existing context vectors\n",
    "        combined = torch.cat((lstm_out,context),2)\n",
    "        out = self.out(combined)\n",
    "        return out\n",
    "    \n",
    "    def attention(self, lstm_to_attn, encoder_outputs, source_lengths, max_len):\n",
    "                \n",
    "        # repeat the lstm out in third dimension and the encoder outputs in second dimension so we can make a meshgrid\n",
    "        # so we can do elementwise mul for all possible combinations of h_j and s_i\n",
    "        h_j = encoder_outputs.unsqueeze(1).repeat(1,lstm_to_attn.size(1),1,1)\n",
    "        s_i = lstm_to_attn.unsqueeze(2).repeat(1,1,encoder_outputs.size(1),1)\n",
    "        \n",
    "        # get the dot product between the two to get the energy\n",
    "        # the unsqueezes are there to emulate transposing. so we can use matmul as torch.dot doesnt accept matrices\n",
    "        energy = s_i.unsqueeze(3).matmul(h_j.unsqueeze(4)).squeeze(4)\n",
    "        \n",
    "#         # this is concat attention, its a different form then the ones we need\n",
    "#         cat = torch.cat((s_i,h_j),3)\n",
    "        \n",
    "#         energy = self.attn_layer(cat)\n",
    "\n",
    "        # reshaping the encoder outputs for later\n",
    "        encoder_outputs = encoder_outputs.unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.repeat(1,energy.size(1),1,1)\n",
    "    \n",
    "        # apply softmax to the energys \n",
    "        allignment = self.attn_soft(energy)\n",
    "        \n",
    "        # create a mask like : [1,1,1,0,0,0] whos goal is to multiply the attentions of the pads with 0, rest with 1\n",
    "        idxes = torch.arange(0,max_len,out=max_len).unsqueeze(0)\n",
    "        mask = Variable((idxes<source_lengths.unsqueeze(1)).float())\n",
    "        \n",
    "        # format the mask to be same size() as the attentions\n",
    "        mask = mask.unsqueeze(1).unsqueeze(3).repeat(1,allignment.size(1),1,1)\n",
    "        \n",
    "        # apply mask\n",
    "        masked = allignment * mask\n",
    "        \n",
    "        # now we have to rebalance the other values so they sum to 1 again\n",
    "        # this is done by dividing each value by the sum of the sequence\n",
    "        # calculate sums\n",
    "        msum = masked.sum(-2).repeat(1,1,masked.size(2)).unsqueeze(3)\n",
    "        \n",
    "        # rebalance\n",
    "        attentions = masked.div(msum)\n",
    "        \n",
    "        # now we shape the attentions to be similar to context in size\n",
    "        allignment = allignment.repeat(1,1,1,encoder_outputs.size(3))\n",
    "\n",
    "        # make context vector by element wise mul\n",
    "        context = attentions * encoder_outputs\n",
    "        \n",
    "        context2 = torch.sum(context,2)\n",
    "        \n",
    "        return context2\n",
    "    \n",
    "    def encoder(self, source_sentences, positions):\n",
    "        words = self.word_embeddings(source_sentences)\n",
    "        pos = self.pos_embeddings(positions)\n",
    "        return torch.cat((words,pos),2)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialiase models, loss and optimizer\n",
    "if (cuda):\n",
    "    decoder = Decoder(dataset.source_vocab_size, max_sentence_len, dataset.target_vocab_size, embedding_dims).cuda()\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index = dataset.target_pad).cuda()\n",
    "else:\n",
    "    decoder = Decoder(dataset.source_vocab_size, max_sentence_len, dataset.target_vocab_size, embedding_dims).cpu()\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index = dataset.target_pad).cpu()\n",
    "    \n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['football', 'football', 'football', 'wearing', 'a', 'white', 'and', 'white', 'uniform', 'catching', 'the', 'ball', '<EOS>', 'a', 'crowd', 'with', 'the', 'opponent', 'wearing', 'a', 'blue', 'and', 'white', 'uniform', 'is', 'in', 'the', 'background', '<EOS>', 'with', 'a', 'crowd', 'of', 'a', 'for', 'the', 'game', '<EOS>']\n",
      "['A', 'soccer', 'player', 'wearing', 'a', 'red', 'and', 'white', 'uniform', 'catching', 'the', 'ball', 'for', 'a', 'score', 'as', 'the', 'opponent', 'wearing', 'a', 'blue', 'and', 'white', 'uniform', 'is', 'in', 'the', 'background', 'along', 'with', 'a', 'crowd', 'of', 'people', 'watching', 'the', 'game', '<EOS>']\n",
      "[Epoch 001/030] Batch 000001/000454 [0.2/s] ['A', 'track', 'red,', 'black', 'red', 'race', 'car', 'zooms', 'by', 'dancing,', 'a', 'gray', 'track', 'with', 'the', 'blue', 'border,', 'sharply', 'in', 'focus', 'compared', 'to', 'a', 'blue', 'crowd', 'in', 'the', 'foreground.', '<EOS>']\n",
      "['A', 'black,', 'red,', 'and', 'white', 'race', 'car', 'zooms', 'by', 'on', 'a', 'gray', 'track', 'with', 'a', 'blue', 'border,', 'sharply', 'in', 'focus', 'compared', 'to', 'a', 'blurred', 'crowd', 'in', 'the', 'foreground.', '<EOS>']\n",
      "[Epoch 001/030] Batch 000002/000454 [0.4/s] ['A', 'child', 'is', 'doing', 'an', 'activity', 'involving', 'a', 'circular', 'disk', 'with', 'radiating', 'lines', 'on', 'it', '(', 'glue', 'blue,', 'and', 'blue,', 'a', 'glue', 'stick,', 'and', 'cut-out', 'numbers.', '<EOS>']\n",
      "['A', 'child', 'is', 'doing', 'an', 'activity', 'involving', 'a', 'circular', 'disk', 'with', 'radiating', 'lines', 'on', 'it', '(', 'in', 'red', 'and', 'blue,', 'a', 'glue', 'stick,', 'and', 'cut-out', 'numbers.', '<EOS>']\n",
      "[Epoch 001/030] Batch 000003/000454 [0.3/s] ['A', 'man', 'that', 'is', 'wearing', 'a', 'gray', 'military', 'shirt', 'with', 'medals', 'on', 'a', 'light', 'blue', 'cap', 'with', 'medals', 'on', 'it', 'is', 'waving', 'his', 'arm.', '<EOS>']\n",
      "['A', 'man', 'that', 'is', 'wearing', 'a', 'gray', 'military', 'shirt', 'with', 'medals', 'and', 'a', 'light', 'blue', 'cap', 'with', 'gold', 'on', 'it', 'is', 'waving', 'his', 'arm.', '<EOS>']\n",
      "[Epoch 001/030] Batch 000004/000454 [0.2/s] ['A', 'man', 'in', 'a', 'yellow', 'jumpsuit', 'is', 'standing', 'on', 'a', 'podium', 'accepting', 'a', 'trophy', 'next', 'to', 'and', 'man', 'in', 'a', 'red', 'and', 'with', 'jumpsuit.', '<EOS>']\n",
      "['A', 'man', 'in', 'a', 'yellow', 'jumpsuit', 'is', 'standing', 'on', 'a', 'podium', 'accepting', 'a', 'trophy', 'next', 'to', 'a', 'man', 'in', 'a', 'red', 'and', 'with', 'jumpsuit.', '<EOS>']\n",
      "[Epoch 001/030] Batch 000005/000454 [0.2/s] ['Man', 'man', 'in', 'a', 'black', 'white', 'and', 'white', 'shirt', 'and', 'a', 'helmet', 'cleans', 'one', 'floor', 'while', 'another', 'one', 'helps', 'him', 'on', 'the', 'background.', '<EOS>']\n",
      "['A', 'man', 'with', 'a', 'striped', 'black', 'and', 'white', 'shirt', 'and', 'a', 'helmet', 'cleans', 'wooden', 'floor', 'while', 'another', 'one', 'helps', 'him', 'on', 'the', 'background.', '<EOS>']\n",
      "[Epoch 001/030] Batch 000006/000454 [0.2/s] ['In', 'the', 'pouring', 'rain,', 'a', 'marathon', 'runner', 'smiles', 'big', 'as', 'a', 'approaches', 'groups', 'of', 'bystanders,', 'as', 'several', 'pace', 'car', 'drives', 'along', 'as', 'him.', '<EOS>']\n",
      "['In', 'the', 'pouring', 'rain,', 'a', 'marathon', 'runner', 'smiles', 'big', 'as', 'he', 'approaches', 'groups', 'of', 'bystanders,', 'as', 'the', 'pace', 'car', 'drives', 'along', 'behind', 'him.', '<EOS>']\n",
      "[Epoch 001/030] Batch 000007/000454 [0.2/s] ['A', 'young', 'young', 'boy', 'boy', 'is', 'in', 'jeans', 'and', 'sneakers', 'is', 'standing', 'outside.', 'holding', 'on', 'to', 'a', 'piece', 'of', 'gym', 'equipment', 'outside.', '<EOS>']\n",
      "['A', 'very', 'young', 'smiling', 'boy', 'is', 'in', 'jeans', 'and', 'sneakers', 'is', 'standing', 'and', 'holding', 'on', 'to', 'a', 'piece', 'of', 'gym', 'equipment', 'outside.', '<EOS>']\n",
      "[Epoch 001/030] Batch 000008/000454 [0.2/s] ['Artwork', 'of', 'young', 'young', 'child', 'and', 'his', 'dog', 'overlook', 'a', 'quiet', 'sidewalk', 'and', 'a', 'man', 'sleeping', 'against', 'the', 'side', 'of', 'a', 'building.', '<EOS>']\n",
      "['Artwork', 'of', 'a', 'young', 'child', 'and', 'his', 'dog', 'overlook', 'a', 'quiet', 'sidewalk', 'and', 'a', 'man', 'sleeping', 'against', 'the', 'side', 'of', 'a', 'building.', '<EOS>']\n",
      "[Epoch 001/030] Batch 000009/000454 [0.2/s] ['A', 'man', 'in', 'a', 'long-sleeved', 'long-sleeve', 'T-shirt', 'is', 'excited', 'to', 'try', 'some', 'food', 'that', 'has', 'been', 'food', 'out', 'on', 'a', 'coffee', 'table.', '<EOS>']\n",
      "['A', 'man', 'in', 'a', 'black', 'long-sleeve', 'T-shirt', 'is', 'excited', 'to', 'try', 'some', 'food', 'that', 'has', 'been', 'set', 'out', 'on', 'a', 'coffee', 'table.', '<EOS>']\n",
      "[Epoch 001/030] Batch 000010/000454 [0.2/s] ['A', 'car', 'with', 'a', 'heavily', 'damaged', 'anterior', 'damaged', 'being', 'pushed', 'through', 'the', 'intersection', 'by', 'a', 'few', 'people', 'and', 'a', 'police', 'officer.', '<EOS>']\n",
      "['A', 'car', 'with', 'a', 'heavily', 'damaged', 'anterior', 'is', 'being', 'pushed', 'through', 'the', 'intersection', 'by', 'a', 'few', 'people', 'and', 'a', 'police', 'officer.', '<EOS>']\n",
      "[Epoch 001/030] Batch 000011/000454 [0.2/s] ['A', 'group', 'of', 'three', 'men', 'and', 'two', 'women', 'sitting', 'at', 'a', 'table', 'that', 'has', 'food,', 'drinks,', 'and', 'glasses.', 'game', 'on', 'a', '<EOS>']\n",
      "['A', 'group', 'of', 'three', 'men', 'and', 'two', 'woman', 'sitting', 'at', 'a', 'table', 'that', 'has', 'food,', 'drinks,', 'and', 'a', 'game', 'on', 'it.', '<EOS>']\n",
      "[Epoch 001/030] Batch 000012/000454 [0.2/s] ['A', 'guy', 'wearing', 'a', 'black', 'attire', 'is', 'doing', 'ballet', 'in', 'front', 'of', 'people', 'pier', 'where', 'you', 'in.', 'see', 'a', 'pier', 'ship.', '<EOS>']\n",
      "['A', 'guy', 'wearing', 'a', 'black', 'attire', 'is', 'doing', 'ballet', 'in', 'front', 'of', 'the', 'pier', 'where', 'you', 'can', 'see', 'a', 'cruise', 'ship.', '<EOS>']\n",
      "[Epoch 001/030] Batch 000013/000454 [0.2/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-43c4b71f2c70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m#print(target_batch[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_out2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_i2w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mtest_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_i2w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtarget_batch2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "iterations = len(data_loader)\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    start = time.time()\n",
    "    batch_losses = []\n",
    "    batch = 0\n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in data_loader:\n",
    "        batch_start = time.time()\n",
    "        max_len = source_lengths.max()\n",
    "        source_batch = source_batch[:,:max_len]\n",
    "        batch_positions = batch_positions[:,:max_len]\n",
    "        \n",
    "        source_batch = torch.autograd.Variable(source_batch)\n",
    "        target_batch = torch.autograd.Variable(target_batch)\n",
    "        source_lengths = torch.autograd.Variable(source_lengths)\n",
    "        target_lengths = torch.autograd.Variable(target_lengths)\n",
    "        batch_positions = torch.autograd.Variable(batch_positions)\n",
    "        \n",
    "        if (cuda):\n",
    "            source_batch = source_batch.cuda()\n",
    "            target_batch = target_batch.cuda()\n",
    "            source_lengths = source_lengths.cuda()\n",
    "            target_lengths = target_lengths.cuda()\n",
    "            batch_positions = batch_positions.cuda()\n",
    "            \n",
    "            \n",
    "        # get encoder and decoder outputs\n",
    "        # encoder_out = encoder(source_batch, batch_positions)\n",
    "        decoder_out = decoder(target_batch, target_lengths, source_lengths, source_batch, batch_positions, max_len)\n",
    "        \n",
    "        # remove the start token from the targets and the end token from the decoder\n",
    "        decoder_out2 = decoder_out[:,:-1,:]\n",
    "        target_batch2 = target_batch[:,1:decoder_out.size(1)]\n",
    "        \n",
    "        # pytorch expects the inputs for the loss function to be: (batch x classes)\n",
    "        # cant handle sentences so we just transform our data to treath each individual word as a batch:\n",
    "        # so new batch size = old batch * padded sentence length\n",
    "        # should not matter as it all gets averaged out anyway\n",
    "        decoder_out3 = decoder_out2.contiguous().view(decoder_out2.size(0)*decoder_out2.size(1),decoder_out2.size(2))\n",
    "        target_batch3 = target_batch2.contiguous().view(target_batch2.size(0)*target_batch2.size(1))\n",
    "        \n",
    "        # calculate loss\n",
    "        #print(decoder_out2.size())\n",
    "        #print(target_batch[0])\n",
    "        _, sentence = torch.max(decoder_out2[0],1)\n",
    "        test_pred = [dataset.target_i2w[word] for word in sentence.cpu().numpy()]\n",
    "        print(test_pred)\n",
    "        test_real = [dataset.target_i2w[word] for word in target_batch2[0].cpu().numpy()]\n",
    "        print(test_real)\n",
    "        \n",
    "        loss = loss_function(decoder_out3,target_batch3)\n",
    "        batch_losses.append(loss)\n",
    "        \n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_time = time.time() - batch_start \n",
    "        print('\\r[Epoch {:03d}/{:03d}] Batch {:06d}/{:06d} [{:.1f}/s] '.format(epoch+1, num_epochs, batch+1, iterations, batch_time), end='')\n",
    "        batch += 1\n",
    "        \n",
    "    avg_loss = sum(batch_losses) / iterations \n",
    "    epoch_losses.append(avg_loss)\n",
    "    print('Time: {:.1f}s Loss: {:.3f} score2?: {:.6f}'.format(time.time() - start, avg_loss, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rasyan\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(decoder, \"decoder30.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_set, target_path, reference_file_path):\n",
    "    data_loader = DataLoader(eval_set, batch_size=5)\n",
    "    softmax = nn.Softmax(dim=2)\n",
    "    idx2word = evaluation_set.target_i2w\n",
    "    sorted_sentence_ids = evaluation_set.target_sentence_ids.cpu().numpy()\n",
    "    translated_sentences = []\n",
    "\n",
    "    # Decode\n",
    "    #for source_batch, target_batch, source_lengths, target_lengths, batch_positions in data_loader:\n",
    "    for source_batch, target_batch, source_lengths, target_lengths, batch_positions in data_loader:\n",
    "        # TODO: Don't use target sentences for prediction!\n",
    "        max_len = source_lengths.max()\n",
    "        output = model(target_batch, target_lengths, source_lengths, source_batch, batch_positions, max_len)\n",
    "        #normalized_output = softmax.forward(output)\n",
    "        predictions = output.max(2)[1].cpu().numpy()  # Only get indices\n",
    "\n",
    "        for sentence_index in range(predictions.shape[0]):\n",
    "            token_indices = predictions[sentence_index]\n",
    "            tokens = list(map(lambda idx: idx2word[idx], token_indices))\n",
    "\n",
    "            eos_index = len(tokens)\n",
    "            if \"<eos>\" in tokens:\n",
    "                eos_index = tokens.index(\"<eos>\")\n",
    "\n",
    "            tokens = tokens[:eos_index]  # Cut off after first end of sentence token\n",
    "            translated_sentence = \" \".join(tokens).replace(\"@@ \", \"\")\n",
    "            translated_sentences.append(translated_sentence)\n",
    "            print(translated_sentence)\n",
    "            \n",
    "            \n",
    "\n",
    "    # Bring sentence back into the order they were in the test set\n",
    "    translated_sentences = np.array(translated_sentences)[sorted_sentence_ids]\n",
    "\n",
    "    # Write to file\n",
    "    with codecs.open(target_path, \"wb\", \"utf-8\") as target_file:\n",
    "        for sentence in translated_sentences:\n",
    "            target_file.write(\"{}\\n\".format(sentence))\n",
    "\n",
    "    out = subprocess.getoutput(\n",
    "        \"perl ./multi-bleu.perl {} < {}\".format(reference_file_path, target_path)\n",
    "    )\n",
    "    print(out[out.index(\"BLEU\"):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module defining class and function relevant for data I/O and preprocessing.\n",
    "\"\"\"\n",
    "\n",
    "# STD\n",
    "import codecs\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "# EXT\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ParallelCorpus(Dataset):\n",
    "    \"\"\"\n",
    "    Class that contains a parallel corpus used for training IBM Model 1 and 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, source_path, target_path, max_sentence_length=50, max_source_vocab_size=np.inf,\n",
    "                 max_target_vocab_size=np.inf, use_indices_from=None):\n",
    "        if use_indices_from:\n",
    "            assert type(use_indices_from) == type(self), \"You can only use indices from another ParallelCorpus class\"\n",
    "\n",
    "        # Enable a way to read multiple paths into one corpus if wanted\n",
    "        source_paths = source_path if self.is_listlike(source_path) else (source_path, )\n",
    "        target_paths = target_path if self.is_listlike(target_path) else (target_path, )\n",
    "\n",
    "        # Read in all the data\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.source_sentences = self.read_all(source_paths)\n",
    "        self.target_sentences = self.read_all(target_paths)\n",
    "        self.size = len(self.source_sentences)\n",
    "\n",
    "        # Index corpus and create vocabulary\n",
    "        indexed_source = self.index_corpus(\n",
    "            self.source_sentences, max_vocab_size=max_source_vocab_size,\n",
    "            given_word2idx=None if not use_indices_from else use_indices_from.source_w2i\n",
    "        )\n",
    "        self.source_idx, self.source_vocab, self.source_w2i, self.source_i2w, self.source_vocab_size, self.source_lengths = indexed_source\n",
    "        indexed_target = self.index_corpus(\n",
    "            self.target_sentences, add_sentence_delimiters=True, max_vocab_size=max_target_vocab_size,\n",
    "            given_word2idx=None if not use_indices_from else use_indices_from.target_w2i\n",
    "        )\n",
    "        self.target_idx, self.target_vocab, self.target_w2i, self.target_i2w, self.target_vocab_size, self.target_lengths = indexed_target\n",
    "\n",
    "        # Create position vectors\n",
    "        self.pos_base = [i for i in range(1, max_sentence_length + 1)]\n",
    "        self.positions = [self.pos_base[:i] for i in self.source_lengths]\n",
    "        self.source_pad = self.source_w2i['<pad>']\n",
    "        self.target_pad = self.target_w2i['<pad>']\n",
    "\n",
    "        # Pad the sentences and positions - useful in order to convert everything to tensors and train batch\n",
    "        self.source_padded = self.pad_sequences(self.source_idx, self.source_lengths, self.source_w2i['<pad>'])\n",
    "        self.target_padded = self.pad_sequences(self.target_idx, self.target_lengths, self.target_w2i['<pad>'])\n",
    "        self.positions = self.pad_sequences(self.positions, self.source_lengths, 0)\n",
    "        self.source_sentence_ids = np.array(range(len(self.source_sentences)))\n",
    "        self.target_sentence_ids = np.array(range(len(self.source_sentences)))\n",
    "\n",
    "        # Convert everything into tensors for pytorch and sort by sentence size, descending\n",
    "        data = [\n",
    "            self.source_padded, self.target_padded, self.source_lengths, self.target_lengths, self.positions,\n",
    "            self.source_sentence_ids, self.target_sentence_ids\n",
    "        ]\n",
    "        tensors = list(map(self.convert_to_tensor, data))\n",
    "        sorted_tensors = self.sort_tensors(*tensors)\n",
    "        self.source_tensor, self.target_tensor, self.source_lengths, self.target_lengths, self.positions, \\\n",
    "            self.source_sentence_ids, self.target_sentence_ids = sorted_tensors\n",
    "\n",
    "    def read_all(self, paths):\n",
    "        \"\"\"\n",
    "        Read all corpus file at given paths and merge results.\n",
    "        \"\"\"\n",
    "        def _combine_lists(a, b):\n",
    "            a.extend(b)\n",
    "            return a\n",
    "\n",
    "        sentences_per_path = [self.read_corpus_file(path, self.max_sentence_length) for path in paths]\n",
    "        return reduce(_combine_lists, sentences_per_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_corpus_file(path, max_sentence_length, filter_characters=[]):\n",
    "        sentences = []\n",
    "\n",
    "        with codecs.open(path, \"rb\", \"utf-8\") as corpus:\n",
    "            for line in corpus.readlines():\n",
    "                tokens = [token for token in line.strip().split() if token not in filter_characters]\n",
    "\n",
    "                # Filter out sentences that are too long\n",
    "                if len(tokens) <= max_sentence_length:\n",
    "                    sentences.append(tokens)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def index_corpus(self, sentences, max_vocab_size=np.inf, add_sentence_delimiters=False, given_word2idx=None):\n",
    "        # Use indices create on other set if given\n",
    "        word2idx = self.init_word2idx() if not given_word2idx else given_word2idx\n",
    "        indexed_sentences = []\n",
    "        sentence_lengths = []\n",
    "        token_freqs = defaultdict(int)\n",
    "\n",
    "        for line in sentences:\n",
    "            current_indexed_sentence = [] if not add_sentence_delimiters else [word2idx[\"<bos>\"]]\n",
    "\n",
    "            for word in line:\n",
    "                token_freqs[word] += 1\n",
    "                current_indexed_sentence.append(word2idx[word])\n",
    "\n",
    "            if add_sentence_delimiters:\n",
    "                current_indexed_sentence.append(word2idx[\"<eos>\"])\n",
    "\n",
    "            indexed_sentences.append(current_indexed_sentence)\n",
    "\n",
    "            sentence_length = len(line) if not add_sentence_delimiters else len(line) + 2\n",
    "            sentence_lengths.append(sentence_length)\n",
    "\n",
    "        # Remove infrequent words (they will be mapped to <unk> later) until max_vocab size is reached\n",
    "        vocab_size = len(word2idx)\n",
    "        sorted_token_freqs = sorted(list(token_freqs.items()), key=lambda x: x[1])\n",
    "\n",
    "        while len(sorted_token_freqs) > max_vocab_size:\n",
    "            infrequent_word, _ = sorted_token_freqs.pop(0)\n",
    "            del word2idx[infrequent_word]\n",
    "\n",
    "        vocab = set(word2idx.keys())\n",
    "\n",
    "        idx2word = defaultdict(lambda: \"<unk>\", {idx: w for (w, idx) in word2idx.items()})\n",
    "\n",
    "        # After reading the data, unknown word just return the index of the <unk> token (don't generate new indices)\n",
    "        word2idx = defaultdict(lambda: word2idx[\"<unk>\"], word2idx)\n",
    "\n",
    "        return indexed_sentences, vocab, word2idx, idx2word, vocab_size, sentence_lengths\n",
    "\n",
    "    @staticmethod\n",
    "    def init_word2idx():\n",
    "        word2idx = defaultdict(lambda: len(word2idx))\n",
    "        _ = word2idx[\"<pad>\"], word2idx[\"<unk>\"], word2idx[\"<bos>\"], word2idx[\"<eos>\"]\n",
    "        return word2idx\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sequences(sentence_idx, seq_lengths, pad):\n",
    "        # Fill everything with padding first\n",
    "        padding = np.full((len(sentence_idx), max(seq_lengths)), pad)\n",
    "\n",
    "        # Replace with actual token ids wherever possible\n",
    "        for idx, (seq, seqlen) in enumerate(zip(sentence_idx, seq_lengths)):\n",
    "            padding[idx, :seqlen] = seq\n",
    "\n",
    "        return padding\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_tensor(data):\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.LongTensor(data).cuda()\n",
    "        return torch.LongTensor(data)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_tensors(source, target, source_lengths, target_lengths, pos, source_ids, target_ids):\n",
    "        target_lengths, perm_idx = target_lengths.sort(0, descending=True)\n",
    "        source_tensor = source[perm_idx]\n",
    "        target_tensor = target[perm_idx]\n",
    "        source_lengths = source_lengths[perm_idx]\n",
    "        positions = pos[perm_idx]\n",
    "        source_ids = source_ids[perm_idx]\n",
    "        target_ids = source_ids[perm_idx]\n",
    "        return source_tensor, target_tensor, source_lengths, target_lengths, positions, source_ids, target_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.source_tensor.size(0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.source_tensor[index], self.target_tensor[index], self.source_lengths[index], \\\n",
    "               self.target_lengths[index], self.positions[index]\n",
    "\n",
    "    @staticmethod\n",
    "    def is_listlike(obj):\n",
    "        return type(obj) in (tuple, list, set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rasyan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public system hats several hats several hats hats attire hats standing\n",
      "is patrolling with <unk> her camera standing floor\n",
      "tice sunglasses several several hats process hats\n",
      "is males ATV with males several tree-of every clouds shirt males\n",
      "pulley crew with youths\n",
      "aged males several\n",
      "pulley is several searching searching broken several\n",
      "pulley males jump males tree-@@\n",
      "extended several hats several several hats fourseveral several\n",
      "pulley hide small flamboyant\n",
      "spacwith hats floor hats wearing hats hats standing light-slacks teens starring teens of fluffy hats at of\n",
      "soda pulled tables together hats hats several\n",
      "painting woman several with several several hats youths\n",
      "youths dogs shirt of\n",
      "hole cloth Justseveral hats several\n",
      "painting camping cheerful patch patch\n",
      "pulley bunshirt hats hats several several hats with several several pond\n",
      "painting several with Gablinds several\n",
      "headbands woman several several\n",
      "painting TeGaGashirt hats hats hats police hats of several down plane\n",
      "painting Jesus several hats and of of\n",
      "painting Jesus several several hats painting\n",
      "headbands woman ods attire lays several hats tree-standing tables standing\n",
      "soda . small bunstanding at hats hats hats hats <unk> playhouse painted\n",
      "painting males standing hats hats hats\n",
      "pulley Walkman shirt several clseveral several meinstruments\n",
      "pulley males getting lays and hats outdoor\n",
      "pulley males several challentree-standing village hats dog patch patch\n",
      "painting <unk> standing with standing\n",
      "pulley woman with hats males hats males around between with\n",
      "pulley shirt hats hats several Verope\n",
      "pulley males waiting shirt standing with hats faces light shirt observes\n",
      "painting woman shirt several hats hats hats hats\n",
      "light-and at\n",
      "two Justshirt several hats some hats\n",
      "pulley women several funky hats hats several hats of while of searching directions\n",
      "pulley males with with hats with\n",
      "painting ry Justhats hats standing and hats\n",
      "painting room . one window with hats several with\n",
      "painting woman with youths walls hats several at at several plastic communicating\n",
      "painting males with hats and with and\n",
      "pulley several attire lays 30 together\n",
      "soda tied standing standing several hats several several standing\n",
      "headbands several shirt bunseveral\n",
      "pulley each small hats hats hats\n",
      "painting males males hats several hats standing\n",
      "painting bunplayhouse snow-covered\n",
      "awning vest members standing standing shirt extended\n",
      "two painted standing far@@\n",
      "painting 30 and and attire males patch patrolling\n",
      "pulley males hats\n",
      "Hamales several several several hats\n",
      "painting woman shirt several standing several hats\n",
      "pulley males <unk> males several standing hats wearing males\n",
      "soda them shirt plastic hats standing standing bicycle\n",
      "painting woman shirt with several\n",
      "pulley grate light-several several several several several of several several\n",
      "pulley males shirt youths several with shirt shirt shirt shirt standing standing window shirt with hats display shirt standing youths bunch\n",
      "painting males\n",
      "pulley cloth with with with males standing\n",
      "painting several\n",
      "gathered shirt riding several with top\n",
      "painting cloth standing standing standing standing several hats standing\n",
      "pulley pretending several hats several standing pouring\n",
      "painting several several shirt painted\n",
      "two Justhotel standing several several youths hats standing with standing hats hats standing hats several with smiling\n",
      "pulley males patch standing window\n",
      "painting lift playhouse\n",
      "painting is CGayouths several tables standing preparing hats\n",
      "pulley Teshirt shirt several several\n",
      "pulley attire attire lays\n",
      "headbands grate shirt of at\n",
      "pulley face youths hats standing fall plastic hats off several window hats\n",
      "pulley dragon males hats hats several several standing hats run several several wine dirt several\n",
      "awning tied of attire hats with\n",
      "pulley shirt dog at plaby hats several several\n",
      "kayak standing\n",
      "drilling their\n",
      "painting Techeek lays with several with standing\n",
      "pulley man system with several\n",
      "painting parachutist shirt\n",
      "painting shirt shirt at hats tables hats hats Autumn\n",
      "pulley males males skateboards\n",
      "awning circle several several while hats several window\n",
      "pulley woman shirt with several\n",
      "pulley cloth vocalist shirt\n",
      "painting 30 shirt tree-with with rising hats\n",
      "pulley with with lays shirt hats shirt\n",
      "painting males shirt several painting several and and\n",
      "painting room shirt\n",
      "painting grate cheek hats standing light and and\n",
      "kicking pose standing standing standing hats and bikini sunset painted hats phones\n",
      "painting of shirt several hats hats\n",
      "pulley of with hats several\n",
      "painting males males males hats shirt hats shirt\n",
      "two them\n",
      "tewith with with it large handing\n",
      "painting Teshirt plastic several standing hats hats\n",
      "two them <unk> them several hats several\n",
      "two scene shirt several several blinds several several several and\n",
      "painting attire several hats white lays several hats\n",
      "painting several shirt shirt several hats hats\n",
      "pulley child attire\n",
      "brush\n",
      "pulley several shirt shirt several tables\n",
      "pulley with shirt with several several\n",
      "painting grate shirt window standing several several youths\n",
      "pulley several hats hats hats several several several hats\n",
      "pulley couch shirt window youths several\n",
      "center buncoat hats painted down with at indoor headgear at teeth indoor\n",
      "painting with\n",
      "painting painted several Johns patch with with hats several\n",
      "painting hispanic shirt hats standing hats hats plastic\n",
      "mounted\n",
      "pulley camera attire with with hats with hats roller-bla@@\n",
      "pulley cells jump males several\n",
      "pulley males hills hats hats hats standing\n",
      "painting shirt shirt hats hats several males prepare hats youths\n",
      "painting lift shirt several several several down\n",
      "pulley <unk> shirt with wearing and\n",
      "painting hide dog shirt slacks several several hats\n",
      "painting 30 hills youths standing hats hats hats\n",
      "role standing\n",
      "painting lift coat hats painted hats hats hats hats hats\n",
      "oxen 30 observes hats\n",
      "pulley black with hats standing messy\n",
      "headbands standing attire\n",
      "pulley cloth advertisement window window several standing several rest painted with with standing\n",
      "painting several of shirt hats standing\n",
      "painting males of patch\n",
      "painting bikers\n",
      "drilling . shirt shirt several painted several hose hose hats\n",
      "from organ of\n",
      "kayak shirt shirt shirt hats and at\n",
      "painting shirt shirt at hats in\n",
      "painting of several and several standing shirt\n",
      "painting lake shirt several youths\n",
      "painting Teshirt\n",
      "painting communicating hats shirt several several with hats several\n",
      "painting\n",
      "painting couch shirt window hats standing\n",
      "painting expre. fell lays down hats standing\n",
      "painting each shirt hats and and several standing painted\n",
      "kayak works shirt play several several standing several\n",
      "pulley lift Justseveral\n",
      "bronrun tree-tree-tree-hats hats hats standing hats hats males\n",
      "studying and\n",
      "extended woman zards hats hats standing of street\n",
      "pulley several males males standing standing\n",
      "painting standing\n",
      "painting snowdrift standing hats\n",
      "pulley\n",
      "pulley Tetoddlers hats\n",
      "center shirt several with several\n",
      "pulley attire several standing several hats hats\n",
      "painting woman several males tree-@@\n",
      "painting males Texas with males standing\n",
      "painting red-servwith hats several several and\n",
      "painting hoping hats several hats portrait with\n",
      "painting vest hats\n",
      "painting Jesus several several\n",
      "painting of shirt\n",
      "painting Jesus several youths several tes ponytail youths\n",
      "painting of Bihats standing selhats painted hats\n",
      "painting cloth several worshihats guitar hats hats\n",
      "painting family\n",
      "pulley several hats youths prepare hats and wine and\n",
      "painting shirt at shirt cting\n",
      "pulley males shirt several tree-tree-with funky matching males\n",
      "hats shirt\n",
      "pulley males servshirt window and hats standing\n",
      "inflating sized with shirt\n",
      "pulley males and\n",
      "painting each shirt hats of hats hats hats of\n",
      "painting harness\n",
      "painting\n",
      "headbands Teshirt shirt painted at painted of patch\n",
      "pulley males males pro@@\n",
      "painting shirt shirt several at several hats\n",
      "painting woman several youths several several hats hats several\n",
      "painting woman shirt several several boy standing her wearing standing\n",
      "pulley grate shirt standing\n",
      "painting couch room hats one and hats distance washing of hats hats\n",
      "painting 30\n",
      "painting by hats hats standing C hats hats and hats orange hats\n",
      "painting hide\n",
      "painting woman standing at hats several bikers hats\n",
      "pulley woman several youths standing with wearing youths\n",
      "pulley pool\n",
      "pulley funky look window standing unusually hats has hats\n",
      "pulley several\n",
      "pulley each males toys\n",
      "painting males attire lays . several several hats several\n",
      "awning playhouse of of hats standing guitar\n",
      "painting cloth of with four@@\n",
      "<unk> attire attire hats outside hats\n",
      "painting hide shirt eats hats hats standing yellow\n",
      "painting shirt shirt &amp; patch window\n",
      "pulley several\n",
      "painting each standing hats shirt\n",
      "painting dark-skinned hats hats hats several standing\n",
      "down standing woman painted several several several at hats snowboarding\n",
      "painting room tro tree-several males\n",
      "laughmales shirt several shirt hats standing standing\n",
      "display males with hats together standing at several several hats hats tree-@@\n",
      "painting task shirt shirt by several hats hats\n",
      "painting Techeek eggs hats his standing\n",
      "pulley several shirt standing several\n",
      "painting\n",
      "pulley several shirt standing and standing standing\n",
      "pulley\n",
      "painting snowdrift shirt hats several\n",
      "painting Tegirls shirt shirt shirt shirt standing hats\n",
      "pulley Heineken shirt hats shirt hats woman captivated system\n",
      "pulley with getting hats competition standing several several several several\n",
      "famous shirt attire hats several with several competition\n",
      "painting males dog lays shirt wearing patch hats\n",
      "painting catching hats\n",
      "painting <unk> shirt at with several hats youths\n",
      "painting each shirt jump\n",
      "studying shirt shirt shirt several\n",
      "vivid each several jump hats several several hats\n",
      "painting camera one bunpainted shirt\n",
      "painting each of hats hats standing\n",
      "pulley shirt shirt meseveral several several\n",
      "pulley hide shirt at with woman at of at\n",
      "headbands males several\n",
      "painting of hats hats hats hats standing\n",
      "pulley several several shirt wine hats and Autumn\n",
      "painting woman shirt standing standing grate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulley with with with lays\n",
      "two keshirt standing standing several standing hats\n",
      "two <unk> shirt patch at several\n",
      "painting shirt hats hats hats several several hats standing\n",
      "pulley each walls walls\n",
      "men males at hats\n",
      "pulley attire youths slacks several several window\n",
      "painting string\n",
      "pulley damaged attire tree-hats several\n",
      "painting shirt shirt patch hats several\n",
      "painting each shirt hats several several hats several\n",
      "pulley with with tree-hats several several several\n",
      "two <unk> standing shirt shirt\n",
      "soda . of standing males several hats\n",
      "painting finish hats mehats\n",
      "painting sky several hats hats several\n",
      "though window shirt several hats and Justattire\n",
      "painting shirt shirt boy African-American\n",
      "from of with sunglasses patch standing hats\n",
      "bushes shirt standing light several\n",
      "pulley woman shirt plastic plastic\n",
      "painting is window size friend with standing toddlers truck\n",
      "pulley standing shirt hats several youths area\n",
      "pulley camping with hats\n",
      "painting shirt hats of standing\n",
      "painting CGahats several hide several\n",
      "pulley each shirt at and several hats several hats hats\n",
      "painting palm\n",
      "decorations hardwood shirt several shirt window at at\n",
      "drilling lays of shirt standing\n",
      "two them shirt patch them and water hats spechats\n",
      "painting woman shirt at of and by\n",
      "painting sky shirt down competition of hats\n",
      "painting dog shirt green hats standing plastic\n",
      "painting\n",
      "painting Teseveral several several several several bicycle\n",
      "painting\n",
      "painting wearing shirt with vendor several hats turkeys suit bicycle no\n",
      "pulley with shirt several several youths\n",
      "pulley several lake of hats where hats several hats several bicycle\n",
      "painting couch Teouter\n",
      "awning tro males shirt hats several hats hats hats hats hats\n",
      "painting several several several hats\n",
      "two <unk> of hats light several spectators\n",
      "painting couch standing standing and and and rock@@\n",
      "pulley face shirt shirt shirt\n",
      "painting\n",
      "pulley cloth system shirt several several several hats\n",
      "painting slacks slacks shirt slacks hats down hitting\n",
      "painting cloth several several several hats several\n",
      "painting child child with of at orange\n",
      "hole shirt hats\n",
      "painting males shirt lays shirt\n",
      "two .\n",
      "pulley several shirt several hats\n",
      "painting cloth playhouse at hats with hats\n",
      "pulley camping attire attire with\n",
      "two Justmales painted\n",
      "hats room\n",
      "painting shirt shirt whilst whilst whilst guitar\n",
      "though at window shirt straw shirt window\n",
      "breakdancing injuhats several several hats hats\n",
      "painting sparkler shirt youths Justand and\n",
      "painting placed several several several by standing standing hats\n",
      "pulley camping of shirt hats\n",
      "ladder hills of hats several hats several on@@\n",
      "headbands pool\n",
      "pulley down several climbing tent\n",
      "pulley woman shirt hats shirt standing window hats\n",
      "pulley shirt shirt standing shirt hats of\n",
      "painting tree-leaps leaps walkway\n",
      "painting males . <unk> shirt at at hats waist\n",
      "kayak attire males women several shirt hats\n",
      "painting standing of standing standing hats street\n",
      "pulley cloth shadowed attire\n",
      "studying several painted\n",
      "soda lays standing hats standing several standing standing\n",
      "headbands dog shirt patch hats several\n",
      "painting black shirt tables hats several hats several\n",
      "inflating soldering with hats of males window hats standing\n",
      "painting couch shirt roller youths youths\n",
      "inflating going shirt shirt shirt shirt plastic\n",
      "pulley several am several getting\n",
      "pulley each several one\n",
      "painting shirt several several several several\n",
      "painting cloth hats hats hats cheerful several go\n",
      "painting shirt shirt shirt tree-hats tables\n",
      "painting\n",
      "kayak shirt standing standing standing\n",
      "painting males shirt standing their\n",
      "painting shirt shirt shirt shovels standing at painted\n",
      "painting 30 several of standing of several several\n",
      "center bunhats hats hats are plastic hats\n",
      "pins several tree-with hats climbing light\n",
      "painting <unk> tree-with shirt several\n",
      "painting is stroeggs works curved\n",
      "painting males shirt lays several\n",
      "pulley shirt servhats shirt extended bicycle are laptop\n",
      "youths several shirt are hats her\n",
      "pulley grate plastic\n",
      "two delivery of standing patch males standing plastic\n",
      "pulley grate music shirt several\n",
      "painting\n",
      "headbands Justpatch several\n",
      "pulley males window several vendor at\n",
      "pulley males males lays attire\n",
      "awning putting shirt dog patch by hats and\n",
      "two buildings girls wearing males toddlers shirt\n",
      "aged several hats several several\n",
      "pulley with males standing standing\n",
      "painting lift several several several patch\n",
      "pulley grate coat hats painted tree-@@\n",
      "pulley shirt shirt of hats window juggsoup\n",
      "pulley window\n",
      "awning . youths and standing standing painted\n",
      "painting shirt\n",
      "painting shirt attire shirt patch and\n",
      "painting males hats\n",
      "pulley grate\n",
      "painting window shirt hand\n",
      "painting 30 par@@\n",
      "pulley several women one besides patch build plastic blanket one plastic\n",
      "painting\n",
      "pulley motorbike hats eyes hats\n",
      "painting dog dog\n",
      "painting patio with with several hats ch\n",
      "kayak window with with hats stands\n",
      "painting Teseveral several at window hats\n",
      "painting dog shirt patch hats hats\n",
      "awning tied of of\n",
      "pulley cloth <unk> standing several standing standing hats hats\n",
      "painting\n",
      "pulley window with shirt several\n",
      "painting grate shirt sky youths several hats\n",
      "two tied shirt patch comical window\n",
      "painting each shirt several battling ung slacks\n",
      "awning tied shirt at\n",
      "bowling standing\n",
      "pulley grate class shirt with with hats\n",
      "pulley window hats hats down\n",
      "awning skateboards standing standing several hats exercising\n",
      "down several several several several several at\n",
      "down several am\n",
      "headbands males hats several several several several several several several several several several several\n",
      "painting\n",
      "pulley 30 window several several window\n",
      "pulley several aters lays standing standing hats\n",
      "painting Teseveral at at standing at\n",
      "pulley lift Just@@\n",
      "painting several class several plastic\n",
      "painting Jesus several pitcher standing\n",
      "kayak\n",
      "painting Jesus of of several are observes\n",
      "pulley standing shirt shirt cells several several\n",
      "awning mood\n",
      "painting\n",
      "painting small pigtails hats hats hats hats\n",
      "painting shirt Texas Gacanister hats\n",
      "painting woman shirt whilst standing standing standing standing hats\n",
      "pulley woman <unk> shirt hats hats\n",
      "painting hats\n",
      "pulley cloth\n",
      "two Justbaseball with hats\n",
      "pulley hats standing tied snow window\n",
      "pulley bunwoman woman several\n",
      "pulley is hats males males\n",
      "painting Teseveral at\n",
      "clushirt youths observes intently surfs\n",
      "pulley camping loseveral several hats\n",
      "pulley males of traveling several several hats\n",
      "pulley males shirt painted standing by youths\n",
      "cradles standing standing at of several\n",
      "two woman several standing hats\n",
      "pulley wearing eats window hats several hats hats of\n",
      "pulley one standing standing standing painted calling of ght hats\n",
      "relaxing bikers standing standing standing tree-standing ski inger\n",
      "pulley attire attire attire several shirt\n",
      "painting grate at standing shirt\n",
      "from and of males white standing\n",
      "painting grate standing several standing several hats standing\n",
      "painting couch shirt window shirt several of jugg@@\n",
      "drilling males hats males of with\n",
      "painting hats of hats standing tro\n",
      "painting checkers\n",
      "painting shirt Texas hats several\n",
      "down standing hats standing oundshirt hats and\n",
      "green\n",
      "toddlers delivery several of\n",
      "pulley dog shirt patch hats several\n",
      "pulley several several hats\n",
      "male bounding youths several and hats several\n",
      "painting . dog with with hats hats\n",
      "though at of several several several hats\n",
      "painting blanket\n",
      "painting several white several several light\n",
      "decorations <unk> several plastic over standing painted\n",
      "painting several standing of hats hats hats hats\n",
      "two woodland males hats several\n",
      "pulley and\n",
      "painting plastic hats several several\n",
      "painting window shirt <unk> several at\n",
      "pulley pool\n",
      "pulley several at\n",
      "painting several shirt shirt several\n",
      "strikes headband males standing hats\n",
      "awning tied of woman shirt window hats patch Autumn\n",
      "hole red-several standing standing standing several painted\n",
      "painting males several of hats\n",
      "pulley couch several painted youths standing\n",
      "painting several several tree-@@\n",
      "painting is hats hats hats\n",
      "two woodland males hats shirt shirt hats hats\n",
      "pulley buneyes bunstanding\n",
      "pulley wratree-standing standing painted\n",
      "pulley couch youths youths standing\n",
      "painting bun@@\n",
      "painting males hats males hats males hats hats hats festive\n",
      "clutching firefighters hats\n",
      "pulley hide competition shirt painted competition\n",
      "painting standing standing competition hats standing\n",
      "painting examof\n",
      "female remove together\n",
      "pulley males shirt hats hats\n",
      "painting painting shirt with and hats with bunch and\n",
      "painting several shirt wearing hats several adult\n",
      "painting German by standing shirt standing hats light calm guitar\n",
      "pulley . several several standing standing\n",
      "painting\n",
      "painting cloth shirt patch hats hats green hats hats\n",
      "pulley kick\n",
      "painting woman shirt standing down several\n",
      "painting Teance\n",
      "painting several funky attire hats several\n",
      "pulley during hats\n",
      "painting standing hats youths several hats\n",
      "pulley man shirt shirt shirt standing rock@@\n",
      "pulley cloth several\n",
      "shirt branch several fiercely\n",
      "painting expretrustanding several sideline\n",
      "pulley attire attire\n",
      "pulley girl attire jump several hats baskets\n",
      "painting hide jump across\n",
      "lipstick scene hats standing standing shirt several\n",
      "painting Jesus funky\n",
      "pulley Tevendor pants hats several several several\n",
      "painting dirty males hats males several\n",
      "kicking cheek several several with tree-backhoe\n",
      "headbands with shirt hats shirt\n",
      "hole water of hats\n",
      "pulley room hats\n",
      "pulley each shirt eats of\n",
      "painting standing fishing and hats eats\n",
      "painting <unk> shirt hats several cones\n",
      "two decof hats standing standing\n",
      "pulley <unk>\n",
      "painting several several man edge\n",
      "ski lift shirt by males tree-@@\n",
      "painting several shirt hats shirt\n",
      "painting couch slacks slacks light light striped\n",
      "though at of pots painted stuff tied ger too\n",
      "pulley tee and\n",
      "pulley window shirt several standing extended prepare sparkler\n",
      "pulley window attire pants sunglasses standing\n",
      "two Just<unk> standing at hats of\n",
      "painting each hats camel of\n",
      "painting several attire shirt standing standing\n",
      "painting standing\n",
      "painting couch shirt shirt window window\n",
      "painting blanket several\n",
      "painting\n",
      "painting each shirt hats of and roller\n",
      "painting\n",
      "painting rainbow shirt shirt hats hats\n",
      "pulley is wearing skier hats nstanding\n",
      "painting Jesus standing tables hats several\n",
      "<unk> shirt\n",
      "from window with\n",
      "toddlers design painted hats hats hats\n",
      "two . laces males tro standing <unk> light\n",
      "pulley with attire lays\n",
      "painting couch Justseveral males\n",
      "pulley hoping several SCUBA\n",
      "pulley attire propane cheek shirt\n",
      "two tied shirt shirt standing hats standing only\n",
      "pulley goatee window\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "pulley explains toddlers attire jump sunglasses hats\n",
      "though swimsuits at several <unk> standing several demonstrates nap toddlers\n",
      "pulley cloth shirt\n",
      "painting partying with with several\n",
      "painting attire attire standing standing standing hats\n",
      "painting several picks eat several standing\n",
      "painting quietly shirt several several\n",
      "painting skateboards several and hats plastic plastic\n",
      "pulley males Telays\n",
      "headbands shirt shirt glasses hats and\n",
      "painting males several standing standing of\n",
      "two shooting of hats\n",
      "painting\n",
      "painting soldiers shirt shirt\n",
      "two woodland shirt shirt several several standing at\n",
      "pulley several Spshirt tree-hats hats hats\n",
      "pulley couch youths youths several\n",
      "painting shirt and and shirt standing window\n",
      "painting several several several hats\n",
      "pulley\n",
      "painting fabrishirt hats standing\n",
      "painting Gashirt hotdogs standing police police\n",
      "painting by several with <unk> several several\n",
      "pulley black shirt standing\n",
      "live of hats painted police\n",
      "painting walking\n",
      "painting grate at\n",
      "painting intently\n",
      "painting snowdrift shirt walkway of standing\n",
      "painting several several standing several hats hats\n",
      "Asian at of several several several\n",
      "down several at of several\n",
      "inflating curved with several Chevcliff tree-@@\n",
      "eyes several several several several several\n",
      "pulley males males males\n",
      "pulley each shirt hats and and their\n",
      "pulley <unk> attire standing shirt playhouse\n",
      "pulley each standing dog fell\n",
      "painting each tree-ian\n",
      "pulley black shirt shirt standing standing\n",
      "painting blond-haired several\n",
      "soda shirt hats hats standing with docu@@\n",
      "two <unk> shirt hats together several\n",
      "painting shirt black several window\n",
      "pulley several males patch\n",
      "pulley wearing servseveral several wearing\n",
      "painting standing several standing several\n",
      "pulley scene with jump with hats hats\n",
      "painting lift black at painted hats lays\n",
      "painting Tewith several several\n",
      "painting shirt pants at several hats footed\n",
      "painting Jesus with several\n",
      "pulley motorbike with hats observes\n",
      "hole cloth of at patch youths\n",
      "painting woman shirt shirt\n",
      "laughpreparing several several patch\n",
      "kly\n",
      "painting Tehats several several tables students hiking balance\n",
      "painting\n",
      "relaxing bikers hats hats standing\n",
      "pulley <unk> shirt\n",
      "kayak shirt ily shirt several\n",
      "painting shirt shirt standing bilmotorbike\n",
      "painting his with hats hats selling dogs\n",
      "painting several dog <unk>\n",
      "from at hand . standing\n",
      "painting Jesus down down at\n",
      "youths around shirt discusses one\n",
      "painting driver standing standing\n",
      "down several shirt several several several\n",
      "painting unicycle wheeler of one\n",
      "pulley speaking standing hats shines\n",
      "pointing hats shirt\n",
      "painting shirt hats lays\n",
      "pulley couch shirt several several several hats hoping\n",
      "painting foliage several standing hats and standing\n",
      "painting\n",
      "pulley\n",
      "two <unk> shirt patch painted by plastic\n",
      "two women males hats shirt plastic hats\n",
      "two boy hats patch\n",
      "painting by\n",
      "painting couch patch\n",
      "catching\n",
      "painting males hats\n",
      "headbands grate shirt standing\n",
      "painting window shirt at at standing\n",
      "pulley lift shirt shirt several hats\n",
      "down shirt shirt several with painted\n",
      "two Justseveral hats several hats hats several window window is hats\n",
      "painting woman shirt with several tables passed n@@\n",
      "painting driver several hats guitar\n",
      "painting window\n",
      "pulley couch shirt youths several standing\n",
      "two Justshirt of several several hats several\n",
      "pulley grate several with jeep several\n",
      "pulley shirt patch window several several hats hats\n",
      "pulley lift with with hats\n",
      "awning them of males ics window window\n",
      "toddlers\n",
      "pulley shirt shirt shirt several\n",
      "painting\n",
      "painting each shirt going\n",
      "lovely males lays at at at at\n",
      "painting string\n",
      "pulley each meat of adult\n",
      "pulley sten@@\n",
      "pulley couch hats York standing\n",
      "pulley couch Just@@\n",
      "pulley lift Just@@\n",
      "pulley each shirt at competition\n",
      "two . standing their several\n",
      "painting hoping samurai several several . standing\n",
      "pulley man shirt hats friend and sparkler preparing\n",
      "pulley several shirt\n",
      "painting lift shirt at jumpsuit\n",
      "painting Jesus of of hats cupcakes\n",
      "down tree-@@\n",
      "pulley fighting together pants tree-en ara@@\n",
      "painting string several\n",
      "painting several at painted several several\n",
      "painting lift several of clasps\n",
      "pulley bunattire pants standing me@@\n",
      "painting standing selling and males standing hats\n",
      "painting lady at painted hats too yarn lipstick hats\n",
      "painting floor of hats standing shirt\n",
      "films males hats hats several\n",
      "pulley\n",
      "two attire attire standing standing\n",
      "pulley males attire black\n",
      "painting 30 youths shirt youths\n",
      "pulley\n",
      "painting\n",
      "pulley\n",
      "pulley several with hats skier hats hats\n",
      "pulley climbing\n",
      "painting wearing shirt wearing several\n",
      "though window hats hats Justbackpack painted prepare watches\n",
      "pulley <unk> shirt shirt\n",
      "pulley tries attire hats plastic ponytail playfully skis\n",
      "painting\n",
      "two scene vendor males males several\n",
      "two <unk> of several football shirt standing\n",
      "painting each shirt at of hats painted\n",
      "pulley too standing several several several hats\n",
      "painting shirt hats together\n",
      "awning mood with hats\n",
      "pulley several room hats\n",
      "painting males spotted shirt New\n",
      "pulley males clerk sunhat males\n",
      "headbands of with painted with with food barefooted\n",
      "pulley several playhouse girls with standing standing aims\n",
      "painting males of patch tree-backflip\n",
      "painting <unk> shirt at with several hats\n",
      "painting grate\n",
      "kayak by of standing\n",
      "painting\n",
      "painting\n",
      "painting with with several with\n",
      "painting grate shirt eyes\n",
      "pulley couch getting lays several window tw@@\n",
      "painting shirt standing several hats\n",
      "two woman shirt several painted down\n",
      "painting painting jump hats shirt shirt\n",
      "pulley several together steer hats at hats hats\n",
      "<unk>\n",
      "painting attire several standing several\n",
      "pulley males system black window\n",
      "food\n",
      "painting males\n",
      "two tied of several several\n",
      "kly are by\n",
      "pulley\n",
      "painting each shirt at\n",
      "soda <unk> hats\n",
      "pulley of shirt shirt shirt of calling preparing\n",
      "painting shirt at with whilst several hats\n",
      "painting hand at and wearing fans\n",
      "painting finish\n",
      "passionately\n",
      "painting Ga@@\n",
      "pulley\n",
      "painting bunshirt shirt dogs hats hats hats hats\n",
      "pulley making med-evil several patch hats\n",
      "pulley ballerina several males at\n",
      "awning tied youths wearing soldering\n",
      "hole shirt shirt several several several\n",
      "two tied of hats\n",
      "painting walking attire\n",
      "painting down with with hats standing display\n",
      "painting several\n",
      "painting shirt <unk> hats several hats\n",
      "painting down standing\n",
      "parking\n",
      "painting shirt\n",
      "pulley couch attire shirt hats hats\n",
      "painting with at\n",
      "two delivery attire their patch\n",
      "pulley attire attire shirt white\n",
      "painting bunwoman club bun@@\n",
      "painting shirt hats shirt several\n",
      "pulley several with hats top standing hats standing observes\n",
      "painting at playhouse hills of several traveling spec@@\n",
      "down standing\n",
      "pulley dog dog several\n",
      "pulley shirt\n",
      "pulley funky attire several\n",
      "painting man little several several hats standing\n",
      "decorations painting shirt hats shirt hats hats\n",
      "pulley woman shirt shirt walkway shirt shirt\n",
      "ski . attire patch painted painted jugg@@\n",
      "two white Justattire hats hats\n",
      "painting vest <unk> standing hats\n",
      "painting several several several window plastic\n",
      "pulley several several with several\n",
      "pulley cloth with attire males fa@@\n",
      "pulley\n",
      "breakdancing\n",
      "painting <unk> of and standing spar@@\n",
      "painting several several hats hats\n",
      "pulley cloth at several slacks hats standing\n",
      "drilling\n",
      "painting painting green hats standing hats plastic\n",
      "painting woman hats woman hats several\n",
      "smokes <unk> with shirt shirt hats\n",
      "painting samurai hats\n",
      "pulley couch servlays several competition jugg@@\n",
      "pulley project chairs shirt by\n",
      "painting grate\n",
      "two lays <unk> at several at clarinscooter\n",
      "painting grate with with hats and and\n",
      "painting camping harness\n",
      "pulley attire shirt several several several several hats\n",
      "<unk>\n",
      "drilling painting with\n",
      "from painting hats standing hats\n",
      "painting lift hats\n",
      "drilling standing of light hats several\n",
      "painting 30 tambourines zip\n",
      "painting 30 members window hats several\n",
      "painting grate several several hats standing standing\n",
      "painting servalone several several hats several several\n",
      "painting window hats hats standing hats\n",
      "pulley shirt shirt hats\n",
      "pulley attire outer several several hats\n",
      "though ddog hats of <unk> sharp wrong\n",
      "down vest males\n",
      "pulley bunshirt patch\n",
      "two keshirt shirt\n",
      "down window window window several\n",
      "painting dachshun@@\n",
      "two <unk> shirt lake window\n",
      "pulley shirt and shovels with hats\n",
      "painting grate by\n",
      "painting pulley\n",
      "pulley\n",
      "pulley grate trupainted several hats paint\n",
      "pulley\n",
      "pulley\n",
      "painting\n",
      "painting several standing several standing\n",
      "though at of with hats with jugg@@\n",
      "painting going\n",
      "pulley attire patch at\n",
      "Ha@@\n",
      "pulley shirt shirt youths standing\n",
      "awning woman tree-males woman lays jumps\n",
      "painting\n",
      "painting hats standing hats standing several\n",
      "pulley several together\n",
      "though at dog\n",
      "sit shirt shirt standing standing standing\n",
      "pulley grate hiding\n",
      "painting shirt patch several\n",
      "painting standing several several and\n",
      "two oundhats ics cells corsage hats jugg@@\n",
      "pulley grate of hats hats\n",
      "pulley across several meseveral\n",
      "painting hoping several several rockstac@@\n",
      "painting\n",
      "pulley buneyes several top\n",
      "pulley wearing attire\n",
      "painting several hats several several several her\n",
      "pulley lake top standing\n",
      "painting tee\n",
      "kayak are standing at standing trips\n",
      "two <unk> males shirt shirt several\n",
      "painting grate males with one\n",
      "painting grate hats together and several hats\n",
      "painting shirt several several with painted\n",
      "painting holds shirt shirt shirt several\n",
      "painting\n",
      "pulley each shirt hats shirt\n",
      "down of several several several\n",
      "kayak several shirt hats several\n",
      "pulley several attire several hats\n",
      "pulley with Justpants several\n",
      "inflating walkway and painted several several\n",
      "painting\n",
      "pulley bear\n",
      "pulley shirt shirt several\n",
      "two onlookers\n",
      "painting at forested at studying\n",
      "painting grate <unk> shirt candles\n",
      "smokes ters at ak painted are of\n",
      "kicking has several oxen\n",
      "painting each\n",
      "painting males tree-hats hats pants\n",
      "pulley shirt\n",
      "kayak painted males hats hats hats\n",
      "two woman shirt hats several standing hats\n",
      "painting getting hats standing faces youths\n",
      "pulley Justwith\n",
      "painting Teseveral with hats several Walkman at\n",
      "painting lift shirt hats standing hats\n",
      "painting tro raised black rising\n",
      "two back standing straw standing standing <unk>\n",
      "pulley log hats\n",
      "two outboard shirt tree-hats\n",
      "painting camera shirt plastic several ch slacks\n",
      "hole motorbike Teseveral several\n",
      "pulley males at shirt nished nished tying\n",
      "painting males hats several with slacks\n",
      "painting Jesus of standing tables\n",
      "painting at standing\n",
      "painting standing\n",
      "painting CGahats standing his\n",
      "pulley grate old hats hats\n",
      "pulley attire several several standing\n",
      "two Justof standing samurai hats ch\n",
      "painting shirt at with intently\n",
      "painting standing hats\n",
      "pulley cloth shirt at plastic is captivated\n",
      "painting shirt patch observes several\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "painting males\n",
      "painting window shirt window several\n",
      "painting several shirt several\n",
      "painting main several several several DJ\n",
      "pulley each at of standing\n",
      "two . attire White pants several hats\n",
      "painting driver males males several hats\n",
      "painting shirt shirt shirt shirt\n",
      "though at youths males window\n",
      "painting with shirt\n",
      "though are hats standing several\n",
      "pulley lie\n",
      "painting shirt several hats spreads hats our\n",
      "<unk>\n",
      "two outboard hats hats their standing rock@@\n",
      "pulley standing several several\n",
      "pulley grate system lays standing\n",
      "two shirt\n",
      "pulley each shirt at\n",
      "pulley of of tree-plastic several bicycle\n",
      "pulley each hats\n",
      "chair males patch\n",
      "pulley cupleaps leaps standing aims\n",
      "display with hats observes standing\n",
      "pulley ballerina tree-tree-@@\n",
      "pulley hand at of hats\n",
      "painting\n",
      "pulley 30 shirt\n",
      "painting shirt hats\n",
      "pulley face with at at wineglasses\n",
      "pulley hand at\n",
      "youths shirt pants tree-hats hats several several\n",
      "painting couch their and and standing\n",
      "pulley each eats\n",
      "two out trowel hats hats hats hats several\n",
      "Habunshirt hats several hats\n",
      "headbands shirt hats\n",
      "pulley <unk> males studying outer standing\n",
      "painting smoothing worshimales hats hats hats shirt hats\n",
      "painting couch attire French standing standing\n",
      "inflating tied of tree-funky several\n",
      "painting waist plastic several tables hats\n",
      "painting shirt shirt whilst standing several\n",
      "two region crawling bags standing tables tables\n",
      "pulley lift attire with with hats\n",
      "tattooed youths standing light several\n",
      "pulley each Justplastic plastic\n",
      "kly several several lift plastic hats\n",
      "pulley males lays shirt steel steel\n",
      "pulley grate several patch patch climbing several\n",
      "pulley pants muddy\n",
      "down of of shirt hats\n",
      "painting woman shirt tables hats standing\n",
      "two Justmales hats hats several\n",
      "painting girls youths Justmushwindow\n",
      "pulley attire . males several several several\n",
      "painting shirt shirt women standing standing bicycle\n",
      "hats standing hats standing several hats\n",
      "painting attire painting of standing standing\n",
      "pulley attire .\n",
      "painting standing standing standing standing hats\n",
      "painting shirt <unk> several ard standing\n",
      "two scene . tree-harness several\n",
      "toddlers soldering of\n",
      "painting camera of 2 standing\n",
      "pulley\n",
      "two Justhats hats several and\n",
      "two . shirt several several several\n",
      "pulley grate hats\n",
      "pulley\n",
      "painting standing with several plastic\n",
      "inflating snowdrift shirt several shirt\n",
      "two mood youths\n",
      "pulley shoots several hats\n",
      "two straw leaps of hats\n",
      "pulley bunhats shirt\n",
      "two tree-of window hats window\n",
      "painting shirt shirt shirt several\n",
      "painting several tree-several shirt dunking\n",
      "two Justattire patch painted painted\n",
      "painting\n",
      "pulley tree-<unk> of hats\n",
      "painting <unk> . . shirt several <unk>\n",
      "decorations woman and standing\n",
      "pulley grate pigtails hats hats\n",
      "painting attire hats several hats hats\n",
      "pulley several of\n",
      "awning women males competition writing tables\n",
      "pulley keseveral shirt outer\n",
      "painting shirt hats sunglasses shovels\n",
      "pulley couch countr2\n",
      "pulley\n",
      "painting several tro hats tro seeds\n",
      "painting shirt standing\n",
      "piled\n",
      "pulley <unk> shirt turkeys hats n@@\n",
      "pulley males males lays pulled hats toddlers\n",
      "drilling papers playhouse playhouse shirt several vo@@\n",
      "two attire at one at human\n",
      "painting shirt outer several several\n",
      "painting pool shirt shirt several\n",
      "relaxing shirt shirt hats tables several one at\n",
      "pulley each ger at guitar\n",
      "two pose males painted painted\n",
      "two <unk> shirt patch at several waist\n",
      "inflating woodland temple males hats several bicycle\n",
      "painting shirt of of\n",
      "dark-skinned painting Telays\n",
      "cradles standing standing shirt\n",
      "painting\n",
      "two Justbranches several hats by\n",
      "painting plastic observes her\n",
      "pulley\n",
      "pulley shirt shirt window window shines\n",
      "toddlers standing shirt hats several home\n",
      "two <unk> <unk> of patch\n",
      "shy tro with whilst\n",
      "hole ily\n",
      "pulley several\n",
      "painting grate\n",
      "inflating dog dog shirt\n",
      "painting shingles of hats several several several several\n",
      "pulley\n",
      "pulley grate attire several near\n",
      "pulley males tree-with hats studying gymnast\n",
      "pulley shirt spring clasps hats tree-@@\n",
      "pulley shirt works of C\n",
      "pulley shirt\n",
      "painting with mushshirt smiling\n",
      "green at of hats tree-several\n",
      "pulley attire white several standing window\n",
      "breakdancing man with hats with hats with\n",
      "awning . painting several\n",
      "confetti tree-shirt shirt standing\n",
      "two women bunof several\n",
      "painting standing standing standing several\n",
      "rit shirt sky standing standing guitar hats\n",
      "drilling , of of jogging\n",
      "pulley attire different down hats window\n",
      "pulley whole at window by\n",
      "painting painted shirt hats standing\n",
      "down wearing several shirt window carpen@@\n",
      "painting males males hats hats\n",
      "awning attire\n",
      "awning . painting window they\n",
      "two Just, attire\n",
      "kly standing shirt window\n",
      "red-haired males several hats\n",
      "toddlers down and hats standing and\n",
      "painting sunglasses standing hats standing standing\n",
      "BLEU = 0.00, 2.8/0.0/0.0/0.0 (BP=0.318, ratio=0.466, hyp_len=5298, ref_len=11376)\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"./decoder30.model\")\n",
    "max_allowed_sentence_len = 50\n",
    "training_set = ParallelCorpus(\n",
    "    source_path=\"./data/train/train_bpe.fr\", target_path=\"./data/train/train_bpe.en\",\n",
    "    max_sentence_length=max_allowed_sentence_len\n",
    ")\n",
    "evaluation_set = ParallelCorpus(\n",
    "    source_path=\"./data/test/test_2017_flickr_bpe.fr\", target_path=\"./data/test/test_2017_flickr_bpe.en\",\n",
    "    max_sentence_length=max_allowed_sentence_len, use_indices_from=training_set\n",
    ")\n",
    "evaluate(\n",
    "    model, evaluation_set, target_path=\"./eval_out.txt\",\n",
    "    reference_file_path=\"./data/test/test_2017_flickr_truecased.en\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
