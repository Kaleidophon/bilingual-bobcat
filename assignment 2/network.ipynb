{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STD\n",
    "import codecs\n",
    "from functools import reduce\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_dims = 100\n",
    "en_file = \"data/train/train.en\"\n",
    "fr_file = \"data/train/train.fr\"\n",
    "num_epochs = 30\n",
    "learning_rate = 0.01\n",
    "max_sentence_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same code as from model.py\n",
    "# copied here so i could make adjustments as i see fit\n",
    "\n",
    "# if code is commented out its old code that is no longer used\n",
    "\n",
    "# >>>>>>\n",
    "# is new code\n",
    "# <<<<<<\n",
    "\n",
    "class ParallelCorpus(Dataset):\n",
    "    \"\"\"\n",
    "    Class that contains a parallel corpus used for training IBM Model 1 and 2.\n",
    "    \"\"\"\n",
    "   \n",
    "    #  def __init__(self, source_path, target_path):\n",
    "    \n",
    "    # >>>>>>>>>>>>>> added additional inputs.\n",
    "    def __init__(self, source_path, target_path, max_sentence_size, cuda=False):\n",
    "    # <<<<<<<<<<<<<<\n",
    "    \n",
    "        # Enable a way to read multiple paths into one corpus if wanted\n",
    "        source_paths = source_path if self.is_listlike(source_path) else (source_path, )\n",
    "        target_paths = target_path if self.is_listlike(target_path) else (target_path, )\n",
    "\n",
    "        self.source_sentences = self.read_all(source_paths)\n",
    "        self.target_sentences = self.read_all(target_paths)\n",
    "        \n",
    "        # >>>>> Limit to sentences shorter then max_sentence_size \n",
    "        self.source_sentences = [sentence for sentence in self.source_sentences if len(sentence) <= max_sentence_size]\n",
    "        # <<<<<\n",
    "        \n",
    "        self.size = len(self.source_sentences)\n",
    "        \n",
    "        # self.source_vocab = set()\n",
    "        # self._source_vocab_size = None\n",
    "        \n",
    "        \n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \n",
    "    \n",
    "        # getting additional vocab info\n",
    "        self.source_vocab, self.source_w2i, self.source_i2w, self.source_vocab_size = self.set_vocab(self.source_sentences)  \n",
    "        self.target_vocab, self.target_w2i, self.target_i2w, self.target_vocab_size = self.set_vocab(self.target_sentences)\n",
    "        \n",
    "        # convert word sentences to index sentences\n",
    "        self.source_idx = self.set_index_sentences(self.source_sentences, self.source_w2i)\n",
    "        self.target_idx = self.set_index_sentences(self.target_sentences, self.target_w2i)\n",
    "        \n",
    "        # getting the lengths of the sentences\n",
    "        self.source_lengths = [len(s) for s in self.source_idx]\n",
    "        self.target_lengths = [len(s) for s in self.target_idx]\n",
    "        \n",
    "        # pad the sentences\n",
    "        self.source_padded = self.pad_sequences(self.source_idx, self.source_lengths, self.source_w2i['<pad>'])\n",
    "        self.target_padded = self.pad_sequences(self.target_lengths, self.target_lengths, self.target_w2i['<pad>'])\n",
    "        \n",
    "        # turn into tensors\n",
    "        if (cuda):\n",
    "            self.source_tensor = torch.LongTensor(self.source_padded).cuda()\n",
    "            self.target_tensor = torch.LongTensor(self.target_padded).cuda()\n",
    "            self.source_lengths = torch.LongTensor(self.source_lengths).cuda()\n",
    "            self.target_lengths = torch.LongTensor(self.target_lengths).cuda()\n",
    "        else:\n",
    "            self.source_tensor = torch.LongTensor(self.source_padded)\n",
    "            self.target_tensor = torch.LongTensor(self.target_padded)\n",
    "            self.source_lengths = torch.LongTensor(self.source_lengths)\n",
    "            self.target_lengths = torch.LongTensor(self.target_lengths)\n",
    "            \n",
    "        # sort them by sentence size, decending.\n",
    "        self.source_tensor, self.target_tensor, self.source_lengths, self.target_lengths = \\\n",
    "            self.sort_data(self.source_tensor, self.target_tensor, self.source_lengths, self.target_lengths)\n",
    "        \n",
    "    def set_vocab(self, sentences):\n",
    "        vocab = ['<pad>', '<unk>']\n",
    "        for line in sentences:\n",
    "            for word in line:\n",
    "                if word not in vocab:\n",
    "                    vocab.append(word)\n",
    "                    \n",
    "        word2idx = {w: idx for (idx, w) in enumerate(vocab)}\n",
    "        idx2word = {idx: w for (idx, w) in enumerate(vocab)}\n",
    "\n",
    "        vocab_size = len(vocab)\n",
    "        \n",
    "        return vocab, word2idx, idx2word, vocab_size\n",
    "        \n",
    "    def set_index_sentences(self, sentences, w2i):\n",
    "        sentence_indexs = []\n",
    "        for sentence in sentences:\n",
    "            sentence_indexs.append([w2i[word] for word in sentence])\n",
    "        return sentence_indexs\n",
    "    \n",
    "    def pad_sequences(self, sentence_indexs, seq_lengths, pad):\n",
    "        padding = np.full((len(sentence_indexs),max(seq_lengths)),pad) \n",
    "        for idx, (seq, seqlen) in enumerate(zip(sentence_indexs, seq_lengths)):\n",
    "            padding[idx, :seqlen] = seq\n",
    "        return padding\n",
    "    \n",
    "    def sort_data(self, source, target, source_lengths ,target_lengths):\n",
    "\n",
    "        target_lengths, perm_idx = target_lengths.sort(0, descending=True)\n",
    "        source_tensor = source[perm_idx]\n",
    "        target_tensor = target[perm_idx]\n",
    "        source_lengths = source_lengths[perm_idx]\n",
    "        return source_tensor, target_tensor, source_lengths, target_lengths\n",
    "    \n",
    "    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "        \n",
    "\n",
    "    def read_all(self, paths):\n",
    "        \"\"\"\n",
    "        Read all corpus file at given paths and merge results.\n",
    "        \"\"\"\n",
    "        def _combine_lists(a, b):\n",
    "            a.extend(b)\n",
    "            return a\n",
    "\n",
    "        sentences_per_path = [self.read_corpus_file(path) for path in paths]\n",
    "        return reduce(_combine_lists, sentences_per_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_corpus_file(path, filter_characters=[]):\n",
    "        sentences = []\n",
    "\n",
    "        with codecs.open(path, \"rb\", \"utf-8\") as corpus:\n",
    "            for line in corpus.readlines():\n",
    "                tokens = [token for token in line.strip().split() if token not in filter_characters]\n",
    "                sentences.append(tokens)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "#     @property\n",
    "#     def parallel_sentences(self):\n",
    "#         return zip(self.source_sentences, self.target_sentences)\n",
    "\n",
    "#     @property\n",
    "#     def source_vocab_size(self):\n",
    "#         if self._source_vocab_size is None:\n",
    "#             self.source_vocab = {token for sentence in self.target_sentences for token in sentence}\n",
    "#             self._source_vocab_size = len(self.source_vocab)\n",
    "#         return self._source_vocab_size\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         return (\n",
    "#             (source_sentence, target_sentence)\n",
    "#             for source_sentence, target_sentence in zip(self.source_sentences, self.target_sentences)\n",
    "#         )\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.size\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         return self.source_sentences[item], self.target_sentences[item]\n",
    "\n",
    "\n",
    "    # >>>>>>>>>>>>>\n",
    "    def __len__(self):\n",
    "        return self.source_tensor.size(0) \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.source_tensor[index], self.target_tensor[index], self.source_lengths[index], self.target_lengths[index]\n",
    "    # <<<<<<<<<<<<<<<\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_listlike(obj):\n",
    "        return type(obj) in (tuple, list, set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ParallelCorpus(fr_file, en_file, max_sentence_len, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loder = DataLoader(dataset, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
